{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NPF_NPV_Experiments_wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tk7NlxsLjaXYDrrQH9RG-LOHm51cy-3f",
      "authorship_tag": "ABX9TyNfl5ZBZNbt1lYBB82/Bz0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "952f2930c16747b3a7afb34e21619a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d408faf419884bec90a99d394abbf979",
              "IPY_MODEL_eda76b679c4c48e8bb5ab80e3593503a",
              "IPY_MODEL_9116e8aee4e54ba08dee57cba8e789fc"
            ],
            "layout": "IPY_MODEL_f51a46340b1345e8b84dc556ce4f3352"
          }
        },
        "d408faf419884bec90a99d394abbf979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b9942c21204a77b0cbd646192b08ce",
            "placeholder": "​",
            "style": "IPY_MODEL_074287f943c94ca3aead5131317df941",
            "value": ""
          }
        },
        "eda76b679c4c48e8bb5ab80e3593503a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b1707a569424db29a338d617843faa4",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bc6e483eb174fb98abcaa71fe577863",
            "value": 170498071
          }
        },
        "9116e8aee4e54ba08dee57cba8e789fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b74d456c2db4b9fb19463c3c5a4ba43",
            "placeholder": "​",
            "style": "IPY_MODEL_7ef8e7f12873497781e4e9df9ec52302",
            "value": " 170499072/? [00:06&lt;00:00, 30590221.30it/s]"
          }
        },
        "f51a46340b1345e8b84dc556ce4f3352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b9942c21204a77b0cbd646192b08ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "074287f943c94ca3aead5131317df941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b1707a569424db29a338d617843faa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc6e483eb174fb98abcaa71fe577863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b74d456c2db4b9fb19463c3c5a4ba43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef8e7f12873497781e4e9df9ec52302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshyadav007/research/blob/main/NPF_NPV_Experiments_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QAE_4v74TOb"
      },
      "source": [
        "!pip install torchviz\n",
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py0s_6DyDS1l"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torchviz import make_dot\n",
        "import wandb\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSG61_bL7G6K"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC2UC3tGcQJS"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, training_data):\n",
        "        self.size = len(training_data)\n",
        "        X, Y = training_data[0]\n",
        "        self.x = torch.ones(self.size, X.shape[0],X.shape[1], X.shape[2] )\n",
        "        self.y = torch.tensor([x for _,x in training_data])\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(self.x[idx].shape,self.y[idx].shape )\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8L3R0OU_ag"
      },
      "source": [
        "**MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzdnzqZHK9w"
      },
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59xdskmC5sRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYfYr72nKFGX"
      },
      "source": [
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "counter = 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[counter]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    counter += 1\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM_4UVkPnHog"
      },
      "source": [
        "#x_dummy_dataloader_all = DataLoader(CustomDataset(training_data), batch_size=60000, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFLJ5wDuOjl2"
      },
      "source": [
        "#batch_size = 60000\n",
        "#npv_features = get_activation_output(train_dataloader_all, npv_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOUWPTlKbqCP"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "train_dataloader_all = DataLoader(training_data, batch_size=60000, shuffle=False)\n",
        "x_dummy_dataloader = DataLoader(CustomDataset(training_data), batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_HkX5hOSrAX"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    for batch, (x,y) in enumerate(dataloader):\n",
        "        X, y = x.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch%100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "    return pred\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "class FCNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNeuralNet, self).__init__()\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, 128)\n",
        "        self.fc5 = nn.Linear(128, 128)\n",
        "        self.fc6 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x, batch = None):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        o1 = torch.relu(x)\n",
        "        x = self.fc2(o1)\n",
        "        o2 = torch.relu(x)\n",
        "        x = self.fc3(o2)\n",
        "        o3 = torch.relu(x)\n",
        "        x = self.fc4(o3)\n",
        "        o4 = torch.relu(x)\n",
        "        x = self.fc5(o4)\n",
        "        o5 = torch.relu(x)\n",
        "        x = self.fc6(o5)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "model = FCNeuralNet().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 1e-1) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlfyrijWUn9V"
      },
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    pred = train(train_dataloader,model, loss_fn, optimizer)\n",
        "    test(test_dataloader,model, loss_fn)\n",
        "print(\"Done!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPPvW2UeaSQm"
      },
      "source": [
        "class NPKNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NPKNeuralNetwork, self).__init__()\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, 128)\n",
        "        self.fc5 = nn.Linear(128, 128)\n",
        "        self.fc6 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x, batch = None):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        o1 = torch.relu(x)\n",
        "        x = self.fc2(o1)\n",
        "        o2 = torch.relu(x)\n",
        "        x = self.fc3(o2)\n",
        "        o3 = torch.relu(x)\n",
        "        x = self.fc4(o3)\n",
        "        o4 = torch.relu(x)\n",
        "        x = self.fc5(o4)\n",
        "        o5 = torch.relu(x)\n",
        "        x = self.fc6(o5)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x, [o1,o2,o3,o4,o5]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1axh608IPPQJ"
      },
      "source": [
        "\n",
        "class NPVNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NPVNeuralNetwork, self).__init__()\n",
        "    \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, 128)\n",
        "        self.fc5 = nn.Linear(128, 128)\n",
        "        self.fc6 = nn.Linear(128, 10)\n",
        "        self.soft_relu1 = Soft_Relu()\n",
        "        self.soft_relu2 = Soft_Relu()\n",
        "        self.soft_relu3 = Soft_Relu()\n",
        "        self.soft_relu4 = Soft_Relu()\n",
        "        self.soft_relu5 = Soft_Relu()\n",
        "\n",
        "    def forward(self, x,npk_features, batch = None):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        o1 = self.soft_relu1(x, 0, batch, npk_features)\n",
        "        x = self.fc2(o1)\n",
        "        o2 = self.soft_relu2(x,1, batch, npk_features)\n",
        "        x = self.fc3(o2)\n",
        "        o3 = self.soft_relu3(x,2, batch, npk_features)\n",
        "        x = self.fc4(o3)\n",
        "        o4 = self.soft_relu4(x,3, batch, npk_features)\n",
        "        x = self.fc5(o4)\n",
        "        o5 = self.soft_relu5(x,4, batch, npk_features)\n",
        "        x = self.fc6(o5)\n",
        "        F.log_softmax(x, dim=1)\n",
        "        return x,  [o1,o2,o3,o4,o5]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIVKuk5XD7O4"
      },
      "source": [
        "def soft_relu_gate(x, beta, idx, batch, npk_features):\n",
        "  global batch_size\n",
        "  \n",
        "  start = batch_size*batch\n",
        "  end = batch_size * (batch+1)\n",
        "  if x.shape[0] < batch_size:\n",
        "    end = -1\n",
        " \n",
        "  #out = beta*torch.sign(npk_features[idx])\n",
        "  out = beta*(npk_features[idx])\n",
        "  #print(idx,x.shape[0], start, end, out.shape)\n",
        "  return out\n",
        "  \n",
        "class Soft_Relu(nn.Module):\n",
        "    def __init__(self, beta = 4):\n",
        "        super(Soft_Relu,self).__init__()\n",
        "        self.beta = beta#Parameter(torch.tensor(.5), requires_grad = True) \n",
        "       \n",
        "        #self.beta.requiresGrad = True \n",
        "\n",
        "    def forward(self, x, idx, batch, npk_features):\n",
        "      #Soft Relu\n",
        "      out = torch.mul(x,torch.sigmoid(soft_relu_gate(x, self.beta, idx, batch, npk_features)))\n",
        "      return out\n",
        "      \n",
        "      #Hard Relu\n",
        "      #return torch.mul(x,torch.sign(npk_features[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj-IZp0pogAL"
      },
      "source": [
        "def train_decoupled(X1_dataloader,X2_dataloader, npk_model, npv_model, loss_fn, optimizer):\n",
        "    size = len(X1_dataloader.dataset)\n",
        "    correct = 0\n",
        "    for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "        X1, y1 = X1.to(device), y1.to(device)\n",
        "        X2, y2 = X2.to(device), y2.to(device)\n",
        "        #print(X.shape,y.shape)\n",
        "        # Compute prediction error\n",
        "        _, npk_features = npk_model(X1, batch)\n",
        "        pred, npv_features = npv_model(X2, npk_features, batch)\n",
        "\n",
        "        loss = loss_fn(pred, y1)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #print(npv_model.fc2.weight.grad_fn, npv_model.fc2.weight.grad)\n",
        "        optimizer.step()\n",
        "        #print(\"IN inner loop : \", npk_model.fc1.weight.grad_fn, npk_model.fc2.weight.grad)\n",
        "\n",
        "        if batch%100 == 0:\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            loss, current = loss.item(), batch * len(X1)\n",
        "            print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfWnBOEUqSgH"
      },
      "source": [
        "def test_decoupled(dataloader, npk_model, npv_model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    npk_model.eval()\n",
        "    npv_model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            _, npk_features = npk_model(X, batch)\n",
        "            pred, npv_features = npv_model(X, npk_features, batch)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWBD6iRcpoct"
      },
      "source": [
        "npk_model = NPKNeuralNetwork().to(device)\n",
        "npv_model = NPVNeuralNetwork().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xsqeZI1pdGy",
        "outputId": "94a8c578-ce41-47d7-e6d0-036b26ed2b8a"
      },
      "source": [
        "npk_model.load_state_dict(model.state_dict())\n",
        "#npv_model.load_state_dict(npk_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8eGl6qGvYkM"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': npk_model.parameters()},\n",
        "                {'params': npv_model.parameters()}],\n",
        "                lr = 1e-1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dypys-LJvjN9"
      },
      "source": [
        "for param in npv_model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg01ew8iRN5M"
      },
      "source": [
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    \n",
        "    pred = train_decoupled(train_dataloader,train_dataloader,npk_model, npv_model, loss_fn, optimizer)\n",
        "    test_decoupled(test_dataloader,npk_model, npv_model, loss_fn)\n",
        "    #print(\"IN outer loop : \",npk_model.fc1.weight.grad, npk_model.fc3.weight.grad,npk_model.fc5.weight.grad )\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_uL4schaaH-",
        "outputId": "568c4ad2-0057-403f-a5a5-5f2029d31ec6"
      },
      "source": [
        "test_decoupled(train_dataloader,npk_model, npv_model, loss_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 60.5%, Avg loss: 2.210951 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psm_sAPEAlAz"
      },
      "source": [
        "#torch.save(npk_model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/npk.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX-fKwlNB2jq"
      },
      "source": [
        "#torch.save(npv_model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/npv.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFkEHzruoYPH"
      },
      "source": [
        "make_dot(pred, params=dict(npk_model.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFh4uXMi8hZE"
      },
      "source": [
        "make_dot(pred, params=dict(npv_model.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp6kzY2sU40w"
      },
      "source": [
        "**CIFAR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDxxRH4CwgyY"
      },
      "source": [
        "Conv Net\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "952f2930c16747b3a7afb34e21619a25",
            "d408faf419884bec90a99d394abbf979",
            "eda76b679c4c48e8bb5ab80e3593503a",
            "9116e8aee4e54ba08dee57cba8e789fc",
            "f51a46340b1345e8b84dc556ce4f3352",
            "33b9942c21204a77b0cbd646192b08ce",
            "074287f943c94ca3aead5131317df941",
            "0b1707a569424db29a338d617843faa4",
            "0bc6e483eb174fb98abcaa71fe577863",
            "4b74d456c2db4b9fb19463c3c5a4ba43",
            "7ef8e7f12873497781e4e9df9ec52302"
          ]
        },
        "id": "3Pm1kAdSwgCv",
        "outputId": "1a69561a-2a3f-4baf-98e7-60e0a09c5ac4"
      },
      "source": [
        "# Download training data from open datasets.\n",
        "cifar_training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "cifar_test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "952f2930c16747b3a7afb34e21619a25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "s4CWwqmNwo_l",
        "outputId": "6c518249-99ac-46a4-dda6-a00c0e9c44e3"
      },
      "source": [
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "counter = 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(cifar_training_data), size=(1,)).item()\n",
        "    img, label = cifar_training_data[counter]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    counter += 1\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJ6S51txZXM"
      },
      "source": [
        "batch_size = 32\n",
        "cifar_train_dataloader = DataLoader(cifar_training_data, batch_size=batch_size, shuffle=True)\n",
        "cifar_test_dataloader = DataLoader(cifar_test_data, batch_size=batch_size, shuffle=True)\n",
        "cifar_train_dummy_dataloader = DataLoader(CustomDataset(cifar_training_data), batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_7UoVPGiOoj"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(ConvNet, self).__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(3, 128, kernel_size = 3, stride = 1)\n",
        "      self.conv2 = nn.Conv2d(128, 128, kernel_size = 3, stride = 1)\n",
        "      self.conv3 = nn.Conv2d(128, 128, kernel_size = 3, stride = 1)\n",
        "      self.conv4 = nn.Conv2d(128, 128, kernel_size = 3, stride = 1)\n",
        "      # First fully connected layer\n",
        "      \n",
        "      self.avgpool = nn.AvgPool2d(12)\n",
        "      self.fc1 = nn.Linear(128*2*2, 256)\n",
        "      self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = F.relu(self.conv3(x))\n",
        "      x = F.relu(self.conv4(x))\n",
        "      x = self.avgpool(x)\n",
        "      x = x.view(x.shape[0],-1)\n",
        "      x = self.fc1(x)\n",
        "      x = self.fc2(x)\n",
        "      out = F.log_softmax(x, dim=1)\n",
        "      #print(out.shape)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbPNCU7fwcB5"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        #print(X.shape,y.shape)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch%100 == 0:\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxtfEPzEwpv9"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2E8tUgRqFZI"
      },
      "source": [
        "conv_model = ConvNet()\n",
        "conv_model = conv_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmm-vLHFX40x"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(conv_model.parameters(), lr=3e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu90SWR2v8iz"
      },
      "source": [
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(conv_model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcwlzuLXv_j5"
      },
      "source": [
        "epochs = 30\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(cifar_train_dataloader,conv_model, loss_fn, optimizer)\n",
        "    test(cifar_test_dataloader,conv_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3BTPSPO9a9f"
      },
      "source": [
        "def apply_gate(x, beta, idx, batch, npk_features):\n",
        "  global batch_size\n",
        "  \n",
        "  start = batch_size*batch\n",
        "  end = batch_size * (batch+1)\n",
        "  if x.shape[0] < batch_size:\n",
        "    end = -1\n",
        " \n",
        "  #out = beta*torch.sign(npk_features[idx])\n",
        "  out = beta*(npk_features[idx])\n",
        "  #print(idx,x.shape[0], start, end, out.shape)\n",
        "  return out\n",
        "  \n",
        "class Gate(nn.Module):\n",
        "    def __init__(self,relu_type, beta = 4):\n",
        "        super(Gate,self).__init__()\n",
        "        self.beta = beta#Parameter(torch.tensor(.5), requires_grad = True) \n",
        "        self.relu_type = relu_type\n",
        "        #self.beta.requiresGrad = True \n",
        "\n",
        "    def forward(self, x, idx, batch, npk_features):\n",
        "      if self.relu_type == \"soft\":\n",
        "        #Soft Relu\n",
        "        return torch.mul(x,torch.sigmoid(apply_gate(x, self.beta, idx, batch, npk_features)))\n",
        "      else:\n",
        "        #Hard Relu\n",
        "        return torch.mul(x,torch.sign(npk_features[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf8VWzwW7nlc"
      },
      "source": [
        "class NPKConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(NPKConvNet, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 1000, 3, 1)\n",
        "      self.conv2 = nn.Conv2d(1000, 1000, 3, 1)\n",
        "      self.conv3 = nn.Conv2d(1000, 1000, 3, 1)\n",
        "      self.conv4 = nn.Conv2d(1000, 1000, 3, 1)\n",
        "     \n",
        "      self.avgpool = nn.AvgPool2d(12)\n",
        "      self.fc1 = nn.Linear(1000*2*2, 256)\n",
        "      self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x, batch = None):\n",
        "      o1 = F.relu(self.conv1(x))\n",
        "      o2 = F.relu(self.conv2(o1))\n",
        "      o3 = F.relu(self.conv3(o2))\n",
        "      o4 = F.relu(self.conv4(o3))\n",
        "      x = self.avgpool(o4)\n",
        "      x = x.view(x.shape[0],-1)\n",
        "      x = self.fc1(x)\n",
        "      x = self.fc2(x)\n",
        "      out = x#F.log_softmax(x, dim=1)\n",
        "      #print(out.shape)\n",
        "      return out, [o1,o2,o3,o4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3vYWjG78tgs"
      },
      "source": [
        "class NPVConvNet(nn.Module):\n",
        "    def __init__(self, relu_type):\n",
        "      super(NPVConvNet, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 1000, 3, 1)\n",
        "      self.conv2 = nn.Conv2d(1000, 1000, 3, 1)\n",
        "      self.conv3 = nn.Conv2d(1000, 1000, 3, 1)\n",
        "      self.conv4 = nn.Conv2d(1000, 1000, 3, 1)\n",
        "     \n",
        "      self.avgpool = nn.AvgPool2d(12)\n",
        "      self.fc1 = nn.Linear(1000*2*2, 1000)\n",
        "      self.fc2 = nn.Linear(1000, 10)\n",
        "      self.gate1 = Gate(relu_type)\n",
        "      self.gate2 = Gate(relu_type)\n",
        "      self.gate3 = Gate(relu_type)\n",
        "      self.gate4 = Gate(relu_type)\n",
        "\n",
        "    def forward(self, x, batch, npk_features):\n",
        "      x = self.conv1(x)\n",
        "      o1 = self.gate1(x, 0, batch, npk_features)\n",
        "      x = self.conv2(o1)\n",
        "      o2 = self.gate2(x, 1, batch, npk_features)\n",
        "      x = self.conv3(o2)\n",
        "      o3 = self.gate3(x, 2, batch, npk_features)\n",
        "      x = self.conv4(o3)\n",
        "      o4 = self.gate4(x, 3, batch, npk_features)\n",
        "      #print(o4.shape)\n",
        "      x = self.avgpool(o4)\n",
        "\n",
        "      x = x.view(x.shape[0],-1)\n",
        "      \n",
        "      x = self.fc1(x)\n",
        "      x = self.fc2(x)\n",
        "     \n",
        "      out = x#F.log_softmax(x, dim=1)\n",
        "      #print(out.shape)\n",
        "      return out, [o1,o2,o3,o4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0BCzoFi9a9g"
      },
      "source": [
        "def train_decoupled(X1_dataloader,X2_dataloader, npk_model, npv_model, loss_fn, optimizer):\n",
        "    size = len(X1_dataloader.dataset)\n",
        "    correct = 0\n",
        "    for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "        X1, y1 = X1.to(device), y1.to(device)\n",
        "        X2, y2 = X2.to(device), y2.to(device)\n",
        "        #print(X.shape,y.shape)\n",
        "        # Compute prediction error\n",
        "        _, npk_features = npk_model(X1, batch)\n",
        "        pred, npv_features = npv_model(X2, batch, npk_features)\n",
        "        #print(pred.shape, y1.shape)\n",
        "        loss = loss_fn(pred, y1)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(npk_model.fc5.weight.grad)\n",
        "        \n",
        "        if batch%100 == 0:\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            loss, current = loss.item(), batch * len(X1)\n",
        "            print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "            wandb.log({\"loss\": loss})\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ZnabhZ9a9h"
      },
      "source": [
        "def test_decoupled(data_type,dataloader, npk_model, npv_model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    npk_model.eval()\n",
        "    npv_model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            _, npk_features = npk_model(X, batch)\n",
        "            pred, npv_features = npv_model(X, batch, npk_features)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    \n",
        "    if data_type == \"training\":\n",
        "      wandb.log({\"train_acc\": 100*correct, \"train_loss\" : test_loss })\n",
        "    else:\n",
        "      wandb.log({\"test_acc\": 100*correct, \"test_loss\" : test_loss })\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbMxR59KCK4_"
      },
      "source": [
        "#npk_model.load_state_dict(conv_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qRj7O2AA7oL"
      },
      "source": [
        "def make(config):\n",
        "  lr = config.learning_rate\n",
        "  relu_type = config.activation\n",
        "\n",
        "  npk_model = NPKConvNet().to(device)\n",
        "  npv_model = NPVConvNet(relu_type).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam([\n",
        "                {'params': npk_model.parameters()},\n",
        "                {'params': npv_model.parameters()}],\n",
        "                lr = lr) \n",
        "\n",
        "  return npk_model, npv_model, criterion, optimizer\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpwwNvWcDmc7"
      },
      "source": [
        "def train(npk_model, npv_model, criterion, optimizer, config):\n",
        "  #wandb.watch(npk_model, criterion, log=\"all\", log_freq=10)\n",
        "  #wandb.watch(npv_model, criterion, log=\"all\", log_freq=10)\n",
        "  epochs = config.epochs\n",
        "  npk_input = config.npk_input\n",
        "  npv_input = config.npv_input\n",
        "  npk_initialization = config.npk_initialization\n",
        "  npv_initialization = config.npv_initialization\n",
        "  npk_trainability = config.npk_trainability\n",
        "  npv_trainability = config.npv_trainability\n",
        "\n",
        "  if npk_input == \"original\":\n",
        "    npk_train_dataloader = cifar_train_dataloader\n",
        "  else:\n",
        "    npk_train_dataloader = cifar_train_dummy_dataloader\n",
        "  \n",
        "  if npv_input == \"original\":\n",
        "    npv_train_dataloader = cifar_train_dataloader\n",
        "  else:\n",
        "    npv_train_dataloader = cifar_train_dummy_dataloader\n",
        "\n",
        "  if npk_initialization == \"pre_trained\":\n",
        "    npk_model.load_state_dict(conv_model.state_dict())\n",
        "  if npv_initialization == \"npk_initialization\":\n",
        "    npv_model.load_state_dict(npk_model.state_dict())\n",
        "  if npk_trainability == \"non_trainable\":\n",
        "    for param in npk_model.parameters():\n",
        "      param.requires_grad = False\n",
        "  if npv_trainability == \"non_trainable\":\n",
        "    for param in npv_model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    pred = train_decoupled(npk_train_dataloader,npv_train_dataloader,npk_model, npv_model, criterion, optimizer)\n",
        "    test_decoupled(\"training\",cifar_train_dataloader,npk_model, npv_model, criterion)\n",
        "    test_decoupled(\"test\",cifar_test_dataloader,npk_model, npv_model, criterion)\n",
        "  print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tKAtZQd_7oq"
      },
      "source": [
        "def run_pipeline():\n",
        "    global config\n",
        "    config = dotdict(config)\n",
        "    # tell wandb to get started\n",
        "    if is_wandb_active:\n",
        "      wandb.init()\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "    npk_model, npv_model, criterion, optimizer = make(config)\n",
        "    train(npk_model, npv_model, criterion, optimizer, config)\n",
        "\n",
        "    return npv_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19d_lia721vZ"
      },
      "source": [
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9dOqGVAids"
      },
      "source": [
        "\n",
        "sweep_config = {\n",
        "    'name' : 'npf_npv infinite width cifar sweep',\n",
        "    \"method\": \"grid\",\n",
        "    'early_terminate':{\n",
        "        'type': 'hyperband',\n",
        "        'min_iter': 2,\n",
        "        'eta' : 5\n",
        "        },\n",
        "    'metric': { \n",
        "        'name':'test_acc',\n",
        "        'goal': 'maximize',\n",
        "        },\n",
        "    'parameters':{\n",
        "        'npk_input': {'values' : ['original']},\n",
        "        'npv_input' : {'values' : ['original']},\n",
        "        'npk_initialization':{'values' : ['random']},\n",
        "        'npv_initialization' : {'values' : ['random']},\n",
        "        'npk_trainability' : {'values' : ['non_trainable']},\n",
        "        'npv_trainability' : {'values': ['trainable']},\n",
        "        'activation':{'values' : [ 'hard']},\n",
        "        'learning_rate':{'values' : [3e-4]},\n",
        "        'epochs' : {'values' : [100]},\n",
        "        }\n",
        "}\n",
        "\n",
        "config = dict(\n",
        "    npk_input = ['original'],\n",
        "    npv_input = ['original'],\n",
        "    npk_initialization = ['pre_trained'],\n",
        "    npv_initialization = ['random'],\n",
        "    npk_trainability = ['non_trainable'],\n",
        "    npv_trainability = ['trainable'],\n",
        "    activation = ['soft'],\n",
        "    learning_rate = 3e-4,\n",
        "    epochs = 30\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNzchPXBG_kj"
      },
      "source": [
        "is_wandb_active = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TjUt3KyzOwDu",
        "outputId": "b484785e-92e4-4ff9-b9ca-4459fcd1d1aa"
      },
      "source": [
        "sweepId = wandb.sweep(sweep_config,entity = \"-my\",project = \"npf_npv_experiments\")\n",
        "wandb.agent(sweepId,function=run_pipeline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: e9j3tc0v\n",
            "Sweep URL: https://wandb.ai/-my/npf_npv_experiments/sweeps/e9j3tc0v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g2s6kuv6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: hard\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnpk_initialization: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnpk_input: original\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnpk_trainability: non_trainable\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnpv_initialization: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnpv_input: original\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnpv_trainability: trainable\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/-my/npf_npv_experiments/runs/g2s6kuv6\" target=\"_blank\">warm-sweep-1</a></strong> to <a href=\"https://wandb.ai/-my/npf_npv_experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/-my/npf_npv_experiments/sweeps/e9j3tc0v\" target=\"_blank\">https://wandb.ai/-my/npf_npv_experiments/sweeps/e9j3tc0v</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.299113 Batch:0 [    0/50000]\n",
            "loss: 1.946852 Batch:100 [ 3200/50000]\n",
            "loss: 1.696809 Batch:200 [ 6400/50000]\n",
            "loss: 1.773451 Batch:300 [ 9600/50000]\n",
            "loss: 1.075360 Batch:400 [12800/50000]\n",
            "loss: 1.559173 Batch:500 [16000/50000]\n",
            "loss: 1.230091 Batch:600 [19200/50000]\n",
            "loss: 1.084709 Batch:700 [22400/50000]\n",
            "loss: 1.442473 Batch:800 [25600/50000]\n",
            "loss: 1.475242 Batch:900 [28800/50000]\n",
            "loss: 1.123749 Batch:1000 [32000/50000]\n",
            "loss: 1.168030 Batch:1100 [35200/50000]\n",
            "loss: 1.226458 Batch:1200 [38400/50000]\n",
            "loss: 0.910160 Batch:1300 [41600/50000]\n",
            "loss: 0.887754 Batch:1400 [44800/50000]\n",
            "loss: 0.982789 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.058750 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 60.5%, Avg loss: 1.123510 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.186103 Batch:0 [    0/50000]\n",
            "loss: 0.841597 Batch:100 [ 3200/50000]\n",
            "loss: 1.451695 Batch:200 [ 6400/50000]\n",
            "loss: 1.015056 Batch:300 [ 9600/50000]\n",
            "loss: 0.719295 Batch:400 [12800/50000]\n",
            "loss: 0.628943 Batch:500 [16000/50000]\n",
            "loss: 0.937810 Batch:600 [19200/50000]\n",
            "loss: 1.022478 Batch:700 [22400/50000]\n",
            "loss: 0.959111 Batch:800 [25600/50000]\n",
            "loss: 0.764656 Batch:900 [28800/50000]\n",
            "loss: 0.970096 Batch:1000 [32000/50000]\n",
            "loss: 0.783078 Batch:1100 [35200/50000]\n",
            "loss: 1.052349 Batch:1200 [38400/50000]\n",
            "loss: 0.653491 Batch:1300 [41600/50000]\n",
            "loss: 1.645094 Batch:1400 [44800/50000]\n",
            "loss: 0.955273 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 68.1%, Avg loss: 0.904961 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 1.057970 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.963170 Batch:0 [    0/50000]\n",
            "loss: 0.907093 Batch:100 [ 3200/50000]\n",
            "loss: 0.600167 Batch:200 [ 6400/50000]\n",
            "loss: 1.014089 Batch:300 [ 9600/50000]\n",
            "loss: 0.863164 Batch:400 [12800/50000]\n",
            "loss: 1.021623 Batch:500 [16000/50000]\n",
            "loss: 0.668111 Batch:600 [19200/50000]\n",
            "loss: 0.576928 Batch:700 [22400/50000]\n",
            "loss: 0.811338 Batch:800 [25600/50000]\n",
            "loss: 0.805526 Batch:900 [28800/50000]\n",
            "loss: 0.738914 Batch:1000 [32000/50000]\n",
            "loss: 0.704473 Batch:1100 [35200/50000]\n",
            "loss: 0.890875 Batch:1200 [38400/50000]\n",
            "loss: 1.023239 Batch:1300 [41600/50000]\n",
            "loss: 0.606431 Batch:1400 [44800/50000]\n",
            "loss: 1.051242 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.666393 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.920458 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.507484 Batch:0 [    0/50000]\n",
            "loss: 0.630244 Batch:100 [ 3200/50000]\n",
            "loss: 1.011620 Batch:200 [ 6400/50000]\n",
            "loss: 0.897346 Batch:300 [ 9600/50000]\n",
            "loss: 0.698390 Batch:400 [12800/50000]\n",
            "loss: 0.604460 Batch:500 [16000/50000]\n",
            "loss: 1.222669 Batch:600 [19200/50000]\n",
            "loss: 0.936770 Batch:700 [22400/50000]\n",
            "loss: 0.603893 Batch:800 [25600/50000]\n",
            "loss: 0.374627 Batch:900 [28800/50000]\n",
            "loss: 1.185610 Batch:1000 [32000/50000]\n",
            "loss: 0.956913 Batch:1100 [35200/50000]\n",
            "loss: 0.441190 Batch:1200 [38400/50000]\n",
            "loss: 0.357683 Batch:1300 [41600/50000]\n",
            "loss: 0.607949 Batch:1400 [44800/50000]\n",
            "loss: 0.489247 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.537637 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 0.924979 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.585341 Batch:0 [    0/50000]\n",
            "loss: 0.646728 Batch:100 [ 3200/50000]\n",
            "loss: 0.453845 Batch:200 [ 6400/50000]\n",
            "loss: 0.921905 Batch:300 [ 9600/50000]\n",
            "loss: 0.287083 Batch:400 [12800/50000]\n",
            "loss: 0.525859 Batch:500 [16000/50000]\n",
            "loss: 0.702884 Batch:600 [19200/50000]\n",
            "loss: 0.776506 Batch:700 [22400/50000]\n",
            "loss: 0.467004 Batch:800 [25600/50000]\n",
            "loss: 0.506512 Batch:900 [28800/50000]\n",
            "loss: 0.281732 Batch:1000 [32000/50000]\n",
            "loss: 0.451901 Batch:1100 [35200/50000]\n",
            "loss: 0.379594 Batch:1200 [38400/50000]\n",
            "loss: 0.379555 Batch:1300 [41600/50000]\n",
            "loss: 0.651558 Batch:1400 [44800/50000]\n",
            "loss: 0.521089 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.408193 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.896766 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.262510 Batch:0 [    0/50000]\n",
            "loss: 0.542360 Batch:100 [ 3200/50000]\n",
            "loss: 0.206679 Batch:200 [ 6400/50000]\n",
            "loss: 0.276480 Batch:300 [ 9600/50000]\n",
            "loss: 0.381943 Batch:400 [12800/50000]\n",
            "loss: 0.390446 Batch:500 [16000/50000]\n",
            "loss: 0.501628 Batch:600 [19200/50000]\n",
            "loss: 0.424673 Batch:700 [22400/50000]\n",
            "loss: 0.455041 Batch:800 [25600/50000]\n",
            "loss: 0.599922 Batch:900 [28800/50000]\n",
            "loss: 0.943062 Batch:1000 [32000/50000]\n",
            "loss: 0.324618 Batch:1100 [35200/50000]\n",
            "loss: 0.452789 Batch:1200 [38400/50000]\n",
            "loss: 0.498817 Batch:1300 [41600/50000]\n",
            "loss: 0.659942 Batch:1400 [44800/50000]\n",
            "loss: 0.598883 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 88.5%, Avg loss: 0.336886 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 1.011759 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.441366 Batch:0 [    0/50000]\n",
            "loss: 0.279852 Batch:100 [ 3200/50000]\n",
            "loss: 0.262723 Batch:200 [ 6400/50000]\n",
            "loss: 0.263478 Batch:300 [ 9600/50000]\n",
            "loss: 0.420215 Batch:400 [12800/50000]\n",
            "loss: 0.360799 Batch:500 [16000/50000]\n",
            "loss: 0.231680 Batch:600 [19200/50000]\n",
            "loss: 0.402458 Batch:700 [22400/50000]\n",
            "loss: 0.631354 Batch:800 [25600/50000]\n",
            "loss: 0.600221 Batch:900 [28800/50000]\n",
            "loss: 0.356234 Batch:1000 [32000/50000]\n",
            "loss: 0.164330 Batch:1100 [35200/50000]\n",
            "loss: 0.316590 Batch:1200 [38400/50000]\n",
            "loss: 0.264681 Batch:1300 [41600/50000]\n",
            "loss: 0.204714 Batch:1400 [44800/50000]\n",
            "loss: 0.458297 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 90.1%, Avg loss: 0.288914 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 1.132791 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.313979 Batch:0 [    0/50000]\n",
            "loss: 0.511510 Batch:100 [ 3200/50000]\n",
            "loss: 0.301852 Batch:200 [ 6400/50000]\n",
            "loss: 0.380745 Batch:300 [ 9600/50000]\n",
            "loss: 0.278386 Batch:400 [12800/50000]\n",
            "loss: 0.373416 Batch:500 [16000/50000]\n",
            "loss: 0.446676 Batch:600 [19200/50000]\n",
            "loss: 0.530956 Batch:700 [22400/50000]\n",
            "loss: 0.573253 Batch:800 [25600/50000]\n",
            "loss: 0.402973 Batch:900 [28800/50000]\n",
            "loss: 0.188392 Batch:1000 [32000/50000]\n",
            "loss: 0.372239 Batch:1100 [35200/50000]\n",
            "loss: 0.808946 Batch:1200 [38400/50000]\n",
            "loss: 0.332797 Batch:1300 [41600/50000]\n",
            "loss: 0.392824 Batch:1400 [44800/50000]\n",
            "loss: 0.081019 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.239946 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 1.313053 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.114343 Batch:0 [    0/50000]\n",
            "loss: 0.411637 Batch:100 [ 3200/50000]\n",
            "loss: 0.108060 Batch:200 [ 6400/50000]\n",
            "loss: 0.390919 Batch:300 [ 9600/50000]\n",
            "loss: 0.174160 Batch:400 [12800/50000]\n",
            "loss: 0.335546 Batch:500 [16000/50000]\n",
            "loss: 0.216595 Batch:600 [19200/50000]\n",
            "loss: 0.363399 Batch:700 [22400/50000]\n",
            "loss: 0.347218 Batch:800 [25600/50000]\n",
            "loss: 0.068033 Batch:900 [28800/50000]\n",
            "loss: 0.063714 Batch:1000 [32000/50000]\n",
            "loss: 0.388920 Batch:1100 [35200/50000]\n",
            "loss: 0.175689 Batch:1200 [38400/50000]\n",
            "loss: 0.185344 Batch:1300 [41600/50000]\n",
            "loss: 0.819732 Batch:1400 [44800/50000]\n",
            "loss: 0.195975 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 94.4%, Avg loss: 0.162053 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.6%, Avg loss: 1.335175 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.232679 Batch:0 [    0/50000]\n",
            "loss: 0.114174 Batch:100 [ 3200/50000]\n",
            "loss: 0.068998 Batch:200 [ 6400/50000]\n",
            "loss: 0.069571 Batch:300 [ 9600/50000]\n",
            "loss: 0.235950 Batch:400 [12800/50000]\n",
            "loss: 0.085749 Batch:500 [16000/50000]\n",
            "loss: 0.099334 Batch:600 [19200/50000]\n",
            "loss: 0.350187 Batch:700 [22400/50000]\n",
            "loss: 0.103829 Batch:800 [25600/50000]\n",
            "loss: 0.403118 Batch:900 [28800/50000]\n",
            "loss: 0.125630 Batch:1000 [32000/50000]\n",
            "loss: 0.281137 Batch:1100 [35200/50000]\n",
            "loss: 0.223843 Batch:1200 [38400/50000]\n",
            "loss: 0.206853 Batch:1300 [41600/50000]\n",
            "loss: 0.490638 Batch:1400 [44800/50000]\n",
            "loss: 0.228349 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.183806 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 1.600881 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.274095 Batch:0 [    0/50000]\n",
            "loss: 0.111104 Batch:100 [ 3200/50000]\n",
            "loss: 0.134327 Batch:200 [ 6400/50000]\n",
            "loss: 0.055951 Batch:300 [ 9600/50000]\n",
            "loss: 0.172285 Batch:400 [12800/50000]\n",
            "loss: 0.079449 Batch:500 [16000/50000]\n",
            "loss: 0.269194 Batch:600 [19200/50000]\n",
            "loss: 0.219862 Batch:700 [22400/50000]\n",
            "loss: 0.079274 Batch:800 [25600/50000]\n",
            "loss: 0.301818 Batch:900 [28800/50000]\n",
            "loss: 0.159924 Batch:1000 [32000/50000]\n",
            "loss: 0.008202 Batch:1100 [35200/50000]\n",
            "loss: 0.279799 Batch:1200 [38400/50000]\n",
            "loss: 0.065173 Batch:1300 [41600/50000]\n",
            "loss: 0.200146 Batch:1400 [44800/50000]\n",
            "loss: 0.456419 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.143157 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 1.708318 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.165443 Batch:0 [    0/50000]\n",
            "loss: 0.162652 Batch:100 [ 3200/50000]\n",
            "loss: 0.109582 Batch:200 [ 6400/50000]\n",
            "loss: 0.245524 Batch:300 [ 9600/50000]\n",
            "loss: 0.173240 Batch:400 [12800/50000]\n",
            "loss: 0.089842 Batch:500 [16000/50000]\n",
            "loss: 0.027499 Batch:600 [19200/50000]\n",
            "loss: 0.104077 Batch:700 [22400/50000]\n",
            "loss: 0.033601 Batch:800 [25600/50000]\n",
            "loss: 0.092015 Batch:900 [28800/50000]\n",
            "loss: 0.169460 Batch:1000 [32000/50000]\n",
            "loss: 0.475513 Batch:1100 [35200/50000]\n",
            "loss: 0.097768 Batch:1200 [38400/50000]\n",
            "loss: 0.507410 Batch:1300 [41600/50000]\n",
            "loss: 0.303206 Batch:1400 [44800/50000]\n",
            "loss: 0.060191 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.161161 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 1.993488 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.208539 Batch:0 [    0/50000]\n",
            "loss: 0.153681 Batch:100 [ 3200/50000]\n",
            "loss: 0.153824 Batch:200 [ 6400/50000]\n",
            "loss: 0.141133 Batch:300 [ 9600/50000]\n",
            "loss: 0.088625 Batch:400 [12800/50000]\n",
            "loss: 0.080641 Batch:500 [16000/50000]\n",
            "loss: 0.099369 Batch:600 [19200/50000]\n",
            "loss: 0.365116 Batch:700 [22400/50000]\n",
            "loss: 0.370253 Batch:800 [25600/50000]\n",
            "loss: 0.236067 Batch:900 [28800/50000]\n",
            "loss: 0.311488 Batch:1000 [32000/50000]\n",
            "loss: 0.898647 Batch:1100 [35200/50000]\n",
            "loss: 0.051365 Batch:1200 [38400/50000]\n",
            "loss: 0.457441 Batch:1300 [41600/50000]\n",
            "loss: 0.253349 Batch:1400 [44800/50000]\n",
            "loss: 0.224881 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.170260 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 2.215273 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.025132 Batch:0 [    0/50000]\n",
            "loss: 0.216512 Batch:100 [ 3200/50000]\n",
            "loss: 0.074894 Batch:200 [ 6400/50000]\n",
            "loss: 0.016822 Batch:300 [ 9600/50000]\n",
            "loss: 0.084657 Batch:400 [12800/50000]\n",
            "loss: 0.038137 Batch:500 [16000/50000]\n",
            "loss: 0.144211 Batch:600 [19200/50000]\n",
            "loss: 0.275459 Batch:700 [22400/50000]\n",
            "loss: 0.120691 Batch:800 [25600/50000]\n",
            "loss: 0.097755 Batch:900 [28800/50000]\n",
            "loss: 0.039321 Batch:1000 [32000/50000]\n",
            "loss: 0.370566 Batch:1100 [35200/50000]\n",
            "loss: 0.139905 Batch:1200 [38400/50000]\n",
            "loss: 0.109767 Batch:1300 [41600/50000]\n",
            "loss: 0.050353 Batch:1400 [44800/50000]\n",
            "loss: 0.550551 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.127244 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 2.418271 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.019150 Batch:0 [    0/50000]\n",
            "loss: 0.002009 Batch:100 [ 3200/50000]\n",
            "loss: 0.108256 Batch:200 [ 6400/50000]\n",
            "loss: 0.054370 Batch:300 [ 9600/50000]\n",
            "loss: 0.185833 Batch:400 [12800/50000]\n",
            "loss: 0.069993 Batch:500 [16000/50000]\n",
            "loss: 0.608955 Batch:600 [19200/50000]\n",
            "loss: 0.031782 Batch:700 [22400/50000]\n",
            "loss: 0.208775 Batch:800 [25600/50000]\n",
            "loss: 0.134804 Batch:900 [28800/50000]\n",
            "loss: 0.015791 Batch:1000 [32000/50000]\n",
            "loss: 0.020751 Batch:1100 [35200/50000]\n",
            "loss: 0.016703 Batch:1200 [38400/50000]\n",
            "loss: 0.023659 Batch:1300 [41600/50000]\n",
            "loss: 0.149868 Batch:1400 [44800/50000]\n",
            "loss: 0.109584 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.165253 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 2.644725 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.100184 Batch:0 [    0/50000]\n",
            "loss: 0.107761 Batch:100 [ 3200/50000]\n",
            "loss: 0.070252 Batch:200 [ 6400/50000]\n",
            "loss: 0.110631 Batch:300 [ 9600/50000]\n",
            "loss: 0.278069 Batch:400 [12800/50000]\n",
            "loss: 0.146660 Batch:500 [16000/50000]\n",
            "loss: 0.131741 Batch:600 [19200/50000]\n",
            "loss: 0.121101 Batch:700 [22400/50000]\n",
            "loss: 0.030613 Batch:800 [25600/50000]\n",
            "loss: 0.236677 Batch:900 [28800/50000]\n",
            "loss: 0.116871 Batch:1000 [32000/50000]\n",
            "loss: 0.715863 Batch:1100 [35200/50000]\n",
            "loss: 0.435680 Batch:1200 [38400/50000]\n",
            "loss: 0.300482 Batch:1300 [41600/50000]\n",
            "loss: 0.005668 Batch:1400 [44800/50000]\n",
            "loss: 0.063265 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.150828 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 2.820867 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.019150 Batch:0 [    0/50000]\n",
            "loss: 0.066969 Batch:100 [ 3200/50000]\n",
            "loss: 0.001813 Batch:200 [ 6400/50000]\n",
            "loss: 0.125768 Batch:300 [ 9600/50000]\n",
            "loss: 0.113191 Batch:400 [12800/50000]\n",
            "loss: 0.008419 Batch:500 [16000/50000]\n",
            "loss: 0.512653 Batch:600 [19200/50000]\n",
            "loss: 0.053181 Batch:700 [22400/50000]\n",
            "loss: 0.242534 Batch:800 [25600/50000]\n",
            "loss: 0.236106 Batch:900 [28800/50000]\n",
            "loss: 0.370786 Batch:1000 [32000/50000]\n",
            "loss: 0.353914 Batch:1100 [35200/50000]\n",
            "loss: 0.051729 Batch:1200 [38400/50000]\n",
            "loss: 0.411181 Batch:1300 [41600/50000]\n",
            "loss: 0.065772 Batch:1400 [44800/50000]\n",
            "loss: 0.081821 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.133210 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.9%, Avg loss: 2.979174 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.511687 Batch:0 [    0/50000]\n",
            "loss: 0.090783 Batch:100 [ 3200/50000]\n",
            "loss: 0.116433 Batch:200 [ 6400/50000]\n",
            "loss: 0.012390 Batch:300 [ 9600/50000]\n",
            "loss: 0.070887 Batch:400 [12800/50000]\n",
            "loss: 0.139365 Batch:500 [16000/50000]\n",
            "loss: 0.381846 Batch:600 [19200/50000]\n",
            "loss: 0.464573 Batch:700 [22400/50000]\n",
            "loss: 0.205604 Batch:800 [25600/50000]\n",
            "loss: 0.075823 Batch:900 [28800/50000]\n",
            "loss: 0.300223 Batch:1000 [32000/50000]\n",
            "loss: 0.064750 Batch:1100 [35200/50000]\n",
            "loss: 0.022316 Batch:1200 [38400/50000]\n",
            "loss: 0.019210 Batch:1300 [41600/50000]\n",
            "loss: 0.111862 Batch:1400 [44800/50000]\n",
            "loss: 0.001361 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.156535 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 3.353146 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.138112 Batch:0 [    0/50000]\n",
            "loss: 0.003161 Batch:100 [ 3200/50000]\n",
            "loss: 0.017516 Batch:200 [ 6400/50000]\n",
            "loss: 0.123056 Batch:300 [ 9600/50000]\n",
            "loss: 0.070248 Batch:400 [12800/50000]\n",
            "loss: 0.156774 Batch:500 [16000/50000]\n",
            "loss: 0.002272 Batch:600 [19200/50000]\n",
            "loss: 0.107948 Batch:700 [22400/50000]\n",
            "loss: 0.589085 Batch:800 [25600/50000]\n",
            "loss: 0.207311 Batch:900 [28800/50000]\n",
            "loss: 0.678378 Batch:1000 [32000/50000]\n",
            "loss: 0.224157 Batch:1100 [35200/50000]\n",
            "loss: 0.307994 Batch:1200 [38400/50000]\n",
            "loss: 0.032033 Batch:1300 [41600/50000]\n",
            "loss: 0.030616 Batch:1400 [44800/50000]\n",
            "loss: 0.137282 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.202320 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 3.488113 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.004935 Batch:0 [    0/50000]\n",
            "loss: 0.061671 Batch:100 [ 3200/50000]\n",
            "loss: 0.312088 Batch:200 [ 6400/50000]\n",
            "loss: 0.020927 Batch:300 [ 9600/50000]\n",
            "loss: 0.342097 Batch:400 [12800/50000]\n",
            "loss: 0.000744 Batch:500 [16000/50000]\n",
            "loss: 0.201898 Batch:600 [19200/50000]\n",
            "loss: 0.157904 Batch:700 [22400/50000]\n",
            "loss: 0.001989 Batch:800 [25600/50000]\n",
            "loss: 0.110504 Batch:900 [28800/50000]\n",
            "loss: 0.313161 Batch:1000 [32000/50000]\n",
            "loss: 0.035742 Batch:1100 [35200/50000]\n",
            "loss: 0.003424 Batch:1200 [38400/50000]\n",
            "loss: 0.110378 Batch:1300 [41600/50000]\n",
            "loss: 0.627396 Batch:1400 [44800/50000]\n",
            "loss: 0.058530 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.132094 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 3.520036 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.000248 Batch:0 [    0/50000]\n",
            "loss: 0.074383 Batch:100 [ 3200/50000]\n",
            "loss: 0.025021 Batch:200 [ 6400/50000]\n",
            "loss: 0.011131 Batch:300 [ 9600/50000]\n",
            "loss: 0.006417 Batch:400 [12800/50000]\n",
            "loss: 0.038082 Batch:500 [16000/50000]\n",
            "loss: 0.260360 Batch:600 [19200/50000]\n",
            "loss: 0.022776 Batch:700 [22400/50000]\n",
            "loss: 0.119583 Batch:800 [25600/50000]\n",
            "loss: 0.146040 Batch:900 [28800/50000]\n",
            "loss: 0.000672 Batch:1000 [32000/50000]\n",
            "loss: 0.006749 Batch:1100 [35200/50000]\n",
            "loss: 0.000217 Batch:1200 [38400/50000]\n",
            "loss: 0.232250 Batch:1300 [41600/50000]\n",
            "loss: 0.232847 Batch:1400 [44800/50000]\n",
            "loss: 0.079376 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.089650 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 3.621147 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.094172 Batch:0 [    0/50000]\n",
            "loss: 0.082261 Batch:100 [ 3200/50000]\n",
            "loss: 0.202334 Batch:200 [ 6400/50000]\n",
            "loss: 1.594944 Batch:300 [ 9600/50000]\n",
            "loss: 0.151905 Batch:400 [12800/50000]\n",
            "loss: 0.004549 Batch:500 [16000/50000]\n",
            "loss: 2.343152 Batch:600 [19200/50000]\n",
            "loss: 0.498949 Batch:700 [22400/50000]\n",
            "loss: 0.047724 Batch:800 [25600/50000]\n",
            "loss: 0.817136 Batch:900 [28800/50000]\n",
            "loss: 0.518470 Batch:1000 [32000/50000]\n",
            "loss: 0.131312 Batch:1100 [35200/50000]\n",
            "loss: 0.002249 Batch:1200 [38400/50000]\n",
            "loss: 0.320827 Batch:1300 [41600/50000]\n",
            "loss: 0.045133 Batch:1400 [44800/50000]\n",
            "loss: 0.602805 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 0.133218 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 4.235403 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.027894 Batch:0 [    0/50000]\n",
            "loss: 0.063926 Batch:100 [ 3200/50000]\n",
            "loss: 0.074143 Batch:200 [ 6400/50000]\n",
            "loss: 0.129799 Batch:300 [ 9600/50000]\n",
            "loss: 0.217553 Batch:400 [12800/50000]\n",
            "loss: 0.031861 Batch:500 [16000/50000]\n",
            "loss: 0.083805 Batch:600 [19200/50000]\n",
            "loss: 0.665696 Batch:700 [22400/50000]\n",
            "loss: 0.077788 Batch:800 [25600/50000]\n",
            "loss: 0.122981 Batch:900 [28800/50000]\n",
            "loss: 0.246439 Batch:1000 [32000/50000]\n",
            "loss: 0.703340 Batch:1100 [35200/50000]\n",
            "loss: 0.083952 Batch:1200 [38400/50000]\n",
            "loss: 0.210278 Batch:1300 [41600/50000]\n",
            "loss: 0.029818 Batch:1400 [44800/50000]\n",
            "loss: 0.000400 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.103652 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.1%, Avg loss: 4.227514 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.000031 Batch:0 [    0/50000]\n",
            "loss: 0.395299 Batch:100 [ 3200/50000]\n",
            "loss: 0.005039 Batch:200 [ 6400/50000]\n",
            "loss: 0.042437 Batch:300 [ 9600/50000]\n",
            "loss: 0.004046 Batch:400 [12800/50000]\n",
            "loss: 0.628655 Batch:500 [16000/50000]\n",
            "loss: 0.103423 Batch:600 [19200/50000]\n",
            "loss: 0.001834 Batch:700 [22400/50000]\n",
            "loss: 0.210782 Batch:800 [25600/50000]\n",
            "loss: 0.235666 Batch:900 [28800/50000]\n",
            "loss: 0.305240 Batch:1000 [32000/50000]\n",
            "loss: 0.431260 Batch:1100 [35200/50000]\n",
            "loss: 0.256401 Batch:1200 [38400/50000]\n",
            "loss: 0.058264 Batch:1300 [41600/50000]\n",
            "loss: 1.078936 Batch:1400 [44800/50000]\n",
            "loss: 0.011370 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.112724 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.6%, Avg loss: 4.571797 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.246816 Batch:0 [    0/50000]\n",
            "loss: 0.012554 Batch:100 [ 3200/50000]\n",
            "loss: 0.033218 Batch:200 [ 6400/50000]\n",
            "loss: 0.030523 Batch:300 [ 9600/50000]\n",
            "loss: 0.000227 Batch:400 [12800/50000]\n",
            "loss: 0.029290 Batch:500 [16000/50000]\n",
            "loss: 0.005351 Batch:600 [19200/50000]\n",
            "loss: 0.126201 Batch:700 [22400/50000]\n",
            "loss: 0.260413 Batch:800 [25600/50000]\n",
            "loss: 0.447375 Batch:900 [28800/50000]\n",
            "loss: 0.289885 Batch:1000 [32000/50000]\n",
            "loss: 0.032033 Batch:1100 [35200/50000]\n",
            "loss: 0.018451 Batch:1200 [38400/50000]\n",
            "loss: 0.194878 Batch:1300 [41600/50000]\n",
            "loss: 0.054125 Batch:1400 [44800/50000]\n",
            "loss: 0.271978 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.6%, Avg loss: 0.195901 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.3%, Avg loss: 5.277209 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.369379 Batch:0 [    0/50000]\n",
            "loss: 0.003135 Batch:100 [ 3200/50000]\n",
            "loss: 0.012463 Batch:200 [ 6400/50000]\n",
            "loss: 0.000137 Batch:300 [ 9600/50000]\n",
            "loss: 0.006574 Batch:400 [12800/50000]\n",
            "loss: 0.016040 Batch:500 [16000/50000]\n",
            "loss: 0.585028 Batch:600 [19200/50000]\n",
            "loss: 0.098749 Batch:700 [22400/50000]\n",
            "loss: 0.052431 Batch:800 [25600/50000]\n",
            "loss: 0.012162 Batch:900 [28800/50000]\n",
            "loss: 0.143772 Batch:1000 [32000/50000]\n",
            "loss: 0.003166 Batch:1100 [35200/50000]\n",
            "loss: 0.239835 Batch:1200 [38400/50000]\n",
            "loss: 0.437875 Batch:1300 [41600/50000]\n",
            "loss: 0.185298 Batch:1400 [44800/50000]\n",
            "loss: 0.242401 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.216874 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 5.630820 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.169958 Batch:0 [    0/50000]\n",
            "loss: 0.056245 Batch:100 [ 3200/50000]\n",
            "loss: 0.231567 Batch:200 [ 6400/50000]\n",
            "loss: 0.302865 Batch:300 [ 9600/50000]\n",
            "loss: 0.020117 Batch:400 [12800/50000]\n",
            "loss: 0.099388 Batch:500 [16000/50000]\n",
            "loss: 0.000511 Batch:600 [19200/50000]\n",
            "loss: 0.003350 Batch:700 [22400/50000]\n",
            "loss: 0.012067 Batch:800 [25600/50000]\n",
            "loss: 0.091604 Batch:900 [28800/50000]\n",
            "loss: 0.162127 Batch:1000 [32000/50000]\n",
            "loss: 0.002456 Batch:1100 [35200/50000]\n",
            "loss: 0.016024 Batch:1200 [38400/50000]\n",
            "loss: 0.368066 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.048708 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.156225 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 5.687746 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.000037 Batch:0 [    0/50000]\n",
            "loss: 1.351504 Batch:100 [ 3200/50000]\n",
            "loss: 0.003449 Batch:200 [ 6400/50000]\n",
            "loss: 0.121665 Batch:300 [ 9600/50000]\n",
            "loss: 0.012652 Batch:400 [12800/50000]\n",
            "loss: 0.000221 Batch:500 [16000/50000]\n",
            "loss: 0.000136 Batch:600 [19200/50000]\n",
            "loss: 0.044246 Batch:700 [22400/50000]\n",
            "loss: 0.047123 Batch:800 [25600/50000]\n",
            "loss: 0.000225 Batch:900 [28800/50000]\n",
            "loss: 0.004878 Batch:1000 [32000/50000]\n",
            "loss: 0.324554 Batch:1100 [35200/50000]\n",
            "loss: 0.000024 Batch:1200 [38400/50000]\n",
            "loss: 0.001036 Batch:1300 [41600/50000]\n",
            "loss: 0.156553 Batch:1400 [44800/50000]\n",
            "loss: 0.351084 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.120867 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 6.038840 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.029452 Batch:0 [    0/50000]\n",
            "loss: 0.243990 Batch:100 [ 3200/50000]\n",
            "loss: 0.019825 Batch:200 [ 6400/50000]\n",
            "loss: 0.496959 Batch:300 [ 9600/50000]\n",
            "loss: 0.149124 Batch:400 [12800/50000]\n",
            "loss: 0.000215 Batch:500 [16000/50000]\n",
            "loss: 0.524030 Batch:600 [19200/50000]\n",
            "loss: 0.330094 Batch:700 [22400/50000]\n",
            "loss: 0.247739 Batch:800 [25600/50000]\n",
            "loss: 0.226350 Batch:900 [28800/50000]\n",
            "loss: 0.130670 Batch:1000 [32000/50000]\n",
            "loss: 0.000004 Batch:1100 [35200/50000]\n",
            "loss: 0.032645 Batch:1200 [38400/50000]\n",
            "loss: 0.670454 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.345153 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.141022 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 6.667804 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.006154 Batch:0 [    0/50000]\n",
            "loss: 0.003199 Batch:100 [ 3200/50000]\n",
            "loss: 0.505207 Batch:200 [ 6400/50000]\n",
            "loss: 0.211153 Batch:300 [ 9600/50000]\n",
            "loss: 0.021443 Batch:400 [12800/50000]\n",
            "loss: 0.051859 Batch:500 [16000/50000]\n",
            "loss: 0.349642 Batch:600 [19200/50000]\n",
            "loss: 0.000056 Batch:700 [22400/50000]\n",
            "loss: 0.290364 Batch:800 [25600/50000]\n",
            "loss: 0.125905 Batch:900 [28800/50000]\n",
            "loss: 0.097228 Batch:1000 [32000/50000]\n",
            "loss: 0.632724 Batch:1100 [35200/50000]\n",
            "loss: 0.007783 Batch:1200 [38400/50000]\n",
            "loss: 0.014534 Batch:1300 [41600/50000]\n",
            "loss: 0.246579 Batch:1400 [44800/50000]\n",
            "loss: 0.648958 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.327996 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 7.592384 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.082354 Batch:0 [    0/50000]\n",
            "loss: 0.739801 Batch:100 [ 3200/50000]\n",
            "loss: 0.000115 Batch:200 [ 6400/50000]\n",
            "loss: 0.001301 Batch:300 [ 9600/50000]\n",
            "loss: 0.092968 Batch:400 [12800/50000]\n",
            "loss: 0.100509 Batch:500 [16000/50000]\n",
            "loss: 0.152378 Batch:600 [19200/50000]\n",
            "loss: 0.093628 Batch:700 [22400/50000]\n",
            "loss: 0.002263 Batch:800 [25600/50000]\n",
            "loss: 0.011919 Batch:900 [28800/50000]\n",
            "loss: 0.001631 Batch:1000 [32000/50000]\n",
            "loss: 0.044940 Batch:1100 [35200/50000]\n",
            "loss: 0.022300 Batch:1200 [38400/50000]\n",
            "loss: 0.001597 Batch:1300 [41600/50000]\n",
            "loss: 0.000021 Batch:1400 [44800/50000]\n",
            "loss: 0.001499 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.112057 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 7.153066 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.055330 Batch:0 [    0/50000]\n",
            "loss: 0.000015 Batch:100 [ 3200/50000]\n",
            "loss: 0.262447 Batch:200 [ 6400/50000]\n",
            "loss: 0.028750 Batch:300 [ 9600/50000]\n",
            "loss: 1.059653 Batch:400 [12800/50000]\n",
            "loss: 0.011743 Batch:500 [16000/50000]\n",
            "loss: 0.241630 Batch:600 [19200/50000]\n",
            "loss: 0.749168 Batch:700 [22400/50000]\n",
            "loss: 0.954177 Batch:800 [25600/50000]\n",
            "loss: 0.123651 Batch:900 [28800/50000]\n",
            "loss: 1.116274 Batch:1000 [32000/50000]\n",
            "loss: 0.000830 Batch:1100 [35200/50000]\n",
            "loss: 0.664582 Batch:1200 [38400/50000]\n",
            "loss: 0.000140 Batch:1300 [41600/50000]\n",
            "loss: 0.000391 Batch:1400 [44800/50000]\n",
            "loss: 0.329222 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.200475 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 8.090307 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.005693 Batch:0 [    0/50000]\n",
            "loss: 0.000105 Batch:100 [ 3200/50000]\n",
            "loss: 0.238722 Batch:200 [ 6400/50000]\n",
            "loss: 0.194528 Batch:300 [ 9600/50000]\n",
            "loss: 0.102558 Batch:400 [12800/50000]\n",
            "loss: 0.001730 Batch:500 [16000/50000]\n",
            "loss: 0.034420 Batch:600 [19200/50000]\n",
            "loss: 0.304047 Batch:700 [22400/50000]\n",
            "loss: 0.015879 Batch:800 [25600/50000]\n",
            "loss: 0.747193 Batch:900 [28800/50000]\n",
            "loss: 0.192388 Batch:1000 [32000/50000]\n",
            "loss: 0.424538 Batch:1100 [35200/50000]\n",
            "loss: 0.878087 Batch:1200 [38400/50000]\n",
            "loss: 0.797739 Batch:1300 [41600/50000]\n",
            "loss: 1.075236 Batch:1400 [44800/50000]\n",
            "loss: 0.000007 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.481153 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.0%, Avg loss: 10.365281 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.000001 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 0.570370 Batch:200 [ 6400/50000]\n",
            "loss: 0.927211 Batch:300 [ 9600/50000]\n",
            "loss: 0.328338 Batch:400 [12800/50000]\n",
            "loss: 0.000002 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.000005 Batch:700 [22400/50000]\n",
            "loss: 0.006137 Batch:800 [25600/50000]\n",
            "loss: 0.000000 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 0.000001 Batch:1100 [35200/50000]\n",
            "loss: 0.177066 Batch:1200 [38400/50000]\n",
            "loss: 0.000012 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.117645 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.232575 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 9.902148 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.727748 Batch:0 [    0/50000]\n",
            "loss: 0.854798 Batch:100 [ 3200/50000]\n",
            "loss: 0.000415 Batch:200 [ 6400/50000]\n",
            "loss: 0.469377 Batch:300 [ 9600/50000]\n",
            "loss: 0.218546 Batch:400 [12800/50000]\n",
            "loss: 0.176052 Batch:500 [16000/50000]\n",
            "loss: 0.078677 Batch:600 [19200/50000]\n",
            "loss: 0.027216 Batch:700 [22400/50000]\n",
            "loss: 0.008165 Batch:800 [25600/50000]\n",
            "loss: 0.001295 Batch:900 [28800/50000]\n",
            "loss: 0.000003 Batch:1000 [32000/50000]\n",
            "loss: 0.212968 Batch:1100 [35200/50000]\n",
            "loss: 1.136975 Batch:1200 [38400/50000]\n",
            "loss: 0.166855 Batch:1300 [41600/50000]\n",
            "loss: 1.708559 Batch:1400 [44800/50000]\n",
            "loss: 0.650924 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.331735 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.6%, Avg loss: 10.469237 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.350245 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 0.001118 Batch:200 [ 6400/50000]\n",
            "loss: 0.036774 Batch:300 [ 9600/50000]\n",
            "loss: 0.258755 Batch:400 [12800/50000]\n",
            "loss: 0.000710 Batch:500 [16000/50000]\n",
            "loss: 0.741602 Batch:600 [19200/50000]\n",
            "loss: 0.007942 Batch:700 [22400/50000]\n",
            "loss: 0.443387 Batch:800 [25600/50000]\n",
            "loss: 0.075810 Batch:900 [28800/50000]\n",
            "loss: 0.000003 Batch:1000 [32000/50000]\n",
            "loss: 0.000007 Batch:1100 [35200/50000]\n",
            "loss: 0.014420 Batch:1200 [38400/50000]\n",
            "loss: 0.407198 Batch:1300 [41600/50000]\n",
            "loss: 0.049414 Batch:1400 [44800/50000]\n",
            "loss: 0.810290 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.285394 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 11.259899 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.089807 Batch:0 [    0/50000]\n",
            "loss: 1.680969 Batch:100 [ 3200/50000]\n",
            "loss: 2.280259 Batch:200 [ 6400/50000]\n",
            "loss: 0.020523 Batch:300 [ 9600/50000]\n",
            "loss: 0.047690 Batch:400 [12800/50000]\n",
            "loss: 0.029910 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.000000 Batch:700 [22400/50000]\n",
            "loss: 0.129996 Batch:800 [25600/50000]\n",
            "loss: 1.908755 Batch:900 [28800/50000]\n",
            "loss: 0.000105 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 0.000001 Batch:1200 [38400/50000]\n",
            "loss: 0.177600 Batch:1300 [41600/50000]\n",
            "loss: 0.345904 Batch:1400 [44800/50000]\n",
            "loss: 0.000000 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.190548 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 10.851586 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.000388 Batch:0 [    0/50000]\n",
            "loss: 0.000016 Batch:100 [ 3200/50000]\n",
            "loss: 0.560745 Batch:200 [ 6400/50000]\n",
            "loss: 0.136493 Batch:300 [ 9600/50000]\n",
            "loss: 0.000233 Batch:400 [12800/50000]\n",
            "loss: 0.061065 Batch:500 [16000/50000]\n",
            "loss: 0.000196 Batch:600 [19200/50000]\n",
            "loss: 0.013662 Batch:700 [22400/50000]\n",
            "loss: 0.360278 Batch:800 [25600/50000]\n",
            "loss: 0.000356 Batch:900 [28800/50000]\n",
            "loss: 0.190870 Batch:1000 [32000/50000]\n",
            "loss: 0.002447 Batch:1100 [35200/50000]\n",
            "loss: 0.457331 Batch:1200 [38400/50000]\n",
            "loss: 0.037866 Batch:1300 [41600/50000]\n",
            "loss: 0.754787 Batch:1400 [44800/50000]\n",
            "loss: 0.001408 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.347313 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 13.326335 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 1.057830 Batch:0 [    0/50000]\n",
            "loss: 0.000083 Batch:100 [ 3200/50000]\n",
            "loss: 0.252148 Batch:200 [ 6400/50000]\n",
            "loss: 0.850861 Batch:300 [ 9600/50000]\n",
            "loss: 0.139591 Batch:400 [12800/50000]\n",
            "loss: 0.056281 Batch:500 [16000/50000]\n",
            "loss: 0.000027 Batch:600 [19200/50000]\n",
            "loss: 0.000000 Batch:700 [22400/50000]\n",
            "loss: 0.060504 Batch:800 [25600/50000]\n",
            "loss: 0.100358 Batch:900 [28800/50000]\n",
            "loss: 0.687056 Batch:1000 [32000/50000]\n",
            "loss: 0.000001 Batch:1100 [35200/50000]\n",
            "loss: 0.009406 Batch:1200 [38400/50000]\n",
            "loss: 0.493244 Batch:1300 [41600/50000]\n",
            "loss: 1.751167 Batch:1400 [44800/50000]\n",
            "loss: 0.058472 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.290539 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 13.740254 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.075733 Batch:0 [    0/50000]\n",
            "loss: 1.002186 Batch:100 [ 3200/50000]\n",
            "loss: 0.000000 Batch:200 [ 6400/50000]\n",
            "loss: 0.192309 Batch:300 [ 9600/50000]\n",
            "loss: 0.134161 Batch:400 [12800/50000]\n",
            "loss: 0.284910 Batch:500 [16000/50000]\n",
            "loss: 0.069359 Batch:600 [19200/50000]\n",
            "loss: 0.366611 Batch:700 [22400/50000]\n",
            "loss: 0.142940 Batch:800 [25600/50000]\n",
            "loss: 0.076966 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 0.414775 Batch:1100 [35200/50000]\n",
            "loss: 0.427272 Batch:1200 [38400/50000]\n",
            "loss: 0.313144 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.000041 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.366497 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 14.341607 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.567566 Batch:0 [    0/50000]\n",
            "loss: 0.594693 Batch:100 [ 3200/50000]\n",
            "loss: 0.025968 Batch:200 [ 6400/50000]\n",
            "loss: 0.000000 Batch:300 [ 9600/50000]\n",
            "loss: 0.004359 Batch:400 [12800/50000]\n",
            "loss: 0.695390 Batch:500 [16000/50000]\n",
            "loss: 0.000027 Batch:600 [19200/50000]\n",
            "loss: 0.000026 Batch:700 [22400/50000]\n",
            "loss: 0.255372 Batch:800 [25600/50000]\n",
            "loss: 0.001929 Batch:900 [28800/50000]\n",
            "loss: 0.317321 Batch:1000 [32000/50000]\n",
            "loss: 0.180559 Batch:1100 [35200/50000]\n",
            "loss: 0.000000 Batch:1200 [38400/50000]\n",
            "loss: 0.356168 Batch:1300 [41600/50000]\n",
            "loss: 0.000001 Batch:1400 [44800/50000]\n",
            "loss: 0.802247 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.169219 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 14.085970 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.000221 Batch:0 [    0/50000]\n",
            "loss: 0.009962 Batch:100 [ 3200/50000]\n",
            "loss: 0.320769 Batch:200 [ 6400/50000]\n",
            "loss: 0.000003 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.250643 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.216984 Batch:700 [22400/50000]\n",
            "loss: 1.710140 Batch:800 [25600/50000]\n",
            "loss: 0.000000 Batch:900 [28800/50000]\n",
            "loss: 1.207932 Batch:1000 [32000/50000]\n",
            "loss: 0.000019 Batch:1100 [35200/50000]\n",
            "loss: 0.350020 Batch:1200 [38400/50000]\n",
            "loss: 0.131368 Batch:1300 [41600/50000]\n",
            "loss: 0.522532 Batch:1400 [44800/50000]\n",
            "loss: 0.161287 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.244534 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 16.290996 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.010238 Batch:0 [    0/50000]\n",
            "loss: 0.000002 Batch:100 [ 3200/50000]\n",
            "loss: 0.012347 Batch:200 [ 6400/50000]\n",
            "loss: 0.001196 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.000000 Batch:500 [16000/50000]\n",
            "loss: 0.063057 Batch:600 [19200/50000]\n",
            "loss: 0.000000 Batch:700 [22400/50000]\n",
            "loss: 0.000005 Batch:800 [25600/50000]\n",
            "loss: 2.241189 Batch:900 [28800/50000]\n",
            "loss: 0.232364 Batch:1000 [32000/50000]\n",
            "loss: 0.000152 Batch:1100 [35200/50000]\n",
            "loss: 0.467836 Batch:1200 [38400/50000]\n",
            "loss: 1.832274 Batch:1300 [41600/50000]\n",
            "loss: 0.237604 Batch:1400 [44800/50000]\n",
            "loss: 0.039642 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.310815 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 16.564116 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.277752 Batch:0 [    0/50000]\n",
            "loss: 1.713858 Batch:100 [ 3200/50000]\n",
            "loss: 0.943668 Batch:200 [ 6400/50000]\n",
            "loss: 0.719523 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.416308 Batch:500 [16000/50000]\n",
            "loss: 0.000001 Batch:600 [19200/50000]\n",
            "loss: 0.515126 Batch:700 [22400/50000]\n",
            "loss: 0.416425 Batch:800 [25600/50000]\n",
            "loss: 2.240649 Batch:900 [28800/50000]\n",
            "loss: 0.000049 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 0.000000 Batch:1200 [38400/50000]\n",
            "loss: 0.000000 Batch:1300 [41600/50000]\n",
            "loss: 0.122240 Batch:1400 [44800/50000]\n",
            "loss: 0.000000 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.187893 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 17.428176 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.000000 Batch:0 [    0/50000]\n",
            "loss: 0.755516 Batch:100 [ 3200/50000]\n",
            "loss: 0.000054 Batch:200 [ 6400/50000]\n",
            "loss: 0.000190 Batch:300 [ 9600/50000]\n",
            "loss: 0.006517 Batch:400 [12800/50000]\n",
            "loss: 0.062358 Batch:500 [16000/50000]\n",
            "loss: 0.000288 Batch:600 [19200/50000]\n",
            "loss: 0.016772 Batch:700 [22400/50000]\n",
            "loss: 0.000001 Batch:800 [25600/50000]\n",
            "loss: 0.000000 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 1.798486 Batch:1200 [38400/50000]\n",
            "loss: 0.369195 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.736015 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.195771 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 18.934390 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.000306 Batch:0 [    0/50000]\n",
            "loss: 0.938694 Batch:100 [ 3200/50000]\n",
            "loss: 0.116426 Batch:200 [ 6400/50000]\n",
            "loss: 0.000000 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.310447 Batch:500 [16000/50000]\n",
            "loss: 1.525707 Batch:600 [19200/50000]\n",
            "loss: 1.833289 Batch:700 [22400/50000]\n",
            "loss: 0.000001 Batch:800 [25600/50000]\n",
            "loss: 0.946747 Batch:900 [28800/50000]\n",
            "loss: 0.413225 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 0.486433 Batch:1200 [38400/50000]\n",
            "loss: 0.000000 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.692454 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.298506 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 19.731841 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.469156 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 0.406481 Batch:200 [ 6400/50000]\n",
            "loss: 0.000003 Batch:300 [ 9600/50000]\n",
            "loss: 1.083487 Batch:400 [12800/50000]\n",
            "loss: 0.394587 Batch:500 [16000/50000]\n",
            "loss: 0.000006 Batch:600 [19200/50000]\n",
            "loss: 0.017443 Batch:700 [22400/50000]\n",
            "loss: 0.000092 Batch:800 [25600/50000]\n",
            "loss: 0.250847 Batch:900 [28800/50000]\n",
            "loss: 0.006782 Batch:1000 [32000/50000]\n",
            "loss: 2.351207 Batch:1100 [35200/50000]\n",
            "loss: 0.007820 Batch:1200 [38400/50000]\n",
            "loss: 0.000000 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.000000 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.347651 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.4%, Avg loss: 21.351270 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.247246 Batch:0 [    0/50000]\n",
            "loss: 0.151711 Batch:100 [ 3200/50000]\n",
            "loss: 0.191551 Batch:200 [ 6400/50000]\n",
            "loss: 0.426714 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.000021 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.000002 Batch:700 [22400/50000]\n",
            "loss: 1.046478 Batch:800 [25600/50000]\n",
            "loss: 0.000000 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 0.591411 Batch:1100 [35200/50000]\n",
            "loss: 0.000049 Batch:1200 [38400/50000]\n",
            "loss: 0.911651 Batch:1300 [41600/50000]\n",
            "loss: 1.516596 Batch:1400 [44800/50000]\n",
            "loss: 1.791723 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.350462 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 23.012478 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.000008 Batch:0 [    0/50000]\n",
            "loss: 0.905007 Batch:100 [ 3200/50000]\n",
            "loss: 0.471098 Batch:200 [ 6400/50000]\n",
            "loss: 0.668654 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.632914 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.000000 Batch:700 [22400/50000]\n",
            "loss: 0.004298 Batch:800 [25600/50000]\n",
            "loss: 3.998494 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 1.350212 Batch:1100 [35200/50000]\n",
            "loss: 0.000000 Batch:1200 [38400/50000]\n",
            "loss: 2.531176 Batch:1300 [41600/50000]\n",
            "loss: 0.963712 Batch:1400 [44800/50000]\n",
            "loss: 0.206959 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.367136 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 26.578692 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.221864 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 0.030349 Batch:200 [ 6400/50000]\n",
            "loss: 0.000000 Batch:300 [ 9600/50000]\n",
            "loss: 0.000004 Batch:400 [12800/50000]\n",
            "loss: 1.538046 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.641955 Batch:700 [22400/50000]\n",
            "loss: 0.000001 Batch:800 [25600/50000]\n",
            "loss: 0.018646 Batch:900 [28800/50000]\n",
            "loss: 0.384023 Batch:1000 [32000/50000]\n",
            "loss: 0.861719 Batch:1100 [35200/50000]\n",
            "loss: 0.795581 Batch:1200 [38400/50000]\n",
            "loss: 2.184842 Batch:1300 [41600/50000]\n",
            "loss: 0.974348 Batch:1400 [44800/50000]\n",
            "loss: 0.000000 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.378835 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 24.549684 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.622294 Batch:0 [    0/50000]\n",
            "loss: 0.894405 Batch:100 [ 3200/50000]\n",
            "loss: 0.000000 Batch:200 [ 6400/50000]\n",
            "loss: 2.184417 Batch:300 [ 9600/50000]\n",
            "loss: 0.838371 Batch:400 [12800/50000]\n",
            "loss: 0.001629 Batch:500 [16000/50000]\n",
            "loss: 1.155403 Batch:600 [19200/50000]\n",
            "loss: 1.605996 Batch:700 [22400/50000]\n",
            "loss: 0.000000 Batch:800 [25600/50000]\n",
            "loss: 0.191490 Batch:900 [28800/50000]\n",
            "loss: 0.466910 Batch:1000 [32000/50000]\n",
            "loss: 0.001581 Batch:1100 [35200/50000]\n",
            "loss: 0.000035 Batch:1200 [38400/50000]\n",
            "loss: 0.873390 Batch:1300 [41600/50000]\n",
            "loss: 1.106197 Batch:1400 [44800/50000]\n",
            "loss: 0.000000 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.377818 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.3%, Avg loss: 27.284897 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.000000 Batch:0 [    0/50000]\n",
            "loss: 1.996640 Batch:100 [ 3200/50000]\n",
            "loss: 0.344553 Batch:200 [ 6400/50000]\n",
            "loss: 0.081321 Batch:300 [ 9600/50000]\n",
            "loss: 0.011320 Batch:400 [12800/50000]\n",
            "loss: 0.110254 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 1.115840 Batch:700 [22400/50000]\n",
            "loss: 0.226468 Batch:800 [25600/50000]\n",
            "loss: 1.393675 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 1.030413 Batch:1200 [38400/50000]\n",
            "loss: 1.031131 Batch:1300 [41600/50000]\n",
            "loss: 0.506183 Batch:1400 [44800/50000]\n",
            "loss: 0.000000 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.388275 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 28.067596 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 1.083660 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 1.185078 Batch:200 [ 6400/50000]\n",
            "loss: 2.801745 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 1.300232 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 1.951657 Batch:700 [22400/50000]\n",
            "loss: 0.000000 Batch:800 [25600/50000]\n",
            "loss: 0.000000 Batch:900 [28800/50000]\n",
            "loss: 1.151010 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 0.000000 Batch:1200 [38400/50000]\n",
            "loss: 0.000000 Batch:1300 [41600/50000]\n",
            "loss: 0.985811 Batch:1400 [44800/50000]\n",
            "loss: 0.121889 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.275726 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 30.647036 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 1.010811 Batch:0 [    0/50000]\n",
            "loss: 0.000074 Batch:100 [ 3200/50000]\n",
            "loss: 0.000009 Batch:200 [ 6400/50000]\n",
            "loss: 0.000000 Batch:300 [ 9600/50000]\n",
            "loss: 1.478059 Batch:400 [12800/50000]\n",
            "loss: 0.554138 Batch:500 [16000/50000]\n",
            "loss: 1.305965 Batch:600 [19200/50000]\n",
            "loss: 0.681709 Batch:700 [22400/50000]\n",
            "loss: 0.000000 Batch:800 [25600/50000]\n",
            "loss: 1.177757 Batch:900 [28800/50000]\n",
            "loss: 0.000000 Batch:1000 [32000/50000]\n",
            "loss: 0.164672 Batch:1100 [35200/50000]\n",
            "loss: 0.000000 Batch:1200 [38400/50000]\n",
            "loss: 1.599479 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.130032 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.563207 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 33.439655 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.000000 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 0.000000 Batch:200 [ 6400/50000]\n",
            "loss: 0.000000 Batch:300 [ 9600/50000]\n",
            "loss: 0.000000 Batch:400 [12800/50000]\n",
            "loss: 0.835435 Batch:500 [16000/50000]\n",
            "loss: 0.000000 Batch:600 [19200/50000]\n",
            "loss: 0.000000 Batch:700 [22400/50000]\n",
            "loss: 0.207898 Batch:800 [25600/50000]\n",
            "loss: 0.000000 Batch:900 [28800/50000]\n",
            "loss: 1.960848 Batch:1000 [32000/50000]\n",
            "loss: 0.000000 Batch:1100 [35200/50000]\n",
            "loss: 0.000000 Batch:1200 [38400/50000]\n",
            "loss: 0.000066 Batch:1300 [41600/50000]\n",
            "loss: 0.000000 Batch:1400 [44800/50000]\n",
            "loss: 0.192870 Batch:1500 [48000/50000]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.368891 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 70.2%, Avg loss: 31.175843 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.000000 Batch:0 [    0/50000]\n",
            "loss: 0.000000 Batch:100 [ 3200/50000]\n",
            "loss: 0.459785 Batch:200 [ 6400/50000]\n",
            "loss: 0.000000 Batch:300 [ 9600/50000]\n",
            "loss: 0.001869 Batch:400 [12800/50000]\n"
          ]
        }
      ]
    }
  ]
}