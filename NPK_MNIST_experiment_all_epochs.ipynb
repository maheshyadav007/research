{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshyadav007/research/blob/main/NPK_MNIST_experiment_all_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT0drpoPq8Zj"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlvMamTR1KuX",
        "outputId": "77da2029-d654-4799-f4e9-11fb1d0ad304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.37.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "A-D1NaAQiLtE"
      },
      "outputs": [],
      "source": [
        "# !pip install matplotlib --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "xLLVL4t-zk8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import torch \n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split \n",
        "import seaborn as sns\n",
        "#from torchviz import make_dot\n",
        "from google.colab import files\n",
        "from scipy.linalg import eigh\n",
        "import gc\n",
        "# %matplotlib inline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "%matplotlib inline\n",
        "# matplotlib.use('agg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "s82wzBl2uimR"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import pairwise_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "9t9ztNdKpP8w"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, svm, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "m3o1H1_Gu4iI"
      },
      "outputs": [],
      "source": [
        "# import numpy as np \n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tcijr0Lq35P"
      },
      "source": [
        "#Seed Setter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "OwD9vbnrraWL"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  \n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  # torch.use_deterministic_algorithms(True)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  # Python RNG\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXWpj9Z5qVke"
      },
      "source": [
        "#Making Dataset\n",
        "\n",
        "*   CIFAR 10\n",
        "*   MNIST\n",
        "*   L1 \n",
        "*   Boxed\n",
        "*   Cube\n",
        "*   Circle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "RgLT-lFguTTX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def circle_dataset_routine(n_data, a, b, seed, plot):\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data)\n",
        "\n",
        "  x=np.zeros((len(angles),2))\n",
        "  y=np.zeros(len(angles))\n",
        "\n",
        "  #a := Number of half cycles in the top half of the circle\n",
        "  #b := number of half cycles in the bottom half of the circle\n",
        "\n",
        "  for idx, angle in enumerate(angles):\n",
        "      x[idx,0]=np.cos(angle)\n",
        "      x[idx,1]=np.sin(angle)\n",
        "      if angle<np.pi:\n",
        "          y[idx] = np.sin(a*angle)\n",
        "      else:\n",
        "          y[idx] = np.sin(a*np.pi+b*(angle-np.pi))   \n",
        "\n",
        "  y = np.float32(y)\n",
        "  \n",
        "  if plot:\n",
        "    plt.plot(angles, y)\n",
        "    plt.xlabel(\"angle in radians\")\n",
        "    plt.ylabel(\"label function (y)\")\n",
        "    plt.figure()\n",
        "    plt.axis('equal')\n",
        "    plt.scatter(x[:,0],x[:,1],c=y)\n",
        "    plt.colorbar()\n",
        "    \n",
        "  return x, np.squeeze(y)\n",
        "def circle_dataset(dataset_args):\n",
        "  n_data, a, b, plot = dataset_args['n_data'], dataset_args['a'], dataset_args['b'], dataset_args['plot'] \n",
        "  x_train, y_train = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = plot)\n",
        "  x_val, y_val = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "  x_test, y_test = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "7ihN5vlCgrD0"
      },
      "outputs": [],
      "source": [
        "def circle_dataset_routine(n_data, a, b, seed, plot):\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data)\n",
        "\n",
        "  x=np.zeros((len(angles),2))\n",
        "  y=np.zeros(len(angles))\n",
        "\n",
        "  #a := Number of half cycles in the top half of the circle\n",
        "  #b := number of half cycles in the bottom half of the circle\n",
        "\n",
        "  for idx, angle in enumerate(angles):\n",
        "      x[idx,0]=np.cos(angle)\n",
        "      x[idx,1]=np.sin(angle)\n",
        "      if angle<np.pi:\n",
        "          y[idx] = np.sin(a*angle)\n",
        "      else:\n",
        "          y[idx] = np.sin(a*np.pi+b*(angle-np.pi))   \n",
        "\n",
        "  y = np.float32(y)\n",
        "  \n",
        "  if plot:\n",
        "    plt.plot(angles, y)\n",
        "    plt.xlabel(\"angle in radians\")\n",
        "    plt.ylabel(\"label function (y)\")\n",
        "    # plt.savefig(\"circle.pdf\", format = 'pdf')\n",
        "    plt.figure()\n",
        "    plt.axis('equal')\n",
        "    plt.scatter(x[:,0],x[:,1],c=y)\n",
        "    plt.colorbar()\n",
        "    # plt.savefig(\"circle_label_func.pdf\", format = 'pdf')\n",
        "    \n",
        "  return x, np.squeeze(y)\n",
        "def circle_dataset(dataset_args):\n",
        "  n_data, a, b, plot = dataset_args['n_data'], dataset_args['a'], dataset_args['b'], dataset_args['plot'] \n",
        "  x_train, y_train = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = plot)\n",
        "  x_val, y_val = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "  x_test, y_test = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "4bB2f--FokVV"
      },
      "outputs": [],
      "source": [
        "# _ = circle_dataset(dataset_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "-ojRKBSxCczl"
      },
      "outputs": [],
      "source": [
        "def binary_cifar_dataset_routine(size, is_train,is_binary, flattened):\n",
        "  data = torchvision.datasets.CIFAR10(root = '/CIFAR10', train = is_train, download = True,  transform=ToTensor())\n",
        "  X, Y = [], []\n",
        "  for idx, d in enumerate(data):\n",
        "    X.append(d[0].numpy())\n",
        "    Y.append(d[1])\n",
        "  Y = np.expand_dims(Y, axis = 1)\n",
        "  X = np.array(X)\n",
        "  # X = np.squeeze(X, axis = 1)\n",
        "  if is_binary :\n",
        "    X = X[np.logical_or(Y==4, Y == 8).squeeze()]\n",
        "    Y = Y[np.logical_or(Y==4, Y == 8)]\n",
        "    Y[Y == 4], Y[Y == 8] = 0, 1\n",
        "  if flattened : \n",
        "    X = X.reshape((X.shape[0], -1))\n",
        "  return X, np.squeeze(Y)\n",
        "\n",
        "def binary_cifar_dataset(dataset_args):\n",
        "  flattened, is_binary = dataset_args['flattened'], dataset_args['is_binary']\n",
        "  n_data = 0\n",
        "  x_train, y_train = binary_cifar_dataset_routine(n_data, is_train = True,is_binary = is_binary, flattened = flattened)\n",
        "  x_val, y_val = None, None\n",
        "  x_test, y_test = binary_cifar_dataset_routine(n_data, is_train = False,is_binary = is_binary, flattened = flattened)\n",
        "  print(x_train.shape, y_train.shape)\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "BKig7wTFfZ9p"
      },
      "outputs": [],
      "source": [
        "def binary_mnist_dataset_routine(size, is_train):\n",
        "  mnist_train_data = torchvision.datasets.MNIST(root = '/MNIST', train = is_train, download = True,  transform=ToTensor())\n",
        "  X, Y = [], []\n",
        "  for idx, data in enumerate(mnist_train_data):\n",
        "    X.append(torch.flatten(data[0], start_dim=1, end_dim=- 1).numpy())\n",
        "    Y.append(data[1])\n",
        "  Y = np.expand_dims(Y, axis = 1)\n",
        "  X = np.array(X)\n",
        "  # X = np.squeeze(X, axis = 1)\n",
        "  X = X[np.logical_or(Y==0, Y == 1)]\n",
        "  Y = Y[np.logical_or(Y==0, Y == 1)]\n",
        "  # Y[Y==1] = 0\n",
        "  # Y[Y == 7] = 1\n",
        "  return X, np.squeeze(Y)\n",
        "\n",
        "def binary_mnist_dataset(*args):\n",
        "  n_data = None\n",
        "  x_train, y_train = binary_mnist_dataset_routine(n_data, is_train = True)\n",
        "  x_val, y_val = None, None\n",
        "  x_test, y_test = binary_mnist_dataset_routine(n_data, is_train = False)\n",
        "  print(x_train.shape, y_train.shape)\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "BUPXqUU68xuj"
      },
      "outputs": [],
      "source": [
        "def boxed_complex_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(10)\n",
        "  x1 = rng.random((size,1))*(2*2*np.pi) - 2*np.pi\n",
        "  x2 = rng.random((size,1))*(2*np.pi) - np.pi\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y1 = np.sign(np.sin(2*x[:,0]))\n",
        "  y2 = np.sign(np.sin(x[:,1]))\n",
        "  y = y1*y2\n",
        "  y1[y1 == -1] = 0\n",
        "  y2[y2 == -1] = 0\n",
        "  y[y == -1] = 0\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def boxed_complex_dataset():\n",
        "  n_data = 2000\n",
        "  x_train, y_train = boxed_complex_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = boxed_complex_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = boxed_complex_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "Y1yQzgmwmaV0"
      },
      "outputs": [],
      "source": [
        "def boxed_complex_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(10)\n",
        "  x1 = rng.random((size,1))*(2*2*np.pi) - 2*np.pi\n",
        "  x2 = rng.random((size,1))*(2*np.pi) - np.pi\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y1 = np.sign(np.sin(2*x[:,0]))\n",
        "  y2 = np.sign(np.sin(x[:,1]))\n",
        "  y = y1*y2\n",
        "  y1[y1 == -1] = 0\n",
        "  y2[y2 == -1] = 0\n",
        "  y[y == -1] = 0\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def boxed_complex_dataset():\n",
        "  n_data = 2000\n",
        "  x_train, y_train = boxed_complex_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = boxed_complex_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = boxed_complex_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini\n",
        "def boxed_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  x1 = rng.random((size,1))*(2*2*np.pi) - 2*np.pi\n",
        "  x2 = rng.random((size,1))*(2*np.pi) - np.pi\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y = np.sign(np.sin(2*x[:,0]))\n",
        "  # y2 = np.sign(np.sin(x[:,1]))\n",
        "  # y = y1*y2\n",
        "  # y1[y1 == -1] = 0\n",
        "  # y2[y2 == -1] = 0\n",
        "  y[y == -1] = 0\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def boxed_dataset(*args):\n",
        "  n_data = 1000\n",
        "  x_train, y_train = boxed_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = boxed_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = boxed_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "dpKkE-vQfro3"
      },
      "outputs": [],
      "source": [
        "def union_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  x1 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x2 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  \n",
        "  w1, w2 = np.array([[-1 ],[1]]), np.array([[1 ],[-1]])\n",
        "  w1, w2 = w1/np.linalg.norm(w1), w2/np.linalg.norm(w2)\n",
        "  y1 = np.sign(np.matmul(x, w1) - np.sqrt(2)/4.5)\n",
        "  y2 = np.sign(np.matmul(x, w2) +  2*np.sqrt(2)/3)\n",
        "  y3 = np.sign(np.matmul(x, w1) + 2* np.sqrt(2)/3)\n",
        "  y4 = np.sign(np.matmul(x, w2) -  np.sqrt(2)/4.5)\n",
        "  # y1[y1 == -1],y2[y2 == -1]  = 0, 0\n",
        "  y = y1 + y2 + y3 + y4\n",
        "  x[:,1] = x[:,1] + 3\n",
        "  X, Y = x, y\n",
        "  \n",
        "  x1 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x2 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  w1, w2 = np.array([[1 ],[1]]), np.array([[-1 ],[-1]])\n",
        "  w1, w2 = w1/np.linalg.norm(w1), w2/np.linalg.norm(w2)\n",
        "  y1 = np.sign(np.matmul(x, w1) - np.sqrt(2)/4.5)\n",
        "  y2 = np.sign(np.matmul(x, w2) +  2*np.sqrt(2)/3)\n",
        "  y3 = np.sign(np.matmul(x, w1) + 2* np.sqrt(2)/3)\n",
        "  y4 = np.sign(np.matmul(x, w2) -  np.sqrt(2)/4.5)\n",
        "  y = y1 + y2 + y3 + y4\n",
        "  \n",
        "  X, Y = np.concatenate((X,x), axis = 0), np.concatenate((Y,y), axis = 0)\n",
        "  Y[Y==2] = 1\n",
        "  plt.scatter(X[:,0], X[:,1], c = Y)\n",
        "  plt.axis('equal')\n",
        "  return X, np.squeeze(Y)\n",
        "def union_dataset():\n",
        "  n_data = 2000\n",
        "  x_train, y_train = union_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = union_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = union_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "hU1lLoOpF-Hw"
      },
      "outputs": [],
      "source": [
        "def L1_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(10)\n",
        "  x1 = rng.random((1000,1))*(2) - 1\n",
        "  x2 = rng.random((1000,1))*(2) - 1\n",
        "  # scaler = np.linalg.norm(np.concatenate((x1, x2), axis = 1), ord = 1, axis = 1, keepdims = True) * 1/.5\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y = np.linalg.norm(np.concatenate((x1, x2), axis = 1), ord = 1, axis = 1, keepdims = True) >= 1\n",
        "  y = y*1\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def L1_dataset():\n",
        "  n_data = 1000\n",
        "  x_train, y_train = L1_dataset_routine(1000, 1)\n",
        "  x_val, y_val = L1_dataset_routine(1000, 2)\n",
        "  x_test, y_test = L1_dataset_routine(1000, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "qSRjT4qo3Srv"
      },
      "outputs": [],
      "source": [
        "def parabola_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  x1 = rng.random((size,1))*(2) - 1\n",
        "  x2 = rng.random((size,1))*(2) - .75\n",
        "  y =  x2 >= x1**2/(1)\n",
        "  y = y*1\n",
        "  return np.concatenate((x1, x2), axis = 1), np.squeeze(y)\n",
        "def parabola_dataset():\n",
        "  n_data = 1000\n",
        "  x_train, y_train = parabola_dataset_routine(1000, 1)\n",
        "  x_val, y_val = parabola_dataset_routine(1000, 2)\n",
        "  x_test, y_test = parabola_dataset_routine(1000, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "wQc42HwrNKoO"
      },
      "outputs": [],
      "source": [
        "def get_dist(test_data_curr):\n",
        "  data = test_data_curr\n",
        "  dist = []\n",
        "  start = 0\n",
        "  end = 400\n",
        "  for n in range(6):\n",
        "    \n",
        "    if n == 0 or n == 1:\n",
        "      dist.append(np.minimum(np.minimum(np.abs(data[start:end,2] - .5), np.abs(data[start:end,2] + .5)), np.minimum(np.abs(data[start:end,1] - .5), np.abs(data[start:end,1] + .5))))\n",
        "    if n == 2 or n == 3:\n",
        "      dist.append(np.minimum(np.minimum(np.abs(data[start:end,0] - .5), np.abs(data[start:end,0] + .5)), np.minimum(np.abs(data[start:end,2] - .5), np.abs(data[start:end,2] + .5))))\n",
        "    if n == 4 or n == 5:\n",
        "      dist.append(np.minimum(np.minimum(np.abs(data[start:end,0] - .5), np.abs(data[start:end,0] + .5)), np.minimum(np.abs(data[start:end,1] - .5), np.abs(data[start:end,1] + .5))))\n",
        "    start = end\n",
        "    end = end + 400\n",
        "  dist = np.concatenate(dist)\n",
        "  return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "-V61tbiSjeM3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "kPuUqfIWc9wi"
      },
      "outputs": [],
      "source": [
        "num_modes=6\n",
        "d=num_modes//2\n",
        "def get_cube_data():\n",
        "  num_modes=6 # Each mode is a d-1 dimensional hyperplane perpendicular to one of the exes, and intercept at +b or -b\n",
        "  d=num_modes//2 # Choose d to be a multiple of num_modes\n",
        "  d=3\n",
        "  num_modes=6\n",
        "  a=[2]*6\n",
        "  mode_frac = np.array([1./num_modes]*num_modes)\n",
        "  b=1.\n",
        "\n",
        "  num_train_data = 2400 \n",
        "  num_vali_data=2400\n",
        "  num_test_data=2400\n",
        "  num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "  num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "  num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "  landmarks=[-1]*num_modes\n",
        "  labels=[-1]*num_modes\n",
        "  for i in range(num_modes):\n",
        "      landmarks[i] = np.random.randn(a[i],d)\n",
        "      \n",
        "      landmarks[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(a[i])\n",
        "      print(\"Colum \", str(i//2),\"of mode\", str(i), \"with\", str(2*b*((i%2)-0.5)*np.ones(a[i])))\n",
        "      landmarks[i]/=2.\n",
        "      labels[i] = (np.arange(len(landmarks[i])))%2\n",
        "\n",
        "  data=[-1]*num_modes\n",
        "  data_labels=[-1]*num_modes\n",
        "\n",
        "  train_data=[-1]*num_modes\n",
        "  train_data_labels=[-1]*num_modes\n",
        "\n",
        "  test_data=[-1]*num_modes\n",
        "  test_data_labels=[-1]*num_modes\n",
        "\n",
        "  vali_data=[-1]*num_modes\n",
        "  vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "  modes_data=[]\n",
        "  for i in range(num_modes):\n",
        "      data[i] = np.random.randn(num_data_per_mode[i],d)\n",
        "      data[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(num_data_per_mode[i])\n",
        "      data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "      for j in range(len(data_labels[i])):\n",
        "          dists = pairwise_distances(data[i][j:j+1,:],landmarks[i])                                   \n",
        "          j_star = np.argmin(dists[0])\n",
        "          data_labels[i][j]=labels[i][j_star]\n",
        "      \n",
        "      data[i]/=2.\n",
        "      train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "      train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "      vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "      vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "      test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "      test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "  train_data_curr = np.concatenate(train_data)\n",
        "  train_labels_curr = np.concatenate(train_data_labels)\n",
        "  vali_data_curr = np.concatenate(vali_data)\n",
        "  vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "  test_data_curr = np.concatenate(test_data)\n",
        "  test_labels_curr = np.concatenate(test_data_labels)\n",
        "  dist = get_dist(test_data_curr)\n",
        "  sorted_idx = np.argsort(dist)\n",
        "  sorted_idx = np.concatenate((sorted_idx[:250], sorted_idx[-250:]))\n",
        "\n",
        "  mini_test_data = np.squeeze(test_data_curr[sorted_idx])\n",
        "  mini_test_labels = np.squeeze(test_labels_curr[sorted_idx])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  # num_modes=6\n",
        "  # d=num_modes//2 # Choose d to be a multiple of num_modes\n",
        "  # a=[2]*(num_modes-1) + [40]\n",
        "  # mode_frac = np.array([0.25/(num_modes-1)]*(num_modes-1)+ [0.75])\n",
        "\n",
        "  # label_noise_frac=[0.0]*(num_modes-1)+[0.0]\n",
        "  # instance_noise_stdev = [0.0]*num_modes\n",
        "\n",
        "  # # These two numbers make sense only if d=2*num_modes\n",
        "  # corrs = [0.0]*num_modes\n",
        "  # nn_transform = [np.diag([1.,1.]*num_modes)]*num_modes  \n",
        "\n",
        "  # num_train_data = 640 \n",
        "  # num_vali_data=2000\n",
        "  # num_test_data=2000\n",
        "  # num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "  # num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "  # num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "  # landmarks=[-1]*num_modes\n",
        "  # labels=[-1]*num_modes\n",
        "  # for i in range(num_modes):\n",
        "  #     landmarks[i] = np.zeros((a[i],d))\n",
        "  #     thetas = np.arange(0,2*np.pi-0.01,2*np.pi/a[i])\n",
        "  #     temp = np.concatenate((np.cos(thetas)[:,None], np.sin(thetas)[:,None]),axis=1)\n",
        "      \n",
        "\n",
        "  #     landmarks[i][:, i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "  #     if i<num_modes-1:\n",
        "  #         labels[i] = np.ones((len(landmarks[i])))*(i)%2        \n",
        "  #     else:\n",
        "  #         labels[i] = (np.arange(len(landmarks[i])) + i)%2\n",
        "\n",
        "  # labels_all_modes = np.concatenate(labels, axis=0)\n",
        "  # landmarks_all_modes = np.concatenate(landmarks, axis=0)\n",
        "\n",
        "  # data=[-1]*num_modes\n",
        "  # data_labels=[-1]*num_modes\n",
        "\n",
        "  # train_data=[-1]*num_modes\n",
        "  # train_data_labels=[-1]*num_modes\n",
        "\n",
        "  # test_data=[-1]*num_modes\n",
        "  # test_data_labels=[-1]*num_modes\n",
        "\n",
        "  # vali_data=[-1]*num_modes\n",
        "  # vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "  # modes_data=[]\n",
        "  # for i in range(num_modes):\n",
        "  #     data[i] = np.zeros((num_data_per_mode[i],d))    \n",
        "  #     temp = np.random.randn(num_data_per_mode[i], d//num_modes)\n",
        "  #     temp = temp/np.sqrt(np.sum(temp**2, axis=1)[:,None])\n",
        "\n",
        "  #     data[i][:,i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "  #     data[i] += np.random.randn(num_data_per_mode[i],d)*instance_noise_stdev[i]\n",
        "  #     data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "  #     for j in range(len(data_labels[i])):\n",
        "  #         dists = pairwise_distances(np.dot(data[i][j:j+1,:], nn_transform[i]) \n",
        "  #                                   ,np.dot(landmarks_all_modes, nn_transform[i]))\n",
        "  #         j_star = np.argmin(dists[0])\n",
        "  #         data_labels[i][j]=labels_all_modes[j_star]\n",
        "  #         modes_data.append(i)\n",
        "  #     data_labels[i] = (data_labels[i] + np.int32(np.random.rand(len(data_labels[i]))<label_noise_frac[i]) ) % 2\n",
        "\n",
        "  #     data[i][:,i*d//num_modes:(i+1)*d//num_modes] = np.dot(data[i][:,i*d//num_modes:(i+1)*d//num_modes], \n",
        "  #                                                           np.array([[1., corrs[i]],[corrs[i], 1.]]))\n",
        "      \n",
        "  #     train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "  #     train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "  #     vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "  #                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "  #     vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "  #                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "  #     test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "  #     test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "  # train_data_curr = np.concatenate(train_data)\n",
        "  # train_labels_curr = np.concatenate(train_data_labels)\n",
        "  # vali_data_curr = np.concatenate(vali_data)\n",
        "  # vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "  # test_data_curr = np.concatenate(test_data)\n",
        "  # test_labels_curr = np.concatenate(test_data_labels)\n",
        "  # dist = get_dist(test_data_curr)\n",
        "  # sorted_idx = np.argsort(dist)\n",
        "  # sorted_idx = np.concatenate((sorted_idx[:250], sorted_idx[-250:]))\n",
        "  # mini_test_data = np.squeeze(test_data_curr[sorted_idx])\n",
        "  # mini_test_labels = np.squeeze(test_labels_curr[sorted_idx])\n",
        "  return train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "gMuCcghYkkGS"
      },
      "outputs": [],
      "source": [
        "# train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels = get_cube_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "lTAFOg51Djm4"
      },
      "outputs": [],
      "source": [
        "# num_modes=6 # Each mode is a d-1 dimensional hyperplane perpendicular to one of the exes, and intercept at +b or -b\n",
        "# d=num_modes//2 # Choose d to be a multiple of num_modes\n",
        "# d=3\n",
        "# num_modes=6\n",
        "# a=[2]*6\n",
        "# mode_frac = np.array([1./num_modes]*num_modes)\n",
        "# b=1.\n",
        "\n",
        "# num_train_data = 2400 \n",
        "# num_vali_data=2400\n",
        "# num_test_data=2400\n",
        "# num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "# num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "# num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "# landmarks=[-1]*num_modes\n",
        "# labels=[-1]*num_modes\n",
        "# for i in range(num_modes):\n",
        "#     landmarks[i] = np.random.randn(a[i],d)\n",
        "    \n",
        "#     landmarks[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(a[i])\n",
        "#     print(\"Colum \", str(i//2),\"of mode\", str(i), \"with\", str(2*b*((i%2)-0.5)*np.ones(a[i])))\n",
        "#     landmarks[i]/=2.\n",
        "#     labels[i] = (np.arange(len(landmarks[i])))%2\n",
        "\n",
        "# data=[-1]*num_modes\n",
        "# data_labels=[-1]*num_modes\n",
        "\n",
        "# train_data=[-1]*num_modes\n",
        "# train_data_labels=[-1]*num_modes\n",
        "\n",
        "# test_data=[-1]*num_modes\n",
        "# test_data_labels=[-1]*num_modes\n",
        "\n",
        "# vali_data=[-1]*num_modes\n",
        "# vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "# modes_data=[]\n",
        "# for i in range(num_modes):\n",
        "#     data[i] = np.random.randn(num_data_per_mode[i],d)\n",
        "#     data[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(num_data_per_mode[i])\n",
        "#     data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "#     for j in range(len(data_labels[i])):\n",
        "#         dists = pairwise_distances(data[i][j:j+1,:],landmarks[i])                                   \n",
        "#         j_star = np.argmin(dists[0])\n",
        "#         data_labels[i][j]=labels[i][j_star]\n",
        "    \n",
        "#     data[i]/=2.\n",
        "#     train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "#     train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "#     vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "#     vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "#     test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "#     test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "    \n",
        "# train_data_curr = np.concatenate(train_data)\n",
        "# train_labels_curr = np.concatenate(train_data_labels)\n",
        "# vali_data_curr = np.concatenate(vali_data)\n",
        "# vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "# test_data_curr = np.concatenate(test_data)\n",
        "# test_labels_curr = np.concatenate(test_data_labels)\n",
        "# dist = get_dist(test_data_curr)\n",
        "# sorted_idx = np.argsort(dist)\n",
        "# sorted_idx = np.concatenate((sorted_idx[:250], sorted_idx[-250:]))\n",
        "\n",
        "# mini_test_data = np.squeeze(test_data_curr[sorted_idx])\n",
        "# mini_test_labels = np.squeeze(test_labels_curr[sorted_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "EO4gKIHyelNN"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "# st = 0\n",
        "# end = st+250\n",
        "# x, y, z = zip(*mini_test_data[st:end,:])\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(xs = x, ys = y,zs = z, c = mini_test_labels[st:end])\n",
        "# # plt.scatter(x = y, y = z, c = train_labels_curr[st:end])\n",
        "# # plt.plot(landmarks[1][:, 1], landmarks[1][:, 2])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "mTddmHtchLAj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "g_sY6pCvhJdy"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# x, y, z = zip(*train_data_curr[400:800,:])\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(xs = x, ys = y,zs = z, c = test_labels_curr[400:800])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "Uf8aH2MKhWvv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "KaS0b1pzEOFK"
      },
      "outputs": [],
      "source": [
        "# num_modes=21\n",
        "# d=2*num_modes\n",
        "\n",
        "# def get_multi_modes_data():\n",
        "#   num_modes=21\n",
        "#   d=2*num_modes # Choose d to be a multiple of num_modes\n",
        "#   a=[2]*(num_modes-1) + [40]\n",
        "#   mode_frac = np.array([0.5/(num_modes-1)]*(num_modes-1)+ [0.5])\n",
        "\n",
        "#   label_noise_frac=[0.05]*num_modes\n",
        "#   instance_noise_stdev = [0.0]*num_modes\n",
        "\n",
        "#   # These two numbers make sense only if d=2*num_modes\n",
        "#   corrs = [0.0]*num_modes\n",
        "#   nn_transform = [np.diag([1.,1.]*num_modes)]*num_modes  \n",
        "\n",
        "#   num_train_data = 640 \n",
        "#   num_vali_data=2000\n",
        "#   num_test_data=2000\n",
        "#   num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "#   num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "#   num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "#   landmarks=[-1]*num_modes\n",
        "#   labels=[-1]*num_modes\n",
        "#   for i in range(num_modes):\n",
        "#       landmarks[i] = np.zeros((a[i],d))\n",
        "#       thetas = np.arange(0,2*np.pi-0.01,2*np.pi/a[i])\n",
        "#       temp = np.concatenate((np.cos(thetas)[:,None], np.sin(thetas)[:,None]),axis=1)\n",
        "      \n",
        "\n",
        "#       landmarks[i][:, i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "#       labels[i] = (np.arange(len(landmarks[i])) + i)%2\n",
        "#   labels_all_modes = np.concatenate(labels, axis=0)\n",
        "#   landmarks_all_modes = np.concatenate(landmarks, axis=0)\n",
        "\n",
        "#   data=[-1]*num_modes\n",
        "#   data_labels=[-1]*num_modes\n",
        "\n",
        "#   train_data=[-1]*num_modes\n",
        "#   train_data_labels=[-1]*num_modes\n",
        "\n",
        "#   test_data=[-1]*num_modes\n",
        "#   test_data_labels=[-1]*num_modes\n",
        "\n",
        "#   vali_data=[-1]*num_modes\n",
        "#   vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "#   modes_data=[]\n",
        "#   for i in range(num_modes):\n",
        "#       data[i] = np.zeros((num_data_per_mode[i],d))    \n",
        "#       temp = np.random.randn(num_data_per_mode[i], d//num_modes)\n",
        "#       temp = temp/np.sqrt(np.sum(temp**2, axis=1)[:,None])\n",
        "\n",
        "#       data[i][:,i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "#       data[i] += np.random.randn(num_data_per_mode[i],d)*instance_noise_stdev[i]\n",
        "#       data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "#       for j in range(len(data_labels[i])):\n",
        "#           dists = pairwise_distances(np.dot(data[i][j:j+1,:], nn_transform[i]) \n",
        "#                                     ,np.dot(landmarks_all_modes, nn_transform[i]))\n",
        "#           j_star = np.argmin(dists[0])\n",
        "#           data_labels[i][j]=labels_all_modes[j_star]\n",
        "#           modes_data.append(i)\n",
        "#       data_labels[i] = (data_labels[i] + np.int32(np.random.rand(len(data_labels[i]))<label_noise_frac[i]) ) % 2\n",
        "\n",
        "#       data[i][:,i*d//num_modes:(i+1)*d//num_modes] = np.dot(data[i][:,i*d//num_modes:(i+1)*d//num_modes], \n",
        "#                                                             np.array([[1., corrs[i]],[corrs[i], 1.]]))\n",
        "      \n",
        "#       train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "#       train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "#       vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                       int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "#       vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                       int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "#       test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "#       test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "#   train_data_curr = np.concatenate(train_data)\n",
        "#   train_labels_curr = np.concatenate(train_data_labels)\n",
        "#   vali_data_curr = np.concatenate(vali_data)\n",
        "#   vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "#   test_data_curr = np.concatenate(test_data)\n",
        "#   test_labels_curr = np.concatenate(test_data_labels)\n",
        "#   return train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "5YOEobQDibrK"
      },
      "outputs": [],
      "source": [
        "#cube data\n",
        "def get_mini_multi_modes_data(test_data_curr,num_modes, n_examples_per_mode):\n",
        "  n_data_per_mode = test_data_curr.shape[0]//num_modes\n",
        "  stride_length = n_data_per_mode\n",
        "  X = []#np.zeros((n_examples_per_mode*num_modes, d))\n",
        "  for i in range(num_modes):\n",
        "    X.append(test_data_curr[i*stride_length: (i+1)*stride_length][np.random.choice(stride_length, n_examples_per_mode, replace=False)])\n",
        "  return np.concatenate(X)\n",
        "\n",
        "#multimode data\n",
        "# def get_mini_multi_modes_data(test_data_curr,num_modes, n_examples_per_mode):\n",
        "#   n_data_per_mode = (int(test_data_curr.shape[0]/2))//num_modes\n",
        "#   stride_length = n_data_per_mode\n",
        "#   X = []#np.zeros((n_examples_per_mode*num_modes, d))\n",
        "#   for i in range(num_modes-1):\n",
        "#     X.append(test_data_curr[i*stride_length: (i+1)*stride_length][np.random.choice(stride_length, n_examples_per_mode, replace=False)])\n",
        "#   X.append(test_data_curr[1000: 2000][np.random.choice(1000, n_examples_per_mode, replace=False)])\n",
        "#   return np.concatenate(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "7kVQMTbyBb_H"
      },
      "outputs": [],
      "source": [
        "# def get_mini_multi_modes_data(test_data_curr,num_modes, n_examples_per_mode):\n",
        "#   n_data_per_mode = (test_data_curr.shape[0]//2)//(num_modes-1)\n",
        "#   stride_length = n_data_per_mode\n",
        "#   X = []#np.zeros((n_examples_per_mode*num_modes, d))\n",
        "#   for i in range(num_modes-1):\n",
        "#     X.append(test_data_curr[i*stride_length: (i+1)*stride_length][np.random.choice(stride_length, n_examples_per_mode, replace=False)])\n",
        "#   X.append(test_data_curr[1000: 2000][np.random.choice(1000, n_examples_per_mode, replace=False)])\n",
        "#   return np.concatenate(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "vUiEnLc-Ny-I"
      },
      "outputs": [],
      "source": [
        "# def get_mini_multi_modes_data(instance):\n",
        "#   instance = instance\n",
        "#   thetas = np.arange(0, 2*np.pi, 2*np.pi/instance)\n",
        "#   X = np.zeros((instance*num_modes, d))\n",
        "#   for i in range(num_modes):\n",
        "#     X[i*instance: (i+1)*instance, i*2] = np.cos(thetas)\n",
        "#     X[i*instance: (i+1)*instance, i*2+1] = np.sin(thetas)\n",
        "#   return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "WZUtAoX-KYnE"
      },
      "outputs": [],
      "source": [
        "class Mini(Dataset):\n",
        "      def __init__(self, X):\n",
        "          self.size = len(X)\n",
        "          self.x = X\n",
        "         \n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float()\n",
        "def get_mini_multi_mode_dataloaders(X_mini, batch_size = 100):\n",
        "  X_mini = torch.tensor(X_mini)\n",
        "  mini = Mini(X_mini)\n",
        "\n",
        "  mini_dl = torch.utils.data.DataLoader(mini,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  return mini_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "HFzQMtbjXCd3"
      },
      "outputs": [],
      "source": [
        "class DSphere(Dataset):\n",
        "      def __init__(self, X, Y):\n",
        "          self.size = len(Y)\n",
        "          self.x = X\n",
        "          self.y = Y\n",
        "\n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float(), self.y[idx]\n",
        "def get_mini_dsphere(X,Y,n_data, a,b):\n",
        "  \n",
        "  mini_X = []\n",
        "  mini_Y = []\n",
        "\n",
        "  rng = np.random.default_rng(0)\n",
        "  idxs = rng.choice(np.arange(0, int(n_data/2)-a),50, replace = False)\n",
        "  mini_X.extend([X[idx]  for idx in idxs])\n",
        "  mini_Y.extend([Y[idx]  for idx in idxs])\n",
        "\n",
        "  idxs = rng.choice(np.arange(int(n_data/2)-a,int(n_data)-a-b),50, replace = False)\n",
        "  mini_X.extend([X[idx]  for idx in idxs])\n",
        "  mini_Y.extend([Y[idx]  for idx in idxs])\n",
        "  \n",
        "  return mini_X, mini_Y\n",
        "\n",
        "def get_dsphere_dls(n_data, a, b,d, batch_size, is_perturbed):\n",
        "  \n",
        "  rng = np.random.default_rng(seed = 0)\n",
        "\n",
        "  n_data = n_data\n",
        "  d = d\n",
        "\n",
        "  X1 = rng.random(size = (int(n_data/2),int(d/2)))*(2) - 1 \n",
        "  X1 = X1 / np.sqrt(np.sum(np.square(X1), axis = 1, keepdims = True))\n",
        "  X1 = np.concatenate((X1, np.zeros(X1.shape)), axis = 1)\n",
        "\n",
        "  X2 = rng.random(size = (int(n_data/2),int(d/2)))*(2) - 1 \n",
        "  X2 = X2 / np.sqrt(np.sum(np.square(X2), axis = 1, keepdims = True))\n",
        "  X2 = np.concatenate(( np.zeros(X2.shape), X2), axis = 1)\n",
        "\n",
        "  X = np.concatenate((X1, X2), axis = 0)\n",
        "  Y = np.zeros((int(n_data),), dtype = int)\n",
        "  \n",
        "  # Y_true = copy.deepcopy(Y)\n",
        "\n",
        "\n",
        "  #transform Y\n",
        "  sample_idx_1 = rng.choice(a = np.arange(int(n_data/2)), size = (a,), replace = False )\n",
        "  for idx in sample_idx_1:\n",
        "    Y[idx] = rng.choice(a = [1, 0], size = (1,))\n",
        "\n",
        "  sample_idx_2 = rng.choice(a = np.arange(int(n_data/2), n_data), size = (b,), replace = False )\n",
        "  for idx in sample_idx_2:\n",
        "    Y[idx] = rng.choice(a = [1, 0], size = (1,))\n",
        "\n",
        "\n",
        "  landmarks = np.concatenate((sample_idx_1, sample_idx_2), axis = 0)  \n",
        "  mask=np.full(len(Y),False,dtype=bool)\n",
        "  mask[landmarks]=True\n",
        "  X_landmarks, Y_landmarks = np.array(X)[mask], np.array(Y)[mask]\n",
        "\n",
        "  X_data = np.array(X)[~mask]\n",
        "\n",
        "  neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "  neigh.fit(X_landmarks, Y_landmarks)\n",
        "  Y_data = neigh.predict(X_data)\n",
        "\n",
        "\n",
        "  # X_small = np.concatenate((X[sample_idx_1], X[sample_idx_2]), axis = 0)\n",
        "  # Y_small = np.concatenate((Y[sample_idx_1], Y[sample_idx_2]), axis = 0)\n",
        "\n",
        "  # neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "  # neigh.fit(X_small, Y_small)\n",
        "  # Y_true = neigh.predict(X)\n",
        "\n",
        "  mini_X, mini_Y = get_mini_dsphere(X_data, Y_data,n_data,  a, b)\n",
        "  mode1_idxs, mode2_idxs = np.arange(0,int(n_data/2)-a), np.arange(int(n_data/2)-a, n_data-a-b)\n",
        "  # print(landmarks)\n",
        "  # mode1_idxs, mode2_idxs  = list(set(mode1_idxs) - set(landmarks)), list(set(mode2_idxs) - set(landmarks))\n",
        "  X_mode1_data, Y_mode1_data = X_data[mode1_idxs], Y_data[mode1_idxs]\n",
        "  X_mode2_data, Y_mode2_data = X_data[mode2_idxs], Y_data[mode2_idxs]\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1, random_state=42)\n",
        "  X_train_m1, X_test_m1, y_train_m1, y_test_m1 = train_test_split(X_mode1_data, Y_mode1_data, test_size=0.1, random_state=42)\n",
        "  X_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X_mode2_data, Y_mode2_data, test_size=0.1, random_state=42)\n",
        "\n",
        "  return (X_train, X_test, y_train, y_test), (X_train_m1, X_test_m1, y_train_m1, y_test_m1), (X_train_m2, X_test_m2, y_train_m2, y_test_m2), (mini_X, mini_Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "zZtkWhsnd0Sj"
      },
      "outputs": [],
      "source": [
        "# def perturb_target(y):\n",
        "#   # if y <= 4:\n",
        "#   #   # p=[0.125, 0.125, 0.125, 0.125, 0.125]\n",
        "#   #   p = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "#   #   p[y] = 1\n",
        "#   #   return np.random.choice(5, 1,p = p)[0]\n",
        "#   # else:\n",
        "#   #   return y\n",
        "#   if y == 6 or y == 8:\n",
        "#     return 0\n",
        "#   if y >= 5:\n",
        "#     return 1\n",
        "\n",
        "#   if y <= 4:\n",
        "#     p=[.75, .25]\n",
        "#     return np.random.choice([0, 1], 1,p = p)[0]\n",
        "# def unperturb_target(y):\n",
        "#   return y\n",
        "\n",
        "# def get_mini_mnist(mnist_train_data, Y_perturbed):\n",
        "#   X, Y = [], []\n",
        "#   for idx, data in enumerate(mnist_train_data):\n",
        "#     X.append(data[0])\n",
        "#     Y.append(data[1])\n",
        "\n",
        "#   mini_X = []\n",
        "#   mini_Y = []\n",
        "\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   for i in range(10):\n",
        "#     idxs = rng.choice(np.where(np.array(Y) == i)[0],50, replace = False)\n",
        "#     mini_X.extend([X[idx]  for idx in idxs])\n",
        "#     mini_Y.extend([Y_perturbed[idx]  for idx in idxs])\n",
        "  \n",
        "#   return mini_X, mini_Y\n",
        "\n",
        "# def get_transformed_data(mnist_train_data):\n",
        "  \n",
        "#   X, Y = [], []\n",
        "#   for idx, data in enumerate(mnist_train_data):\n",
        "#     X.append(data[0])\n",
        "#     Y.append(perturb_target(data[1]))\n",
        "#   return X, Y\n",
        "\n",
        "# def get_transformed(X, Y):\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   for i in range(10):\n",
        "#     idxs = rng.choice(np.where(np.array(Y) == i)[0],50, replace = False)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "oagFjEoL15t7"
      },
      "outputs": [],
      "source": [
        "def get_mini_mnist(X, Y, mode1_idxs, mode2_idxs):\n",
        "\n",
        "  mini_X = []\n",
        "  mini_Y = []\n",
        "\n",
        "  rng = np.random.default_rng(0)\n",
        "\n",
        "  \n",
        "\n",
        "  idxs = rng.choice(mode1_idxs ,50, replace = False)\n",
        "  x, y = X[idxs], Y[idxs]\n",
        "  mini_X.extend(x)\n",
        "  mini_Y.extend(y)\n",
        "\n",
        "  idxs = rng.choice(mode2_idxs ,50, replace = False)\n",
        "  x, y = X[idxs], Y[idxs]\n",
        "  mini_X.extend(x)\n",
        "  mini_Y.extend(y)\n",
        "  \n",
        "  return mini_X, mini_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "lG6ZO8NhM8OR"
      },
      "outputs": [],
      "source": [
        "def get_mnist_modified_data(a, b):\n",
        "  mnist_train_data = torchvision.datasets.MNIST(root = '/MNIST', train = True, download = True,  transform=ToTensor())\n",
        "\n",
        "  # X, Y = get_transformed_data(mnist_train_data)\n",
        "  a, b = a, b\n",
        "  modes = 2\n",
        "\n",
        "  X, Y = [], []\n",
        "  Y_perturbed = Y\n",
        "  for idx, data in enumerate(mnist_train_data):\n",
        "    X.append(data[0])\n",
        "    Y.append(data[1])\n",
        "  rng = np.random.default_rng(0)\n",
        "\n",
        "  landmarks = []\n",
        "  for i in range(5):\n",
        "    idxs = rng.choice(np.where(np.array(Y) == i)[0], a, replace = False)\n",
        "    landmarks.extend(idxs)\n",
        "\n",
        "  for i in range(5,10):\n",
        "    idxs = rng.choice(np.where(np.array(Y) == i)[0], b, replace = False)\n",
        "    landmarks.extend(idxs)\n",
        "\n",
        "  for i in landmarks:\n",
        "    Y[i] = rng.choice([0,1], 1, p = [.5, .5], replace = False)[0]\n",
        "\n",
        "  mask=np.full(len(Y),False,dtype=bool)\n",
        "  mask[landmarks]=True\n",
        "  X = [np.array(x).flatten() for x in X]\n",
        "  X_landmarks, Y_landmarks = np.array(X)[mask], np.array(Y)[mask]\n",
        "\n",
        "  Y_temp = np.array(Y)[~mask]\n",
        "  mode1_idxs = []\n",
        "  for i in range(5):\n",
        "    idxs = np.where(Y_temp == i)[0]\n",
        "    mode1_idxs.extend(idxs)\n",
        "\n",
        "  mode2_idxs = []\n",
        "  for i in range(5,10):\n",
        "    idxs = np.where(Y_temp == i)[0]\n",
        "    mode2_idxs.extend(idxs)\n",
        "\n",
        "\n",
        "\n",
        "  X_data = np.array(X)[~mask]\n",
        "  neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "  neigh.fit(X_landmarks, Y_landmarks)\n",
        "\n",
        "\n",
        "  Y_data = neigh.predict(X_data)\n",
        "  scaler = StandardScaler()\n",
        "  X_data = scaler.fit_transform(X_data)\n",
        "\n",
        "\n",
        "  mini_X, mini_Y = get_mini_mnist(X_data, Y_data, mode1_idxs, mode2_idxs)\n",
        "  \n",
        "  X_mode1_data, Y_mode1_data = X_data[mode1_idxs], Y_data[mode1_idxs]\n",
        "  X_mode2_data, Y_mode2_data = X_data[mode2_idxs], Y_data[mode2_idxs]\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1, random_state=42)\n",
        "  X_train_m1, X_test_m1, y_train_m1, y_test_m1 = train_test_split(X_mode1_data, Y_mode1_data, test_size=0.1, random_state=42)\n",
        "  X_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X_mode2_data, Y_mode2_data, test_size=0.1, random_state=42)\n",
        "\n",
        "  return (X_train, X_test, y_train, y_test), (X_train_m1, X_test_m1, y_train_m1, y_test_m1), (X_train_m2, X_test_m2, y_train_m2, y_test_m2), (mini_X, mini_Y)\n",
        "\n",
        "def get_dataloaders(X_train, X_test, y_train, y_test, batch_size):\n",
        "  if is_classification:\n",
        "    X_train, X_test, y_train, y_test = torch.tensor(np.array(X_train), dtype = torch.float32), torch.tensor(np.array(X_test), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.long),torch.tensor(np.array(y_test), dtype = torch.long)\n",
        "    X_train_PWC, X_test_PWC, y_train_PWC, y_test_PWC = torch.tensor(np.ones(X_train.shape), dtype = torch.float32), torch.tensor(np.ones(X_test.shape), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.long),torch.tensor(np.array(y_test), dtype = torch.long)\n",
        "  else:\n",
        "    X_train, X_test, y_train, y_test = torch.tensor(np.array(X_train), dtype = torch.float32), torch.tensor(np.array(X_test), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.float32),torch.tensor(np.array(y_test), dtype = torch.float32)\n",
        "    X_train_PWC, X_test_PWC, y_train_PWC, y_test_PWC = torch.tensor(np.ones(X_train.shape), dtype = torch.float32), torch.tensor(np.ones(X_test.shape), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.float32),torch.tensor(np.array(y_test), dtype = torch.float32)\n",
        "  \n",
        "  train = DATA(X_train,y_train)\n",
        "  test = DATA(X_test,y_test)\n",
        "  train_PWC = DATA(X_train_PWC,y_train)\n",
        "  test_PWC = DATA(X_test_PWC,y_test)\n",
        "  train_dl = torch.utils.data.DataLoader(train,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True\n",
        "                                          )\n",
        "  \n",
        "  test_dl = torch.utils.data.DataLoader(test,\n",
        "                                            batch_size=len(y_test),\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  train_dl_PWC = torch.utils.data.DataLoader(train_PWC,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  \n",
        "  test_dl_PWC = torch.utils.data.DataLoader(test_PWC,\n",
        "                                            batch_size=len(y_test),\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "\n",
        "\n",
        "  return train_dl, test_dl, train_dl_PWC, test_dl_PWC\n",
        "\n",
        "def get_mini_dls(X_mini, y_mini, batch_size = 100):\n",
        "  X_mini, y_mini = torch.tensor(X_mini), torch.tensor(y_mini)\n",
        "  mnist_mini = MNIST(X_mini,y_mini)\n",
        "\n",
        "  mnist_mini_dl = torch.utils.data.DataLoader(mnist_mini,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  return mnist_mini_dl, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "cHkuKOpOaulU"
      },
      "outputs": [],
      "source": [
        "class DATA(Dataset):\n",
        "      def __init__(self, X, Y):\n",
        "          self.size = len(Y)\n",
        "          self.x = X\n",
        "          self.y = Y\n",
        "\n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float(), self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "70tWh3oLnVZu"
      },
      "outputs": [],
      "source": [
        "# labels_map = {\n",
        "#     0: \"0\",\n",
        "#     1: \"1\",\n",
        "#     2: \"2\",\n",
        "#     3: \"3\",\n",
        "#     4: \"4\",\n",
        "#     5: \"5\",\n",
        "#     6: \"6\",\n",
        "#     7: \"7\",\n",
        "#     8: \"8\",\n",
        "#     9: \"9\",\n",
        "# }\n",
        "# figure = plt.figure(figsize=(8, 8))\n",
        "# cols, rows = 3, 3\n",
        "# sample_idx = 0\n",
        "# for i in range(1, cols * rows + 1):\n",
        "#     # sample_idx = torch.randint(len(mnist_train_data), size=(1,)).item()\n",
        "#     img, label = mnist_train_data[sample_idx]\n",
        "#     sample_idx += 1\n",
        "#     figure.add_subplot(rows, cols, i)\n",
        "#     plt.title(labels_map[label])\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "puNEm1gA0fii"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "      def __init__(self, X, Y):\n",
        "          self.size = len(Y)\n",
        "          self.x = torch.tensor(X)\n",
        "          self.y = torch.tensor(Y)\n",
        "\n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float(), self.y[idx].float()\n",
        "\n",
        "def make_circle_dataset(n_data_points, a, b, plot = False):\n",
        "  n_data_points = n_data_points\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data_points)\n",
        "\n",
        "  x=np.zeros((len(angles),2))\n",
        "  y=np.zeros(len(angles))\n",
        "\n",
        "  a=a # Number of half cycles in the top half of the circle\n",
        "  b=b # number of half cycles in the bottom half of the circle\n",
        "\n",
        "  for idx, angle in enumerate(angles):\n",
        "      x[idx,0]=np.cos(angle)\n",
        "      x[idx,1]=np.sin(angle)\n",
        "      if angle<np.pi:\n",
        "          y[idx] = np.sin(a*angle)\n",
        "      else:\n",
        "          y[idx] = np.sin(a*np.pi+b*(angle-np.pi))   \n",
        "\n",
        "  # plt.plot(np.concatenate((angles,angles+np.pi*2)),np.concatenate((y,y)))\n",
        "  \n",
        "  if plot:\n",
        "    plt.plot(angles, y)\n",
        "    plt.figure()\n",
        "    plt.axis('equal')\n",
        "    plt.scatter(x[:,0],x[:,1],c=y)\n",
        "  return x,y\n",
        "\n",
        "def get_sorted_circle_dataset(X,Y):\n",
        "\n",
        "  angles = [angle + 2*np.pi if angle < 0 else angle for angle in np.arctan2(X[:,1],X[:,0]) ]\n",
        "\n",
        "  sorted_arg = np.argsort(angles)\n",
        "  X_sorted = X[sorted_arg]\n",
        "  Y_sorted = Y[sorted_arg]\n",
        "  Y_sorted = Y_sorted.reshape((len(Y_sorted),1))\n",
        "  return X_sorted, Y_sorted\n",
        "\n",
        "def get_tempdataloaders(X,Y):\n",
        "  batch_size = 32\n",
        "  Y = Y.reshape((len(Y),1))\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, shuffle = True)\n",
        "  train_dataloader = DataLoader(CustomDataset(X,Y), batch_size = batch_size, shuffle = True)\n",
        "  # test_dataloader = DataLoader(CustomDataset(X_test, y_test), batch_size = batch_size, shuffle = True)\n",
        "  dataloader_all = DataLoader(CustomDataset(X,Y), batch_size = len(Y), shuffle = False)\n",
        "  return train_dataloader, dataloader_all\n",
        "\n",
        "def plot_dataset(X_sorted, Y_sorted, points_idxs, save_fig):\n",
        "  n_data_points = len(X_sorted)\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data_points)\n",
        "  f, axes = plt.subplots(1,2, figsize = (2*5,5))\n",
        "  axes[0].plot(angles, Y_sorted)\n",
        "  axes[0].set_xlabel(\"angle in radians\")\n",
        "  axes[0].axis('equal')\n",
        "  axes[1].scatter(X_sorted[:,0],X_sorted[:,1],c=Y_sorted)\n",
        "  \n",
        "  for i, point in enumerate(points_idxs):\n",
        "    axes[1].scatter(X_sorted[point,0],X_sorted[point,1], c = 'r')\n",
        "    axes[1].annotate(f'x{i+1}', (X_sorted[point,0],X_sorted[point,1]), xytext = (X_sorted[point,0]-.15,X_sorted[point,1]) )\n",
        "  if save_fig:\n",
        "    plt.savefig('input_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "8BsFX08lxkID",
        "outputId": "56482ccb-3535-4188-bb4c-84ea87893c6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABJ00lEQVR4nO2deZhcZZW431Nda+9r9n0hEBYDRBBQAQEF9UdQcQYclHFUHAV1XGbEbVxGZ2CcGfdlGERxRcQFHEFEFpVBliABAiQkJCGdvdNb9Vb79/vj3ltdSXqpqvvdW1Xp732eflJ1762q07c653xn+c4RpRQGg8FgmLkEKi2AwWAwGCqLMQQGg8EwwzGGwGAwGGY4xhAYDAbDDMcYAoPBYJjhBCstQDl0dnaqJUuWVFoMg8FgqCkef/zxg0qprsOP16QhWLJkCevXr6+0GAaDwVBTiMiLEx03oSGDwWCY4RhDYDAYDDMcYwgMBoNhhmMMgcFgMMxwjCEwGAyGGY4WQyAiN4nIARHZOMl5EZGvishWEXlKRE4pOHeliGyxf67UIY/BYDAYikeXR/A94MIpzl8ErLR/rgK+BSAi7cCngdOB04BPi0ibJpkMBoPBUARa9hEopf4oIkumuGQd8H1l9bx+WERaRWQucA5wj1KqD0BE7sEyKD/RIZehNBLpLFsPDLOjd4T+kRQDo2kAwsEAzbEQC9piLGqvZ1F7PSJSYWlnLo/t6COTVZyxvKNiMiil+PVTezlhXjPLuhorJkc2p/jtxn2smtPEilmVk2MkmeG3G/dx7rGzaG8IV0yOcvFrQ9l8oLvg+S772GTHj0BErsLyJli0aJE3Us4wMtkcj2zv4w/P9/DH53vYcmCYbG76+RSt9SFOXdTGmSs6ueiEOcxrjfkgrQHgoRcO8pb/eQSA+z58dsWU8O0b9vAPP90AwPpPnk9nY6Qicnzj/q381z3PM68lyr0fPodYuK4iclz/2018/88vcuL8Fu645qyaWyjVzM5ipdQNwA0Aa9euNdN0XLCtZ5ifPtbNL57YTc9QknBdgLVL2rj6nOWsmtPM8lkNtDeEaY1ZK5t0Nkf/aIpd/WPsODjCEzsHeGxHH/duOsC//O+znLKolb85fTGvf8lcIsHK/EecKXz/ofGNod//84t85uLjKyLHdx/akX/8qyd2885XLPNdhmxO8ZNHdwKwZzDB/ZsP8NoT5/oux3Ayk5fj6d2D/GXnAKcurq0It1+GYDewsOD5AvvYbqzwUOHxB3ySacbx7J4437h/K3du3EtAhFcdO4s3nbKAV6zspCEy+Z9COBigIRJkQVs9L1vWwWWnWR7Ztp5h7tq4j1/8ZRcf/tmT/Ntdz/H2s5by9rOWUB+umTVGzZDNKf60pYe/OX0R3f1jPLj1YEXk6BtJ8dSuAT54/jHctXEv9206UBFD8NzeOHsHE/zHm1/CZ3/9DH/Y3FMRQ/Do9l7SWcW3rziV9/7ocf74fI8xBJNwB3CNiNyClRgeVErtFZG7gX8tSBC/GviYTzLNGPYNJrj+t5v45RO7aYoEec/Zy/nbs5Ywqynq6n2XdTVy9bkreO85y3lw60G+8+B2vnj3Zr730A4+cN5KLj9tEXWB2nKRq5nN+4YYSWV56ZJ2FrUn+Le7NtEzlKSryd+wzKPb+1AKXr6yk57hBL96Yg/ZnPL9u36iewCA05e2c+byDh7e3uvr5zs8vK2PcDDAOau6WDWnmb/s7K+IHG7QYghE5CdYK/tOEdmFVQkUAlBKfRu4E3gtsBUYBd5un+sTkX8BHrPf6nNO4tjgnmxOceOftvGVe7eQySmuPnc5V71yOS2xkNbPERFesbKLV6zsYv2OPq7/7SY++auN/Gx9N//6xhM5fl6L1s+bqWywFd8pi9rYNTAKWKvirqYjmkl6yqZ9cURg9dxmth9s44cP7+SFnmGOmd3kqxxPdg/Q0RBmQVuME+a1cPcz+xlJZqb0br3g2T1xVs1uIhqq45RFrdyxYQ9KqZrKE+iqGrp8mvMKuHqSczcBN+mQwzDOtp5hPvyzJ3li5wAXrJ7Np163mkUd9Z5/7tol7dz67jO448k9/Mv/PsvFX/8/rj5nOe8/byXBOrN/0Q3P7x+iIVzHwvYYDZG6/LFXHuOzIdg7xNKOBmLhOk6Y32wd2zfkuyHYtC/O6nnNiAjHzrXk2Lx/iFMW+RuW2bx/iLPt7+DYuc386JGd7IsnmNtSO0UU5n/mUchtj+/itV/9E9t6RvjKZWu44a2n+mIEHESEdWvm8/sPnc0la+bz1fu28tc3PMyu/lHfZDgaeaFnmGVdjYgIHY0ROhsjbN435Lscm/bFWTXHUvpLOhoQsRYefqKUYnvPCMvtqqljbXk27fX3fvSNpOgZSrLKNoLLuxoAeOHAiK9yuMUYgqOIRDrLx37xNB/52ZOsWdjK7z74StatmV8xF7W1Psx//tVL+Mpla3h+3xCv++qDPLilMgnOo4FtPSN5RQOwclYjL/isgDPZHN39Y3kFHA3VMa8lxrYefxXf/niSkVQ2fz/mt8YIBwO82OuvHI4BdPYwrLDvi9/fi1uMIThK6BtJ8Tc3PsJPHt3J35+9nB++43RmN7tLButi3Zr5/Ob9r2BOc5Qrv/so3/2/7VjRQkOxJNJZdg+MHbJvYGF7jO7+MV/l2DuYIJtTLGof9zCXdTWw/aDPCvigpWiXdlr3IxAQFrbF2Nnnr9fZbXu5C+370dUUoTES9N1DcosxBEcBL/aO8KZvPcTTuwf5xltO4dqLjq26ePyijnp+/t4zOe/YWXz218/y2V8/S66IzWsGi72DCcBa+Tosaq+nZyjJWCrrmxyOol1YYAiWdDSww+eVeLctx+KCkOei9nrfDcHOXssQL2izvhcRYUFbjF0+G2i3VJe2MJTMk90DvPGbD9E/muLH7zyd153kfx11sTRGgnz7ilN558uX8r2HdvDBWzeQzuYqLVZNsHfQUixzW8a9PEcZ+5l7cRRtYc5pfluMoUSGoUTaNzn2DSYBDvF6F7XXs7N31Fdvs7t/lNnNEaKh8Y2UC9pi7B4whsDgE4+/2M/f3PgI9ZE6fv6eM1m7pL3SIk1LICB84nXH8Y+vWcXtG/Zw1ffX+7qirVX2xy2PYM4EhqDbR0Owq3+UYECYU6CAnRYjjtfiB/viCToawoSD4ypsYXs9Q8kM8bGMb3J0942ysO3QQoz5rTF2G4/A4AePv9jHlTc9SmdjmFvffUY+eVcLiAhXn7uCf33DiTzwfA/v+v56EmljDKbCUbKFhsDxDpzVsV9yzG6OHrJ5bH6rJYefq+D98cQROTDn3uyL+2eQ9g4mjui1Nb8txlAyw+CYfx6SW4whqEHW7+jjbd95lK6mCLdcdUZN1SsX8pbTF/HFS1/Cg1sP8t4f/YVUxoSJJmPfYIKWWOiQ1h2djRFE4MCQjyvxwQSzmw/dyewowj0+GoJ9g4lDwmRA3ktxwmheo5RiXzxxiHEGmN9qeQh+3g+3GENQYzy7J87bv/sYs5uj3HLVy474I6w1Lj11AZ+/5ATu23SAD9zyBBmTM5iQvYOJQ8IxAKG6AO31YQ4M+ecRTLRRalZTlIDA3gF/Q0OzDzcE9vP9PnkEA6NpUpncEZ7JLNtQ+vm9uMUYghqiu2+UK7/7KA2RID94Z/WUh7rlipct5lOvX81dG/fx8V8+bUpLJ2Df4JErT7DKFXt8UjhKKdsjOFSOuoDQ3hDh4LA/ciQzWfpGUkcYRqd3ll+5CicEdbgcXXZL7oM1ZAhMi8gaoXc4ydtuepRUJsfP/v6MQ8oIjwbe8fKlDI6m+Op9W1nQVs/7z1tZaZGqin3xBMfPaz7ieFdTxLeV51Ayw2gqe0RIBqCzMeybITgQtz7ncAUcDgbobIywz29D0HJoqKzTbgLo1/3QgTEENUAineUdN69n7+AYP3rn6b73dPGLD15wDLsGxqxBI60xLj11QaVFqgpSmRwHh5MTegSzmqK8cMCf3dqO5zFRt9Oupgg9wylf5Ng3QQWVw6wm/zyTA7Ych3fxbQjXEQ0FfPPUdGBCQ1WOUoqP/vwpNnQP8OW/PplTF1d/iWi5iAjXvfEkzlzewbU/f4qHKtRvv9o4MJRAKSZciVsKOOlLOK3XVvQdjUeOYuxsjPgWCpmogsqhozHMQZ8MkvM5hxtGEaHLR4OkA2MIqpxvPvACt2/Ywz++ZhUXnjCn0uJ4TjgY4NtvPZWlnQ2898d/ye8gnck4oY6JckKzmiKksyo/X9pLem3F1tFwpEfghIb8MEj7p7gfHQ1hekf8UcC9wynqw3WHbCZz6Gy0DHStYAxBFfO7Z/bxxbs3c/FL5vHec5ZXWhzfaI6GuOFta8nmFFf94PEZv+FsspVn4TE/8gS9I5YcnZN4BMlMjuGk95u59scTxEJ1NEePjGx3NEbo88kj6BtJTugdgZUwPjjkjxw6MIagStl+cIQP3fokL1nQwr9felJNDbnQwdLOBr56+cls2hfnn37+1IyuJOqzFXB7w5FKZ1beEHifIHVCQ20TyOEMr/cjLNM3kqKjMTzh/4n2hjAjqawvi4fekRTtE3hHYCWMZ5xHICIXishmEdkqItdOcP5LIrLB/nleRAYKzmULzt2hQ55aJ5HO8t4f/YVgnfDNK06d0PWcCZy7ahYfefUqfv3kHr7z4PZKi1Mx+kdtBVw/gSGwwyN+JCZ7R5K0xEKEJmho2OVjpUzfaGpCowjj3oof4aHe4RQdk8jR1RihfzRVM/tiXFcNiUgd8A3gAmAX8JiI3KGUeta5Rin1wYLr3wecXPAWY0qpNW7lOJr4zB3P8NzeON99+0uPujLRUnnvOct5atcA1921iVMXt3Gyz9OnqoHe4ZRdiXLkgsDv0NBkoZBOH2vn+0ZSExpFGM9f9I2kWNDm7TCmvpHUhCW9YHkESlnXzKqB/T46PILTgK1KqW1KqRRwC7BuiusvB36i4XOPSn7++C5ueaybq89dzrmrZlVanIojIvz7m17C7OYo7/vJEzXVv0UX/aMp2idRwI2RILFQnT8ewXBy0hVwZ5N13BePYGTylbhzn3o9DlEppegbmfx76bKP18ruYh2GYD7QXfB8l33sCERkMbAUuK/gcFRE1ovIwyJyyWQfIiJX2det7+np0SB29bFl/xCf/NVGTl/azgfPP6bS4lQNLfUhvnr5yewdTPDxX8y8nce9IynaJ1kBgxUX7x/xPjZvhUImjom314cRwZe9BP0jqQnzFACdDf6EqIaSGVLZ3OShoRrbVOZ3svgy4DalVGEmZ7FSai3wFuDLIjJheYxS6gal1Fql1NquLn+HdftBMpPl/bdsoD5cx9cuP7nqBstUmlMXt/GRV6/iN0/v5ceP7qy0OL7SPzJ5TBygtT6UzyN4Sd8UoaGg3feo12PFl0hnGUllJ70fjnx9HhtGpzJpMsPohMpqZVOZDm2zG1hY8HyBfWwiLuOwsJBSarf97zbgAQ7NH8wY/uue53lub5x/v/SkmogpVoJ3v3IZrzymi8/9+lme3+//0PZK0TfFChisJHK/x/sIsjlF3+jkIRmwN5V5bAic/RKT5Qjqw3VEgoF8qatXOO8/WWiowzYEfhhoHegwBI8BK0VkqYiEsZT9EdU/InIs0Ab8ueBYm4hE7MedwFnAs4e/9mjn4W293PDHbVx+2iLOO252pcWpWgIB4b/+6iU0RoJ8aAZNN5sqJg6WRzDgscLpH02h1LiCm4i2hhD9I94aJKcaqL0hNOF5EfHFIDkex2TfS0O4jmBAPDfQunBtCJRSGeAa4G7gOeBWpdQzIvI5Ebm44NLLgFvUoQHe44D1IvIkcD9wXWG10Uwgnkjz4VufZHF7PZ983XGVFqfq6WyM8IU3nMjG3XG+ft/WSovjOWOpLGPpbMU9gqn2MhTKMTDmsUGyDc1k9fvWubDnoaH8LutJDKOI2Aa6NgyBlqZzSqk7gTsPO/bPhz3/zASvewg4UYcMtcpnbn+GffEEt/39GTRETA/AYrjwhDm88eT5fP3+rZx/3GxOXNBSaZE8o2906pUnQFt9iHgiTTanDpkcppODecU3nWfisUEadQzSxB4BWDJ6XTXUO41HANBaH2bQY8OoC5ORrCC/e2Yfv3hiN9ecu2JG1se74dP/73i6GiN86NYNR/WYSycpOVlMHCyFoxSeltaOh0ImX4m3xMIMjKU9repyqqOmuh/tDd4nrftGJu8z5NAa8z5UpgtjCCrE4FiaT/5qI8fNbeaaV62otDg1R0t9iOsvPYktB4b5z99trrQ4njG+Ap7CI7BXx14mJp2Vfmv95Cvx1voQqUyOMQ8Nc99IChFoiU0hRyzs+X6TqTa15eWotwxjLWAMQYX41988R+9Iii9eetKEW/YN03P2MV285fRF3PjgdjZ0D1RaHE/oyydHp/YIAE8Txo5inVoBh2w5vPVMWmKhKcurW2IhRlJZT4sJBsfSUxpFsAzj4AyqGjKUyINbDvLT9d1c9cplnDD/6I1v+8HHLjqW2U1Rrv35U0dlFVFfPjk6dZIW8DQMER9LEwkGpg6F1PtgCEan3lxXKEfcw9V4UYYgFpo5VUOG0hhJZrj2F0+xrLOBD5hxjK5piob43Lrj2bRviBv+uK3S4mhnYNQKhTRHJ1c6bfX+hIam8gagwDPxMEE63eY6GPdavAzLDIympr0fbQ1hxtLZmshhGUPgM//xu83sHhjj+ktPmrFdRXXz6uPncNEJc/jKvVvYfnCk0uJoZXAsTXM0RGCKaqDx0FCFV8D2+UEP5egfnV6OFkcOTz2CzLSGwDnvpWeiC2MIfGTj7kFufmgHV5y+mJcuOXpHTlaCz158PJFg4KjrRTQ4Nv1KvDkapC4g3noEY9OvgFtjdojKQ0MQH0vTXKQC9sogKaWIj6VpiRUXoqqF8JAxBD6Ryyk+8auNtDdE+MhrVlVanKOOWc1RPv7a4/jztl5+tn5XpcXRRjGGQEQ8j0dbK+DiFJ+XoaF4EffDSVp75RGMpbOksrnpQ0M+JPF1YQyBT9zyWDdPdg/wydcdN+0fkKE8/nrtQl66pI1/u+u5mvjPVwzFKD7wvs3EYBEx8WjI6vPj1Uo8m1MMJTNT5kugIEfg0f1wDMy0ISofchW6MIbAB3qHk1z/2028bFk769bMq7Q4Ry2BgPC5dScQT2T4j6Nkb0ExHgE4bSa8LR+dTvGBt7uL40WUsBaeHxzzZn5yMaW0UFhFVf2LEmMIfOC6uzYxkszw+UtOmHGzh/3muLnNvO2MxfzokZ08vWuw0uK4ZnAsQ3Ns+tYjrfUhzxRfOptjJJUt2iB5FRqKJ4pTwMG6AI2RoGdyOIau+NCQ8QhmPI/t6ONnj+/iXa9cxopZTZUWZ0bwwQuOoaMhwqdu30guV7uJYycpOV1yFKzyUq+qU4oNhYClHL3KVRS7Eneu8SpHUKwc9eE6QnViQkMznUw2x6d+tZH5rTHeZ9pI+EZzNMTHX3ssG7oH+Nnj3dO/oEpJpHNFJSUBmmOh/IpZN6UoYGs3rbdyFGMYW2LeG8ZikvgtsbAJDc10fvzoTjbtG+JTr19Nfdh0FvWTN5w8n5cuaeP6326uif+IE1GKAm6OBhlOZjzxgIoNhYBVQupZaMgOfRWfPPfIEDj3oygPKVgTc7aNIfCIgdEU/3XP85y5vIPXHG+GzfiNiPDZi09gYDRVs4njYmPiYK2SlbJm6WqXo5SQTH3lQzLONV7KURcQmopoG98cCzGU8CZ3oxNjCDziy7/fQnwszadev9okiCvE6nnNvO2MJfz4kZ1s3ld7oy1L8gg83MXqrPCLkaMpErRCWhn9fZ/GQ0PTK+CWWMiz2Ly12ztY1P/rpmiI+EwxBCJyoYhsFpGtInLtBOf/VkR6RGSD/fPOgnNXisgW++dKHfJUmq0HhvjBwy9y2WmLOG5uc6XFmdF84LyVNEVDfP43z9bcjuPBEkIyTm29F3mCwXwL6qk3lAE0RS0lPeSBHPFEmlCdECuiNYvjmXjxnQ8UWdILVshuaCaEhkSkDvgGcBGwGrhcRFZPcOlPlVJr7J8b7de2A58GTgdOAz4tIjU/oeXzv3mO+nAdH77gmEqLMuNpawjz/vNW8qctB3ng+Z5Ki1MS+RXwNBuoYHyVHPeghHQgL0dxoRDAk3CIs6eimJV4S8yajZBIe+OZtBRhFGFmeQSnAVuVUtuUUingFmBdka99DXCPUqpPKdUP3ANcqEGminH/5gM8sLmHD5y3cspB3wb/eOvLFrO0s4Ev/Oa5mmpVXVqy2DuPID6WoTESnHIGgENT1FtDUIxRhMJNZR54SCV6BF5Vc+lEhyGYDxTW6O2yjx3Om0TkKRG5TUQWlvhaROQqEVkvIut7eqpzZZfO5vj8/z7L0s4G3nbGkkqLY7AJBwN87KJj2XpgmJ88urPS4hRNqeWS4E2OYCiRzod8psPT0FCReyosORyD5I0cRRsC2zNJZqq7FbVfyeJfA0uUUidhrfpvLvUNlFI3KKXWKqXWdnV1aRdQBz98+EVe6BnhE689jnDQ5OGriQtWz+Zly9r50j3P10Q5H1ir+6ZIsKiB9OMegf6V+HDS8giKwVvPpHgF7BgkL+5HeYaxusNDOrTVbmBhwfMF9rE8SqlepZQzTfpG4NRiX1srDI6m+fLvt/DyFZ2cd9ysSotjOAwR4ZOvW83AWJqv37el0uIUxWAJK+BGR/F54hFkSlZ8XijgUkMy4I1HMJTIFFU6CgX3o8oXHzoMwWPAShFZKiJh4DLgjsILRGRuwdOLgefsx3cDrxaRNjtJ/Gr7WM3xzT9sJZ5I8/HXHmfKRauUE+a3cOkpC/jeQzt4sbf6B9iUEgpx6tq9WIkPJTM0Fhmbb/Y6R1BE6Sh4l6uwwjy5og2jl/dDJ64NgVIqA1yDpcCfA25VSj0jIp8TkYvty94vIs+IyJPA+4G/tV/bB/wLljF5DPicfaym2DMwxnf/bwdvWDOf1fNMuWg185HXrCJUF+C6uzZVWpRpGUpkiqrUcWiOhTypGiolFOKVZ6KUIp6YfiqYg1chmWF7w16xobImD0NlOtHS90ApdSdw52HH/rng8ceAj03y2puAm3TIUSm+dM/zoOBDrzblotXO7OYo73rFMr5y7xY2dA+wZmFrpUWalKFEhnmt0aKvb/ZoN+1wCaGQuoDQGAlqV8Bj6SzZnMor1unwKlk8bP9excrheDBHvUcw09m8b4if/2UXbztjMQva6istjqEI3vXKZXQ0hLnurueqepPZcDJTtMIB70oVLTmKXzM2RYMeKuDi5GgI1xEQ/QrYub+NRedMvKte0okxBC65/rebaIgEufpc0120VmiMBLnmVSt4eFsff6jiTWZDiXTRIQhwQkN6FU4mm2M0laUxUrxBsgyBbgVcWkhGxPFMNBskOzRUerLYeARHLQ9v6+W+TQd47zkraGsobqehoTp4y+mLWNge4/rfbq7KmQVKKatss5QcQVR/g7ORpFX/XopH0BzV3xI7r4BL8kz034+hEkNDjeEgIsYjOGpRSnHdXZuY0xzl7WctqbQ4hhKJBOv48AWreG5vnF8/tafS4hxBMpMjnVWlKeBYULtHUGooBLzxCEqNzTty6C5jHU6Wdj8Cds6k2ttMGENQJr/duI8N3QN86IJjiBbRBMtQfVz8knkcN7eZ//zd8550y3RDfuVZSmgoGmIomSGr0cNxVuKlVC9ZK3G9Bsl5v5JCZZ7IUbpn4oWHpBtjCMognc3x73dv5pjZjbzp1AWVFsdQJoGA8E8XrmJn32jVtZ7IlymWuBIHGEnpW30O5WPzlV2JD5VYtunI4VVoqNJy6MYYgjK4dX032w+O8E+vObao7f+G6uWcY7o4fWk7X7tvCyMeDHUpF2cl21SiAobxMIoOSg2FgDOMRW8LaOd3KrbpHNgKOKk/VxGuC5QUBfBynrQujCEokUQ6y9fu3cqpi9tMK4mjABHhoxcdy8HhFDf+aXulxcnjKL5SFLCzate5+iwnFNIUDZLOKpIaw22OHA2R4hWwN8nidEnfiSWH8QiOOn78yE72xRN8+NXHmFYSRwmnLGrjNcfP5oY/vkDvcHL6F/jAUBlVMo6CGta4Ci4nV+HFbtrhZJpYqK6oVtjjclgKWLdnUsp3AnZZr8kRHD2MpjJ884GtnLm8gzOXd1ZaHING/vE1qxhLZ/nvP26rtChAoQIuPTTkjUdQ2sY20Fs7X+qmNrBkzuYUY2l9LaCHEsV3Yh2Xw3gERxU3P/QiB4dTfNi0kjjqWDGriXVr5vP9P+/gwFCi0uIwXE7ZZkS/IRhOWoPao6HiVUWzB7tp44nS9lSAR4axhJbcDk71UjXvYjeGoEjiiTTf/sMLnLuqi1MXt1daHIMHfOC8laSzim/e/0KlRSm5uRkUhob0egRNRQ5qd/BCAVshmeK9kkPl0BsqK0eOnIKRVPUOpzGGoEi+86ftDI6l+fCrV1VaFINHLOls4NJTFvDjR3ayZ2CsorIMJTJEgoGSBhw5Ckpr1VBZoRAvktbpkvIU4M2wnuFk8Z1YHWqh35AxBEXQP5LiOw9u56IT5nDC/JZKi2PwkPedtwKF4uv3b62oHENlxMTrQ3Xa2xnEy1gBOx039SaLy4vNgxeeSanJ4urvQGoMQRH89x+3MZLK8MELTG7gaGdBWz2XvXQRtz7WTXffaMXkKCcEEQgIjeFgvuJIB8PJ0lfiXqyAy1HAuuVQSpWZLPZunrQutBgCEblQRDaLyFYRuXaC8x8SkWft4fX3isjignNZEdlg/9xx+GsrzYGhBN97aDvrXjKPY2Y3VVocgw9cfe4KAgHhq/dWbqTlcImdRx0ao0HNG8pKV8BetIAeqoJkcTKTI1PCTASv5PAC14ZAROqAbwAXAauBy0Vk9WGXPQGstYfX3wb8e8G5MaXUGvvnYqqMbz3wAums4gPnG29gpjCnJcoVpy/mF0/sZlvPcEVkKEcBg/5SxXIUsNMCWtcKOJdTDKeKH47joDtZXE4DPijMVRzdHsFpwFal1DalVAq4BVhXeIFS6n6llONnP4w1pL7q2TMwxo8e3smlpyxgaWdDpcUx+Mh7zllOuC7AVyrkFZQTggCrykhn1VA5IRlw2kzokWM0nUWp0vYyADTkW0DrkWO8zUWphsDJmRzFHgEwH+gueL7LPjYZ7wDuKngeFZH1IvKwiFwy2YtE5Cr7uvU9Pf4ME/n6/VtRKN53nhk6M9Poaopw5ZlLuOPJPTy/f8j3zy9nJQ7QaHcg1SpHCZvaHJqiIW2Kb6jMlXhA89jMchrOwQzKERSLiFwBrAW+WHB4sVJqLfAW4Msisnyi1yqlblBKrVVKre3q6vJc1u6+UW59rJvLT1tkRlDOUN79ymU0hIN8+ffP+/7Zw8lMSQ3WHHSOiUxmsqSyubJDVLpCIcNlKmDQ2wK6nL0dANFQgGBAtHpqutFhCHYDCwueL7CPHYKInA98ArhYKZVv6KKU2m3/uw14ADhZg0yu+cb9WwmI8N5zjDcwU2lrCPN3L1/KnU/v45k9g759bn46WRmKrymiL1lcTsM5h2aNSety+i456MyZ5DvClmigRYQmzUl83egwBI8BK0VkqYiEgcuAQ6p/RORk4L+xjMCBguNtIhKxH3cCZwHPapDJFd19o9z2+C4uO20hc1qilRbHUEHe8fKlNEWDvlYQjaWzZHOlTSdz0JkjcLMSb4qGtMnhxiDp9JDcyNGoUQ4vcG0IlFIZ4BrgbuA54Fal1DMi8jkRcaqAvgg0Aj87rEz0OGC9iDwJ3A9cp5SquCH41h9eICDCe86ZMEplmEG0xEL83VlLufuZ/Ty3N+7LZ+Zj0WUpvhCjqayWKWXlNJxz0Dk4ftwglZer0JYsduOZRPQZRi8o/TeaAKXUncCdhx3754LH50/yuoeAE3XIoIvdA2P8bH03f/3ShcxtiVVaHEMV8HdnLeWmB7fz1Xu38K0rTvX888pNSkJBv6FEhpb60hXnIXI4Q2nK8gjGW0C7bdfutNUu1yPYekCvZ9JQ5vdytFcNHVV86wGrtcB7TG7AYNNSH+LtZy3hro372LTPe69gfE5wGStgpwOphpkEbkMhmZye4TRuPCStnkkyQyxUR6iEmQgOOnMmXmAMQQF7B8e49bFdvHntQua3Gm/AMM7fvXwpjZEgX7vX+x5E5ZZLFr5GRxhi2FVsXt8mqrwhCJefq9DRArqc6WQOjRH9YzN1YgxBAd964AVySvGes01uwHAorfVh/vbMJdy5ca/n+wrcJWn1tTMot0oGxj0THatgp4IqUMZ8cJ1jM52W3OXQFA0Zj6AW2DeY4JZHu7n01AUsbDf7BgxH8o6XL6U+VOd5BZGbcslGzQq48D1LQbdBKkcG/XKU3ubCodGDsZk6MYbA5tt/sLyBq881uQHDxLQ1hLnyzCX85um9bPHQKyhnTKVDXvFpCA2VMxPBIW+QdISoyuy75IUc5YaGmjTmTLzAGAJgfzzBjx/dyRtPmW+8AcOUvPMVy4iF6vjafd7lCoZdlo+CnkZr5cxE8ESOMtttFMqhw0OyhuOUV4nleBLV2njOGAIsbyCbU1xz7spKi2KoctobwrztjCX8+qk9bD3gjVcwlEhTH66jroyYuNbQUBkzERx0h2TKDQ05r9NhkIarxCB5wYw3BAfiCX78yE7ecPJ8FnUYb8AwPe96xVKiQe+8gnLbSwDUh60pZTpCIdUSmy+379Ihcui4HxpCVNU6k2DGG4Ib/riNTE5xjckNGIqkozHC285czK+f3MMLHswrcKNwnFkAuhRwuXI0aIzNV4NByuWs/k9uksWg5354wYw2BD1DSX74yIusWzOPJWbegKEErnrFMiLBOr7ugVdgxcTL3xXcrKmtgpuQTKguQCxUV/GQzHiozJ0c5c5EcNA9JEc3M9oQ/M+ftpHK5Hjfq0xuwFAaHY0R3nrGYm7foH+K2XCi9DnBhViN5/QkactVfGCPzXS5As7mFCOpbPkhGU0rcTeb/GC8AsyEhqqMg8NJfvDnF1m3Zr6ZPmYoi3e9YhnhYICv36/XK3ATkoHxmnW3DCXSruRo0tBfx81eBoBIsI5wMOD6frjZZV34OmMIqoz/+dM2Epms2TdgKJuupghXnL6Y2zfsYcfBEW3v6yYkA5bScbsCdjMTIS+HhtkIbjp+OjRHg66TxY5BK7t6yeQIqo++kRQ/+POL/L+T5rFiVmOlxTHUMFedvYxgQLR6BW7KNsEODblUwKOpLDnlTgFbLaDdhajGV+Lu7odrj8ClQQrVBYiGAiZHUE38z5+2MZbO8n4zi9jgkllNUf7m9MX88ond7Owddf1+uZxiOFV+chQ0h2TchKg0DMnJx+ZdeUgh18liN32XHBqreCbBjDME/SMpvv/QDl534lxWzGqqtDiGo4B3n72MuoDwzQfcewUjqYxVneJW8blMFutQfDrGRA5ViUFy0wjQobmKZxJoMQQicqGIbBaRrSJy7QTnIyLyU/v8IyKypODcx+zjm0XkNTrkmYobH9zGaDrL+88zlUIGPcxujnL5Sxdy2+O72NXvzivQERNvjARJpHOks+X3tRnvd+Quae06R5BwZjNUNnmu5Xup4pkErg2BiNQB3wAuAlYDl4vI6sMuewfQr5RaAXwJuN5+7WqsGcfHAxcC37TfzxMGRlPc/NCLvPaEuRwz23gDBn28++zliFjtStzgZgiLg442E26G0jg0RUMMpzLkXIzNHJ/WVlnPJJ7IIAINZcxEOFSOozdHcBqwVSm1TSmVAm4B1h12zTrgZvvxbcB5Ys2vWwfcopRKKqW2A1vt9/OEmx7cznAyw/tMbsCgmXmtMd68diG3PraL/fFE2e/jZkylg44KFR05guZoEKWscFf5cpQ/ptKhSVNoqDFc3kwEBx0hKq/QYQjmA90Fz3fZxya8xh52Pwh0FPlaAETkKhFZLyLre3p6yhK0dyTF606ay7Fzmst6vcEwFZeeuoBUNsdTuwbLfg8dsflmDTXrepKjOuSwVuL14fIDBTqmlLmZTlYoR7XuI9AyvN4PlFI3ADcArF27tqxv9AtvOJGsCzfVYJiKtvowgKtErZ4cQeiQ9yoHHZ5JvuOmSzkaI0GsAEJ5NEaDZHOKsXSW+jJDO243+YGesl6v0OER7AYWFjxfYB+b8BoRCQItQG+Rr9VKOa19DYZi0BGbd7uDFcbDOW7i0W539OqUo9zOo3k5NHkmbu4F2APsXeZMvEKHIXgMWCkiS0UkjJX8veOwa+4ArrQfXwrcpyw/7Q7gMruqaCmwEnhUg0wGg+84yttNiaCelbj7HMFQIkNDmTMRDpfDbYjKrQLWIkfSXSNAsAyj25yJV7gODSmlMiJyDXA3UAfcpJR6RkQ+B6xXSt0BfAf4gYhsBfqwjAX2dbcCzwIZ4GqlVNatTAZDJYgEA4TqxJ0CTmqoTtGwAna7u1mbHC7GQ+bl0GIY0yxoi7mUY7zxnNt7qxstOQKl1J3AnYcd++eCxwngzZO89gvAF3TIYTBUEhFx3VZBS3WKlhWwnuSoWzmGExnaGsKa5HD3vbjZUwF65yfrZsbtLDYYvMRtQlBHdUosZIV03Cg+a9XqUvHlV+Lu5HAbGtKSu9GQLK7mmQTGEBgMGnG7eUmHwnGmlOmo1nFDgz02021sXke1DpQvRyabYzSVdbWpDaq7FbUxBAaDRhoj7loe61DAUF0GqdK5CqfqqNzvZSRppS3dewTVO5zGGAKDQSNuNw3pqE4Zl8NNSCadn6rlBjdjM9PZHGPprHvPJGJtRis3NBR3OZ3MweQIDIYZgjUUxk1S0t1UsLwcGlbibhUfuBubOaJhLwNAsC5Afbj8+cn5TX7ayljLk2PTvjj/+bvN9AwlXckxEcYQGAwacRuSGdJQneJWDrdzgnXJoaPxnYObnMm4HO48pIZwEJHyPZNn98T52n1b8wZSJ8YQGAwacaqGyu1r43Y8pENTNMhQmStxHbuKHdwMsNdqCFwYJMejceshBQJCY7j8mQQ6GgFOhjEEBoNGmqIhMjlFMlP6LIBsTjGaymrZbOSm970TunDb2gHc5UzG+y5pkqNKDJJbw6jDQB+OMQQGg0ac1Vq8jDhwfgqWlpCMpYDL8Ux0rjzdVA3lV+I6PKRIsOxxlTqG9OTlcDGTYDiZIVwXIBrSP7LFGAKDQSNOC+hyVuNDGnrvOzRFg2V7Jjoa3zk0u1B8Oob0OLjJVej0TNzkKnQl8CfCGAKDQSNuNi9pXXlGyvdMdIYgGiNBkpkcqTIMUvUki9PUBYRoyL26dBMq09GAbzKMITAYNOKmB7/OkIybzUtDWmPz5dfOjxtGPTmTsj0Cu92Gm5kIhXKUm7vRVUgwEcYQGAwaGfcIys8RaFXAZXkm+kJUzua4cuQYTupdiQ8ny5sFoGu3N1ihsnKrhoZMaMhgqA3c9JPJ72DVFJIpVw6dOYIml8lzXStxJ1RWziyAIY0rcTcb7IaTevaYTIQxBAaDRtwYAuc1zVpDQ+XlCOoCQkxDdUqTi7YKOlfi7r6XtJZSWkuOEIl0jnS2jCS+htkMk2EMgcGgkQaXig/0hobKqZ13YtFaVuIuchVxjYag0UWuQqcCdtMSW6dhPBxXhkBE2kXkHhHZYv/bNsE1a0TkzyLyjIg8JSJ/XXDueyKyXUQ22D9r3MhjMFSaUF2AWKi8vjZDiTRBbTFxdyEq/Qq4vPvRHNOzEndbzaUjTAblJ8+VUlVdPnotcK9SaiVwr/38cEaBtymljgcuBL4sIq0F5/9RKbXG/tngUh6DoeKUu3t0SGd1iouV57AHiq9cBawjTGbJUX6obNiDEFWpOZNkJkcqm9MWojoct4ZgHXCz/fhm4JLDL1BKPa+U2mI/3gMcALpcfq7BULU0lVkZMpRIa5tl66bjps4VsKuVeFLf/XBVxprUN2O4qcwqqni+7Ud1egSzlVJ77cf7gNlTXSwipwFh4IWCw1+wQ0ZfEpHIFK+9SkTWi8j6np4el2IbDN7RVOa4Sp0KGMpv76CzXj0aqiNcF6iakEypciQzWVKZXMUNo8780URMawhE5PcisnGCn3WF1ymrqcmkRboiMhf4AfB2pZSTMv8YcCzwUqAd+Ohkr1dK3aCUWquUWtvVZRwKQ/VS7lAY3YagqewQlb6V+Lgcpd0PpZQnnkmpBlpnKW3h+5T6vejcZT0R076rUur8yc6JyH4RmauU2msr+gOTXNcM/Ab4hFLq4YL3dryJpIh8F/hISdIbDFVIYyTIgaFEya+LJ9IsbK/XJkdTNFRW/X48kaE5ptEzKWNX71g6SzantBkkZxZAqVVUujt+NuY9k9K+l/FNftWZI7gDuNJ+fCVw++EXiEgY+CXwfaXUbYedm2v/K1j5hY0u5TEYKk65Dc688AhKlcNaiXvgEZQdCtFzP5xZAKUq4LjGltyF71OqQYqP2XtMNBroQtwaguuAC0RkC3C+/RwRWSsiN9rX/BXwSuBvJygT/ZGIPA08DXQCn3cpj8FQccrtJxPXuHEJygsNWZudlFY5yslVxMf0r4DL+V7ym/w0lbFGggGCASkjR+CtR+DKvCileoHzJji+Hnin/fiHwA8nef2r3Hy+wVCNNEVDDKesvjaBQHGloLmcsloI6PQIIqXnKnT2GcrLEQ2xq3+spNfEPYiJl+MhjRskPXKISFV4SIdjdhYbDJppigRRqrS+NiOpDErp/Y9eTmw+HwrRtAIG636Ua5B0eyblJml13o/GMmY0DCXSiEBj2BgCg6EmKKcyxIvywKZokNGUlXQtlmpZievsu+TQWMa4yrgXHlIkVLJBctptFOthlooxBAaDZsYrQ8oxBHr3EUBpJZNOKETrStzOVZQyNtMrw1hysnhM/0q8sYwNh7rzR4djDIHBoJlyGq15kQwcr1ApXvl5sRJviobI5hRj6WwJcnixEi89Nu/FSry5zByBV/kBMIbAYNBOOcNpvPAIytlN68VKvBzPxGmFXR/WN6i9rGSxByvxxkiwJOMMziY/YwgMhpqhnByBF71kyglRjSeL9RukUsIhznxeHQ34HBojIcbSWTIlzALwYiXeFA2V5RGY0JDBUENUy0p8fH5yKZ5JWttQmnE5yjGMXihge0pZsvgQVXxMXytsB6eaq9ScifEIDIYaotxQCFQ+NBQfs1o/61yJl9MCWvfuZhj3kEppu6GzFbZDUzRIJqdIZor3TOIe3I9CjCEwGDST72tTouLTvhKPlBeS0a6Ay6le8kIBlzE9zoscwfj3Utzfh+4GfBNhDIHBoJl8X5sS9xHoGkrjUE7ve90N5yw5yguV6TZI5VVzeZMjgOK/F90N+CbCGAKDwQNKrVDxYuUZDTl9bUoMyUR0r4BLb7RmDYzXq4BLHZvpNODTniMocSbB+O5m4xEYDDVFqQ3OvFh5ikjJYzPjY/o9gnJaL3uzEi9NAY+ksuSU3s11UGiQijUE3jacA2MIDAZPKHUWgFd14qV6Jl7kCJz9AMUaRqWcBnzexOaLvR+6G87l5SjRMA6OedtwDowhMBg8oSVWqiHQr/ig9A6kcY/q1UsxSE5/JN2Kr9SVuBcN+KAgVFZ0aMjbecVgDIHB4AnN0WB+mEgxeFUVUkoH0qwHrbDzcpTQ+TPuUSgkFqqjLiBFeyZetX4uNUQ13vajSkNDItIuIveIyBb737ZJrssWDKW5o+D4UhF5RES2ishP7WlmBkPN0xILMThWykrcm6ZizSUYAkdRexOiKt5D8koBiwiNkWDRcnjRgA/KyRF4O7ge3HsE1wL3KqVWAvfazydiTCm1xv65uOD49cCXlFIrgH7gHS7lMRiqguaYFZLJFdEC2ouhNA5N0VDRfW3yik9zKMSSo3iPwIuGcw4tsVD+95xeDm8MUqguQDQUKDpk5+X9cHBrCNYBN9uPb8aaO1wU9pziVwHOHOOSXm8wVDMtsRA5BcNFDKfxYiiNQ3M0yOBoaYrPi1h0KTmCeL5cUr9BKsVT8ypHAFbfo1JCZbob8B2OW0MwWym11368D5g9yXVREVkvIg+LyCX2sQ5gQCnl3I1dwPzJPkhErrLfY31PT49LsQ0Gb3HCCcUoYS9d/5aYNYylGM9E96D2QhpLaAHtpUEqxRB4OR6yuYSZBF5sNjycaX9DEfk9MGeCU58ofKKUUiIy2V/bYqXUbhFZBtxnD6wfLEVQpdQNwA0Aa9euLb5bk8FQAZxVZDHxaE8VTiyEUtZntNRPreC9NEhN0eKrl7ysm2+Jhdg7WNz85PhYmkgwQCSofyVeShLf6/YSUIQhUEqdP9k5EdkvInOVUntFZC5wYJL32G3/u01EHgBOBn4OtIpI0PYKFgC7y/gdDIaqo8U2BMWsPp1rWjwKhTifMZ0hGM8ReFM1NGKXhdZNM+TFa8M4WGQ1V9yjkl4oNVehf7f34bgNDd0BXGk/vhK4/fALRKRNRCL2407gLOBZZfVgvR+4dKrXGwy1iKNMiykh9dIQtNaHD/mMqfB6JV74GVMxOJYmVKe3AV+hHPGxdFEtoOOJtGdtHZpLMARetOQ+HLeG4DrgAhHZApxvP0dE1orIjfY1xwHrReRJLMV/nVLqWfvcR4EPichWrJzBd1zKYzBUBY7iK+Y/+8BoCoDWmP7q6VI8Ey8G1zu01pfmIbXEwp7ExFtiIVLZHIn09C2g42PezQlujYUYKNYQeDAT4XBcfeNKqV7gvAmOrwfeaT9+CDhxktdvA05zI4PBUI2UkiPIewTThG7KoRRDMJRIEwvVEarTv8/UkWNgNM3ijqmvHRxN0+LRSrzwfsSmqcLxMjbfWm8lrZVS0xq8gdE0Jy2o7tCQwWCYgMZwkIAUvwIWGe+Fo5O8Ah5LTXttfMxbxWfJUYSHNJbKh7R0U5qH5N1KvCUWyu/kng4v74eDMQQGgwcEAkJTtLhSxYHRNC2xEIFpkqjlUD2Kz1JkThhsKgZG07R6qIChWA9J/3Ach9b8/ZhajkQ6SyKd8yR/VIgxBAaDRxRbGTI45p3ii4YChOsCRSm+/tEUbR6Ep6C0HMHA6PQVTuVSrCFQSjHoYY6gpcj74Zxv9eh+OBhDYDB4RHMsWJziG0t7tuITkaIrVAZG096HZIrYYBcfS3uSOD9Ejmnux1g6SyqTq3iIyvEYvLofDsYQGAweYbWiLqJ8dDRFi4cx4JZiDdJo2jOPIFQXoDESnDZHkM7mGEpmPFsBF6uA+20F7LWHNF1oKF9RZjwCg6E2KbadgZehIbD2EhQbGvIyKdkSC02r+BzPxSsPyWrVUMxK3FHA3twPZ4VfbGjI5AgMhhqlOVpkSMbD0BAUZ5AS6SzJTM7Tlaclx9TJ4gGPY+KBgNAUCU77vQx47BEUW83l9f1wMIbAYPCIlnprBTzVLtZcTlkxcc8V8HShEO82tTm01k/vETjnPTWM9SXcD488gmgoQDg4fRJ/0If7AcYQGAye0V4fJpXNMZrKTnrNUDJDTnms+IoIyfSPeLsChvFNVFMRz6+AvQ1RVTpHICK0xkLTJs8HxlLUBayBOl5iDIHB4BFtDZYy6xuZ3P33Y8VnDcnJkJ2iFbUTovBWAYenTRbn5ahwqGzQY4/AkaMYD6k1FvK0BTUYQ2AweEa7rUT6p9hENejTChimbviWL1P02iOYJlTmjxzhKb8TsDyChnAd4aB3KrIYD2mgiK6xOjCGwGDwiGI8gvGVuHf/2Z3wxpRy5EMh3hqkVDbHWHryUJkjh5fzeTsawlPeC/C+ggqK85AGPdxlXYgxBAaDR7Q3TO8R+JEcbS/CIPX7UK/uKLSpwiHWbt7gtDML3NBml9NmspN3ILU213mrgFtioXwIalI5fOgzBMYQGAye4YSG+kamCMn4UCfe0RABoHdKjyBFNBQg6sEMAIdi2kwMjnm3u9mhozGMUlM3wLPabXgrR2v99K2ovey7VIgxBAaDRzTZK9v+KRRw37B1zkul09FYRIhq1Lu2Dg5O47mpPCQrJOOt4ivGQxr0wSNobwgzmsoyNkVV2aCHfZcKMYbAYPCIQEBoqw/RN4Xi6x1J0hILeZqULC405L3i67QNUu/wFPdjOEVHg7cGyfHUppLDD48gfz9GkhOez7fb8NhAg0tDICLtInKPiGyx/22b4JpzRWRDwU9CRC6xz31PRLYXnFvjRh6Dodpoqw9P2Xq5dziVX7F7RTRUR0O4bkrFN+CD4utotENUwxMrPuecc51XtDdO7ZnkclbnUS/3VEBByG6S78U53tlU5YYAuBa4Vym1ErjXfn4ISqn7lVJrlFJrgFcBo8DvCi75R+e8UmqDS3kMhqqirX7qCpWDw0k6G7xVfGApv75JVp5gxcu99ghaYyECMnmuQinFwRHvDaPjIU0mRzyRtjb5+ZCrsOSY+Hs5aBvMDh/+PtwagnXAzfbjm4FLprn+UuAupdSoy881GGqCtoZQftfuRPT6oPgA2hsiUyaLe4eTeQXpFYGA0N4QySu4wxlKZkhlcp4bRsfz6ZtkJe71rmKHTtvzOTiJHM596qoBj2C2Umqv/XgfMHua6y8DfnLYsS+IyFMi8iURmfQvQESuEpH1IrK+p6fHhcgGg3+0N4SnzhEMJ30xBFPVzqezOfpH03Q1eb/y7GwMT6r4/AqFhOoCNEeDk4aGnNBVp8chqo5pcibOffJaDijCEIjI70Vk4wQ/6wqvU9Z2wUm3DIrIXKwh9ncXHP4YcCzwUqAd+Ohkr1dK3aCUWquUWtvV1TWd2AZDVdBWH6Z/JDXhbtpMNsfAWNoX1799CkPgKCJ/DEFk0hxBr4+hkI7GyT2kniFnJe6tHPXhILFQ3aQhu3xoyAdDMG0nI6XU+ZOdE5H9IjJXKbXXVvQHpnirvwJ+qZTK+8kF3kRSRL4LfKRIuQ2GmqC9IUwmpxhKZo4Ye9g/mkap8eoRL+loCNNrG6TD+9bkFZ8PCqejMUx398SRYWcF7IeH1FYfmlQB9wz7YwjA+l0nTxYniYYCNIS929vh4DY0dAdwpf34SuD2Ka69nMPCQrbxQKy/zEuAjS7lMRiqiqni0U6SsN0njyCVyTEyQc16z3AC8EnxNUQ4ODT1CtiPUEh7Q2RSBXwgnqQuIJ5XUYFloA9O4pkcHE7R2RjxvOEcuDcE1wEXiMgW4Hz7OSKyVkRudC4SkSXAQuAPh73+RyLyNPA00Al83qU8BkNV4SjXngnCIb0+roDzewkmUH4Hh3wMDTWFGZlkE5VzP7xOWoOlgCfLEfQMJeloCHva5iIvxxShsoM+lNI6uGpyrZTqBc6b4Ph64J0Fz3cA8ye47lVuPt9gqHbmtEQB2DeYOOLc+ArYB8VXUKq4qKP+kHM9Pq7EO/PtLpIsCB8qR+9Iktb6EKE67/e5ttk5kwlDZcNJZjX7o4A7GsI8uyc+4bmDwynmt0Z9kcPsLDYYPGR2k/UfeX/8SEOQ9wh8CQ1NvnmpZyhJUzToaZ8hB8cgTVQ5dHA46fmuYoeupgjprJqwAV7PUNKXfAk4SevkhMUEB4eTvhhnMIbAYPCU5liQaCgwoUfQO2LFor0eQwgwa4oQVc9Q0pewEIx7HROFQ5yYuB/Msz21vRN8LweGEj7ejzDprCKeyBxyPJdT9Pm0xwSMITAYPEVEmNMcZf8ECdKDQ1Zbh4APsWhHsU3kmfQM+7kCnrx2vtfHFfCcvCEYO+R4Lqc4OJzyzRCMe0iH/n0MjKXJ5pTxCAyGo4VZzVH2T7Dy3D+UYLZPsehQXYDOxvCEhuBgBTyCyTwTv1bAc1tiwJEeQf9oimxO+WYYZ00SOvSzggqMITAYPGdOc5R9EyjgfYOJvELyg9nNUfbHKxsaiobqaK0PHbESH01liCcy+ZW613Q1RagLyBFyjO8h8EeOuS0TG4IDcf/2MoAxBAaD58xpibI/njgiIbgvnsgrAj+Y3Rw9IleRSGcZSmZ8W3mCtRrfO3CoHHvs5/N8Mox1AWF2U+QIj8BRwH5VDc2ZJFexZ8AyUH7dD2MIDAaPmdUUIZnJHTKZayyVZWA07dsKGGB2c+SIladjGGY3+yfH3JboEYrPkcPP+zG39UiDdGDI35BMfThISyx0hIHeMziGCMxuMR6BwXBUkN9LUKCEncd+egTzW2P0jqQO2cy1q99aeS5o8y9EZRmCQ0Myewb9XQGD9b0cHrJzVuJ+fi8TGca9Awk6GyNEgt6X9IIxBAaD58xpduLA4/F5R+H4uQJe0GZt4No9MN7rZ1f/qH3OPwU8rzVG/2j6EIPkrMz9WgGDVUK6Z2DskJBdd98os5oivuypcJjbEmV3/5GGcV6rf9+JMQQGg8c4YZe9A+P/2V/stRTw4o4G3+RY2G4plu4CpbN7YIy6gOSNlR84RscxQpZMlgL2awUMMKclRjKTO2RT2a7+MRa210/xKv0saq+nu2/0EIO0e2Asv9fBD4whMBg8Zl5rjFCdsKN3XPG92DdCuC7gswK2FNyuAkOwq3+MOc1Rgj60dXBwjN8h96N3hCWd/hlFGA//7CkIU+0aGPXVOwJY1NHAUDKTH4iTyebo7hv1dZFgDIHB4DF1AWFRez07Do7kj+3sHWVBe8yXxmYOXY0RIsEAO3vH5dh2cITFHf6ugJfYn1d4P7YfHM0f94uFtmHcaRukVCbHnoFE/rhfLLY9kBft72X3wBjprGKZj4bRGAKDwQeWdjawvUDx7egdzSsAvwgEhGVdjWw5MAxYM4JfODDMilmNvsrRWh+mtT7EDlvxDSczHBxO+u4RLJ9lfZ5zP3b0jpDNKd/vh2OInXCh83fi5/0whsBg8IGlnQ15RZPO5nihZ5iVs5t8l2PlrEa27LcU3/54kuFkxnfFB9b9cBTwC/a/fq6AwSrdnN8aY6v9+c/vHwKogCFoIFQnbLY/f1uPYwj8WygYQ2Aw+MBxc5tJZiwDsK1nhFQmx+q5zb7LcczsRnYPjDGczLDlgK34uvw3BMfPa+a5PXFyOcXGPYP2sRbf5Vgxa9xDen7/MAHx3xCEgwFWzGrKt6PeuGeQzsaIb20uwBgCg8EXTlpgKbknuwd4xlZ8q+f5bwicz3yqe4Andg4gUhkFfMK8FoaSGXb2jbJxd5zmaND3JC1Y38vz+4cYSWZ4atcASzsbfC0ddVg9t5ln9sRRSvH0rkFOWtDiy2QyB1eGQETeLCLPiEhORNZOcd2FIrJZRLaKyLUFx5eKyCP28Z+KiD8dpwwGn1nW2UhjJMhfdvbz8LZemqJB30MhAKcubicg8PD2Ph7b0ceq2U201HvfBvtwTlrQCsCjO/pYv6OPkxa0+qr4HNYuaSebUzy6o4/HtvfxsmUdvssAcPKiVg4OJ/nLzgG29gznFw5+4dYj2Ai8EfjjZBeISB3wDeAiYDVwuYistk9fD3xJKbUC6Afe4VIeg6EqCQSEs1d18b9P7eV3z+7n7GO6fC3ZdGiJhThhfgu3PtbNIxVUfMfNbWJ+a4xv3L+VLQeGOf+4WRWR49TFbYTrAnzmjmcYSWUrdj/OPdb6/T906waUglcd6+/9cPWXqJR6Tim1eZrLTgO2KqW2KaVSwC3AOntg/auA2+zrbsYaYG8wHJVcesoChhIZBkbTvOmUBRWT44qXLWZfPEEqk+OKly2qiAwiwiUnz8tXylx04tyKyNEYCfKGk+fzYu8oc5qjXLB6dkXkmN8a44xlHbzYO8qKWY2cON9fj8DVzOIimQ90FzzfBZwOdAADSqlMwfEj5ho7iMhVwFUAixZV5o/XYHDDOau6+I83v4SxdDa/AqwEbzx5Pv0jKZZ3NbJilv+VSw7/cP4xCMJZKzp9bXp3OB9/3XEs6qjntKXtFckPOHz5sjX89x+28fazlvgeJpOJZmUecoHI74E5E5z6hFLqdvuaB4CP2EPrD3/9pcCFSql32s/fimUIPgM8bIeFEJGFwF1KqROmE3rt2rVq/fojPspgMBgMUyAijyuljsjnTusRKKXOd/nZu4GFBc8X2Md6gVYRCdpegXPcYDAYDD7iR7bqMWClXSEUBi4D7lCWK3I/cKl93ZXA7T7IYzAYDIYC3JaPvkFEdgFnAL8Rkbvt4/NE5E4Ae7V/DXA38Bxwq1LqGfstPgp8SES2YuUMvuNGHoPBYDCUzrQ5gmrE5AgMBoOhdCbLEZidxQaDwTDDMYbAYDAYZjjGEBgMBsMMxxgCg8FgmOHUZLJYRHqAF8t8eSdwUKM4laDWf4dalx9q/3eodfmh9n+HSsi/WCnVdfjBmjQEbhCR9RNlzWuJWv8dal1+qP3fodblh9r/HapJfhMaMhgMhhmOMQQGg8Eww5mJhuCGSguggVr/HWpdfqj936HW5Yfa/x2qRv4ZlyMwGAwGw6HMRI/AYDAYDAUYQ2AwGAwznBllCETkQhHZLCJbReTaSstTKiJyk4gcEJGNlZalHERkoYjcLyLPisgzIvKBSstUCiISFZFHReRJW/7PVlqmchGROhF5QkT+t9KylIqI7BCRp0Vkg4jUZPdJEWkVkdtEZJOIPCciZ1RUnpmSIxCROuB54AKssZiPAZcrpZ6tqGAlICKvBIaB7xczya3aEJG5wFyl1F9EpAl4HLikVr4De852g1JqWERCwIPAB5RSD1dYtJIRkQ8Ba4FmpdTrKy1PKYjIDmCtUqpmN5OJyM3An5RSN9pzWuqVUgOVkmcmeQSnAVuVUtuUUingFmBdhWUqCaXUH4G+SstRLkqpvUqpv9iPh7DmU0w6p7raUBbD9tOQ/VNzKykRWQC8Drix0rLMRESkBXgl9vwVpVSqkkYAZpYhmA90FzzfRQ0poaMNEVkCnAw8UmFRSsIOqWwADgD3KKVqSn6bLwP/BOQqLEe5KOB3IvK4iFxVaWHKYCnQA3zXDs/dKCINlRRoJhkCQ5UgIo3Az4F/UErFKy1PKSilskqpNVgztk8TkZoK0YnI64EDSqnHKy2LC16ulDoFuAi42g6Z1hJB4BTgW0qpk4ERoKI5y5lkCHYDCwueL7CPGXzEjq3/HPiRUuoXlZanXGxX/n7gwgqLUipnARfbcfZbgFeJyA8rK1JpKKV22/8eAH6JFfatJXYBuwq8yduwDEPFmEmG4DFgpYgstZMzlwF3VFimGYWdbP0O8JxS6r8qLU+piEiXiLTaj2NYhQebKipUiSilPqaUWqCUWoL1f+A+pdQVFRaraESkwS40wA6nvBqoqSo6pdQ+oFtEVtmHzgMqWjARrOSH+4lSKiMi1wB3A3XATUqpZyosVkmIyE+Ac4BOEdkFfFop9Z3KSlUSZwFvBZ624+wAH1dK3Vk5kUpiLnCzXYEWAG5VStVc+WWNMxv4pbWmIAj8WCn128qKVBbvA35kL0q3AW+vpDAzpnzUYDAYDBMzk0JDBoPBYJgAYwgMBoNhhmMMgcFgMMxwjCEwGAyGGY4xBAaDwTDDMYbAYDAYZjjGEBgMBsMM5/8D5OgXwfP21zIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "_,_ = make_circle_dataset(500, 1, 9, plot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnr75mA8p-Kf"
      },
      "source": [
        "#Plot heatmap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_formatter(denominator=2, number=np.pi, latex='\\pi'):\n",
        "    def gcd(a, b):\n",
        "        while b:\n",
        "            a, b = b, a%b\n",
        "        return a\n",
        "    def _multiple_formatter(x, pos):\n",
        "        den = denominator\n",
        "        num = np.int(np.rint(den*x/number))\n",
        "        com = gcd(num,den)\n",
        "        (num,den) = (int(num/com),int(den/com))\n",
        "        if den==1:\n",
        "            if num==0:\n",
        "                return r'$0$'\n",
        "            if num==1:\n",
        "                return r'$%s$'%latex\n",
        "            elif num==-1:\n",
        "                return r'$-%s$'%latex\n",
        "            else:\n",
        "                return r'$%s$'%latex\n",
        "        else:\n",
        "            if num==1:\n",
        "                return r'$\\frac{%s}{%s}$'%(latex,den)\n",
        "            elif num==-1:\n",
        "                return r'$\\frac{-%s}{%s}$'%(latex,den)\n",
        "            else:\n",
        "                return r'$\\frac{%s%s}{%s}$'%(num,latex,den)\n",
        "    return _multiple_formatter"
      ],
      "metadata": {
        "id": "LxWavIM9BO_B"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as tck"
      ],
      "metadata": {
        "id": "NiQNE1lyEa2z"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "vBIgP-_iAtdY"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(K, YKYs, state_info, all_input = True, save_fig = True, show_fig = False, mode = 'NA'):\n",
        "  # plt.figure(figsize = (15,500))\n",
        "  # f, axes = plt.subplots(1, len(K),figsize = (15*8,15))\n",
        "  f, axes = plt.subplots(1, len(K),figsize = (10*len(K),10))\n",
        "  if len(K) == 1:\n",
        "    axes = [axes]\n",
        "  model_title = 'unassigned'\n",
        "  if mode != 'NA':\n",
        "    mode_val = f'mode={int(mode):04d}'\n",
        "  for i in range(len(K)):\n",
        "    eigen_vals = eigh(K[i], eigvals_only = True)\n",
        "    K[i] = K[i]/np.sum(eigen_vals)\n",
        "    print(np.sum(eigen_vals))\n",
        "    if all_input:\n",
        "      if K[0].shape[0] <= 100:\n",
        "        if i == len(K)-1:\n",
        "          ax = sns.heatmap(K[i], linewidth = 0, ax = axes[i], vmin = 0, vmax = 20)\n",
        "        else:\n",
        "          ax = sns.heatmap(K[i], linewidth = 0, ax = axes[i])\n",
        "      else:\n",
        "        if i == len(K)-1:\n",
        "          \n",
        "          ax = sns.heatmap(K[i][0:500:5,0:500:5], linewidth = 0, ax = axes[i], vmin = 0, vmax = .004, cbar_kws={\"shrink\": .82})\n",
        "          # ax = sns.heatmap(K[i][0:500:5,0:500:5], linewidth = 0, ax = axes[i])\n",
        "        else:\n",
        "          ax = sns.heatmap(K[i][0:500:5,0:500:5], linewidth = 0, ax = axes[i])\n",
        "    else:\n",
        "      ax =sns.heatmap(K[i][[x1_idx, x2_idx, x3_idx, x4_idx]][:,[x1_idx, x2_idx, x3_idx, x4_idx]], linewidth=.2, ax = axes[i] )\n",
        "    \n",
        "    \n",
        "    pi = np.pi\n",
        "\n",
        "    plt.xticks([0, 50, 100], ['0', '', '2'])\n",
        "    plt.yticks([ 50, 100], [ '', '2'])\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_aspect('equal')\n",
        "    m_p_t = state_info[\"model_protocol_type\"]\n",
        "    n_h_l = f'n_h_l={state_info[\"n_hidden_layers\"]}'\n",
        "    n_n = f'n_n={state_info[\"n_neurons\"]}'\n",
        "    run = f'Run={state_info[\"run\"]}'\n",
        "    epoch = f'Epoch={state_info[\"epoch\"]}'\n",
        "    step = f'step={state_info[\"step\"]}'\n",
        "    s_i = state_info['learning_status']\n",
        "    tr_loss = f'train_loss={state_info[\"train_loss\"]}'\n",
        "    tr_acc = f'train_acc={state_info[\"train_acc\"]}'\n",
        "    te_loss = f'test_loss={state_info[\"test_loss\"]}'\n",
        "    te_acc = f'test_acc={state_info[\"test_acc\"]}'\n",
        "\n",
        "    if mode == 'NA':\n",
        "      model_title = f'{m_p_t}({n_h_l},{n_n},{run},{epoch},{step},{s_i},{tr_loss},{tr_acc},{te_loss},{te_acc})'\n",
        "    else:\n",
        "      \n",
        "      model_title = f'{m_p_t}({mode_val},{n_h_l},{n_n},{run},{epoch},{step},{s_i},{tr_loss},{tr_acc},{te_loss},{te_acc})'\n",
        "\n",
        "\n",
        "\n",
        "    if i < len(K)-1:\n",
        "      # title = 'K'+str(i+1)+ ', ' +\"Y'KY_n = \"+ YKYs[i] \n",
        "      title = 'K'+str(i+1)\n",
        "      # ax.set_xlabel(title, fontsize = 75)\n",
        "      pass\n",
        "    else:\n",
        "      title = 'K'\n",
        "      # title = 'K'+', ' +\"Y'KY_n = \"+  YKYs[i] \n",
        "      # ax.set_xlabel(title, fontsize = 75)\n",
        "\n",
        "    del ax \n",
        "  # plt.suptitle(model_title,x = 0.5, y = 1.05,ha = 'center', fontsize = 90, fontweight = 20)\n",
        "  if save_fig:\n",
        "    plt.savefig(model_title + \".pdf\" ,format = \"pdf\",bbox_inches='tight', dpi = 100)\n",
        "  if show_fig:\n",
        "    plt.show()  \n",
        "  plt.clf()\n",
        "  plt.close(f)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)"
      ],
      "metadata": {
        "id": "_PK37zj7U5rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC85OWcNoA8F"
      },
      "source": [
        "#Helper functions\n",
        "\n",
        "\n",
        "*   generating x-axis for plots\n",
        "*   formatting state_infos\n",
        "*   container routine\n",
        "*   unwrap routine\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "EEqma-k6gI-v"
      },
      "outputs": [],
      "source": [
        "def get_x_axis_global(state_info_5_runs, x_axis_global):  \n",
        "  x_axis_global = []\n",
        "  for r in range(len(state_info_5_runs)):\n",
        "    for e in range(len(state_info_5_runs[r])):\n",
        "      for s in range(len(state_info_5_runs[r][e])):\n",
        "        x_axis_global.append(round(((int(state_info_5_runs[r][e][s]['epoch'])-1) *(n_batches) + int(state_info_5_runs[r][e][s]['step']))/n_batches, 2))\n",
        "  x_axis_global[0] = 0\n",
        "  return x_axis_global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "VMS7cYUiHZuN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_state_info(state_info):\n",
        "  state_info[\"run\"] = str(state_info[\"run\"])\n",
        "  state_info[\"epoch\"] = f'{int(state_info[\"epoch\"]):05d}'\n",
        "  state_info[\"step\"] = f'{int(state_info[\"step\"]):03d}'\n",
        "  state_info[\"train_loss\"] = float(\"{:.3f}\".format(state_info[\"train_loss\"]))\n",
        "  state_info[\"train_acc\"] = float(\"{:.2f}\".format(state_info[\"train_acc\"]))\n",
        "  state_info[\"test_loss\"] = float(\"{:.3f}\".format(state_info[\"test_loss\"]))\n",
        "  state_info[\"test_acc\"] = float(\"{:.2f}\".format(state_info[\"test_acc\"]))\n",
        "  state_info[\"learning_status\"] = state_info[\"learning_status\"]\n",
        "  return state_info\n",
        "\n",
        "\n",
        "def routine(hidden_layer_outs_container, state_info_container, predictions_container,hyp_container,  hidden_layer_outputs,state_info, predictions, hyp):\n",
        "  # pass\n",
        "  hidden_layer_outs_container.append(hidden_layer_outputs)\n",
        "  state_info_container.append(copy.deepcopy(state_info))\n",
        "  predictions_container.append(predictions)\n",
        "  hyp_container.append(hyp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "dWPVgSW73aBO"
      },
      "outputs": [],
      "source": [
        "def unwrap_routine(arr_all):\n",
        "  result = [[] for _ in range(n_hidden_layers)]\n",
        "  for r in range(len(arr_all)):\n",
        "    for e in range(len(arr_all[r])):\n",
        "      for l in range(len(arr_all[r][e])):\n",
        "        result[l].append(arr_all[r][e][l])\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHzf9mTzrLYm"
      },
      "source": [
        "#Unknown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "w1d8gEYHpVUh"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 8, 5, padding = 'same')\n",
        "#         self.pool = nn.AvgPool2d(1, 1)\n",
        "#         self.conv2 = nn.Conv2d(8, 8, 5, padding = 'same')\n",
        "#         self.conv3 = nn.Conv2d(8, 8, 5, padding = 'same')\n",
        "#         self.fc1 = nn.Linear(8 * 32 * 32, 64)\n",
        "#         # self.fc2 = nn.Linear(128, 32)\n",
        "#         self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "#         # self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         # self.pool = nn.MaxPool2d(2, 2)\n",
        "#         # self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         # self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "#         # self.fc2 = nn.Linear(120, 84)\n",
        "#         # self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x = self.pool(F.relu(self.conv1(x)))\n",
        "#         # x = self.pool(F.relu(self.conv2(x)))\n",
        "#         # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         # x = F.relu(self.fc1(x))\n",
        "#         # x = F.relu(self.fc2(x))\n",
        "#         # x = self.fc3(x)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.conv3(x)\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         # x = self.fc2(x)\n",
        "#         x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# net = Net()\n",
        "# net.to(device)\n",
        "# import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# for epoch in range(30):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 100 == 99:    # print every 2000 mini-batches\n",
        "#           print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "#           running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITAB5txo6F8"
      },
      "source": [
        "#MLP class and train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "TXqBYjhMpPS_"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,inp_dim, out_dim, n_hidden_layers, n_neurons, protocol_type, bias):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        \n",
        "        self.protocol_type = protocol_type\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.bias = bias\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layers = nn.ModuleList([nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device)])\n",
        "        \n",
        "        for i in range(self.n_hidden_layers-1):\n",
        "          self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias ).to(device))\n",
        "\n",
        "        self.layers.append(nn.Linear( self.n_neurons, out_dim, bias = self.bias).to(device))\n",
        "  def forward(self, x, batch = None):\n",
        "        # x = self.flatten(x)\n",
        "        \n",
        "        # print(self.layers[-2].weight)\n",
        "\n",
        "        hidden_layer_outputs = []\n",
        "        for i in range(len(self.layers)-1):\n",
        "          x = self.layers[i](x)\n",
        "          hidden_layer_outputs.append(x)\n",
        "\n",
        "          out = torch.relu(x)\n",
        "          \n",
        "          x = out\n",
        "        x = self.layers[-1](x)\n",
        "        # x = torch.squeeze(nn.Sigmoid()(x))\n",
        "        return x, hidden_layer_outputs\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "mP_jnR0eB5Co"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer,  test_dataloader, mini_dl, state_info = None):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    state_info_over_batches = []\n",
        "    hidden_layer_outs_over_batches = []\n",
        "    predictions_over_batches = []\n",
        "    hyp_over_batches, hyp = [], None\n",
        "    for batch, (x,y) in enumerate(dataloader):\n",
        "        X, y = x.to(device), y.to(device)\n",
        "        pred, _ = model(X.float())\n",
        "        \n",
        "        loss = loss_fn(pred.flatten(), y.flatten())\n",
        "        \n",
        "\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        correct += (pred.softmax(1).argmax(1) == y).type(torch.float).sum().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss = loss.item()\n",
        "        # if batch%100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"Train Loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "        if int(state_info['epoch']) <= step_stop_point and (batch+1) % step_stepsize == 0:\n",
        "          \n",
        "         \n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,model, loss_fn)\n",
        "          _, _, train_loss, train_acc = evaluate(dataloader, None, model, loss_fn, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'step' : batch+1})\n",
        "      \n",
        "          state_info = format_state_info(state_info)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_over_batches, state_info_container = state_info_over_batches, predictions_container = predictions_over_batches,hyp_container = hyp_over_batches,\n",
        "            hidden_layer_outputs = hidden_layer_outputs, state_info = state_info, predictions = predictions, hyp = hyp\n",
        "            )\n",
        "          # plot_heatmap(kernels, state_info)\n",
        "    train_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"End of Epoch {state_info['epoch']} \\nTrain : \\n Accuracy = {(100*correct):>0.1f}, Loss =  {train_loss:>7f}\")\n",
        "    return predictions_over_batches, hidden_layer_outs_over_batches, state_info_over_batches\n",
        "\n",
        "def evaluate(test_dataloader,mini_dl, model, loss_fn, is_training = False):\n",
        "    size = len(test_dataloader.dataset)\n",
        "    num_batches = len(test_dataloader)\n",
        "    model.eval()\n",
        "    loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred , out = model(X.float())\n",
        "            \n",
        "            loss += loss_fn(pred.flatten(), y.flatten()).item()\n",
        "            \n",
        "            correct += (pred.softmax(1).argmax(1) == y).type(torch.float).sum().item()\n",
        "            # correct += ((pred >= .5) == y).type(torch.float).sum().item()\n",
        "    loss /= num_batches\n",
        "    correct /= size\n",
        "    if not is_training:\n",
        "      # print(f\"Test : \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")\n",
        "      # with torch.no_grad():\n",
        "      #     for X, y in mini_dl:\n",
        "      #         X, y = X.to(device), y.to(device)\n",
        "      #         pred , out = model(X.float())\n",
        "      with torch.no_grad():\n",
        "          for X in mini_dl:\n",
        "              X = X.to(device)\n",
        "              _ , out = model(X.float())\n",
        "      out = [x.clone().detach().to('cpu').numpy() for x in out]\n",
        "    temp_var = \"Training\" if is_training else \"Test\"\n",
        "    print(f\"Loss over all {temp_var} data : {loss:>5f} \\n\")\n",
        "    return pred, out, loss, correct*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "wZ36s-5ExMP5"
      },
      "outputs": [],
      "source": [
        "# def fibo(n):\n",
        "#   print(n)\n",
        "#   if n == 1 or n==0 :\n",
        "#     return 1\n",
        "  \n",
        "#   return fibo(n-1) + fibo(n-2)\n",
        "\n",
        "# print(fibo(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "LDp8cIlkd865"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# for n_h_l in [5]:\n",
        " \n",
        "  # for n_n in [32]:\n",
        "def run_mlp_model(train_dataloader, test_dataloader,mini_dl, inp_dim = None, out_dim = None, n_h_l = 5,n_n = 32, n_runs = 5, n_epochs = 100, bias = True, is_mnist_data = None):\n",
        "    model_protocol_type = \"MLP\"\n",
        "    n_hidden_layers = n_h_l\n",
        "    n_neurons = n_n\n",
        "\n",
        "    state_info = {\"model_protocol_type\" : model_protocol_type,\"n_hidden_layers\":n_hidden_layers, \"n_neurons\" : n_neurons}\n",
        "    # kernel_5_runs = []\n",
        "    \n",
        "    # intermediate_outs_5_runs = []\n",
        "    hidden_layer_outs_5_runs = []\n",
        "    state_info_5_runs = []\n",
        "    predictions_5_runs = []\n",
        "    # rand_kernels_over_5 = []\n",
        "    for run in range(n_runs):\n",
        "\n",
        "      \n",
        "     \n",
        "\n",
        "      #model init\n",
        "      mlp_model = NeuralNet(inp_dim, out_dim, n_hidden_layers, n_neurons, model_protocol_type, bias).to(device)\n",
        "      loss_fn = nn.MSELoss() \n",
        "      optimizer = torch.optim.Adam(mlp_model.parameters(),lr = lr)  \n",
        "\n",
        "      #Local variables init\n",
        "      # model_learning_status = \"UnLearned\"\n",
        "      # kernels_run = []\n",
        "      hidden_layer_outs_run = []\n",
        "      state_info_run = []\n",
        "      # intermediate_outs_run = []\n",
        "      predictions_run = []\n",
        "      hyp_run, hyp = [], None\n",
        "      epochs = n_epochs\n",
        "\n",
        "\n",
        "      #Evaluate before training starts\n",
        "      predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "      _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "      state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "      # print(torch.argmax(nn.Softmax(dim = 1)(predictions), dim = 1))\n",
        "      # debug()\n",
        "      state_info.update({'run' : run+1, 'epoch' : 0,'step' : 0, 'learning_status' : 'UnLearned'})\n",
        "      state_info = format_state_info(state_info)\n",
        "      routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "              hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "              )\n",
        "      # print(state_info_run)\n",
        "      # hidden_layer_outs_run.append([hidden_layer_outputs])\n",
        "      # state_info_run.append([state_info.copy()])\n",
        "      # predictions_run.append([predictions])\n",
        "      # print(state_info)\n",
        "      # debug()\n",
        "      #Store init result\n",
        "      # kernels_run.append([kernels])\n",
        "      # state_info_run.append([state_info.copy()])\n",
        "      # intermediate_outs_run.append(np.array((hidden_layer_outputs[1][[x1_idx, x2_idx, x3_idx, x4_idx]]).detach().to('cpu')))\n",
        "     \n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : None, 'learning_status' : 'Learned'})\n",
        "          predictions_batches, hidden_layer_outs_batches, state_info_batches = train(train_dataloader,mlp_model, loss_fn, optimizer,\n",
        "                                                           test_dataloader = test_dataloader,mini_dl = mini_dl, state_info = state_info)\n",
        "          \n",
        "          hyp_over_batches = None\n",
        "          if int(state_info['epoch']) <= step_stop_point: \n",
        "            \n",
        "            # hidden_layer_outs_run.append(hidden_layer_outs_batches)\n",
        "            # state_info_run.append(state_info_batches)\n",
        "            # predictions_run.append(predictions_batches)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "                    hidden_layer_outputs = hidden_layer_outs_batches, state_info = state_info_batches, predictions = predictions_batches, hyp = hyp_over_batches\n",
        "                    )\n",
        "\n",
        "          # elif int(state_info['epoch']) <= epoch_stop_point or (int(state_info['epoch']) % epoch_stepsize) == 0:\n",
        "          elif int(state_info['epoch']) <= 10 :\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          elif int(state_info['epoch']) <= 200 and (int(state_info['epoch']) % 10) == 0:\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          elif int(state_info['epoch']) <= 1000 and (int(state_info['epoch']) % 100) == 0 :\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          elif int(state_info['epoch']) <= 10000 and (int(state_info['epoch']) % 1000) == 0 :\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "            # hidden_layer_outs_run.append([hidden_layer_outputs])\n",
        "            # state_info_run.append([state_info.copy()])\n",
        "            # predictions_run.append([predictions])\n",
        "          \n",
        "          if train_loss <= train_loss_stopping_criteria or epoch == 10000:\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "            break\n",
        "          print(f\"End of Epoch {state_info['epoch']} \\nTest : \\n Accuracy = {(test_acc):>0.1f}, Loss =  {test_loss:>5f}\")\n",
        "          _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "          # print(state_info_run)\n",
        "            \n",
        "          \n",
        "\n",
        "    # intermediate_outs_5_runs.append(intermediate_outs_run)\n",
        "    # kernel_5_runs.append(kernels_run)\n",
        "      state_info_5_runs.append(state_info_run)\n",
        "      hidden_layer_outs_5_runs.append(hidden_layer_outs_run)\n",
        "      predictions_5_runs.append(predictions_run)\n",
        "\n",
        "    return hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYhzpVHvow-l"
      },
      "source": [
        "#NPF NPV class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "uvkj9iqqxyDg"
      },
      "outputs": [],
      "source": [
        "class NPFNeuralNetwork(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim, n_hidden_layers, n_neurons, protocol_type, bias):\n",
        "\n",
        "        super(NPFNeuralNetwork, self).__init__()\n",
        "        self.protocol_type = protocol_type\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.bias = bias\n",
        "        \n",
        "        self.layers = nn.ModuleList([])\n",
        "   \n",
        "        #For DLGN-SF, DGN-DLGN-SF\n",
        "        if self.protocol_type == \"DLGN-SF\":\n",
        "          self.layers.append(nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device))\n",
        "          for i in range(self.n_hidden_layers-1):\n",
        "            self.layers.append(nn.Linear( inp_dim,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "        elif self.protocol_type == \"DGN-DLGN-SF\":\n",
        "          self.layers.append(nn.Sequential(nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "                                            )\n",
        "                            )\n",
        "          \n",
        "          for i in range(self.n_hidden_layers-1):\n",
        "            self.layers.append(nn.Sequential(\n",
        "                                  nn.Linear( inp_dim,  self.n_neurons, bias = self.bias).to(device),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "                              )\n",
        "                        )\n",
        "        #For DGN, DLGN\n",
        "        else:\n",
        "          if is_DNN :\n",
        "            self.layers.append(nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device))\n",
        "            for i in range(self.n_hidden_layers-1):\n",
        "              self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "          else:\n",
        "           \n",
        "            self.pool = nn.AvgPool2d(32, 32)\n",
        "           \n",
        "            filter_size = 40\n",
        "            kernel_size = 5\n",
        "            self.layers.append(nn.Conv2d(3, filter_size, kernel_size, padding = 'same').to(device))\n",
        "            for i in range(n_cnn_layers-1):\n",
        "              self.layers.append(nn.Conv2d(filter_size, filter_size, kernel_size, padding = 'same').to(device))\n",
        "            self.layers.append(nn.Linear(40, self.n_neurons).to(device))\n",
        "            for i in range(self.n_hidden_layers-n_cnn_layers-1):\n",
        "                self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "      \n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden_layer_outputs = []\n",
        "        if is_DNN:\n",
        "          if self.protocol_type == \"DGN\":\n",
        "            for i in range(len(self.layers)):\n",
        "              x = self.layers[i](x)\n",
        "              hidden_layer_outputs.append(x)\n",
        "              out = torch.relu(x)\n",
        "              x = out\n",
        "            #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "          elif  self.protocol_type == \"DLGN\":\n",
        "            for i in range(len(self.layers)):\n",
        "              # print(x.size())\n",
        "              # print(self.layers[i].weight.size())\n",
        "              x = self.layers[i](x)\n",
        "              out = x\n",
        "              hidden_layer_outputs.append(out)\n",
        "              x = out\n",
        "            #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "          elif  self.protocol_type == \"DLGN-SF\":\n",
        "            for i in range(len(self.layers)):\n",
        "              out = self.layers[i](x)\n",
        "              hidden_layer_outputs.append(out)\n",
        "          elif  self.protocol_type == \"DGN-DLGN-SF\":\n",
        "            for i in range(len(self.layers)):\n",
        "              out = self.layers[i](x) \n",
        "              hidden_layer_outputs.append(out)\n",
        "        else:\n",
        "            if  self.protocol_type == \"DLGN\":\n",
        "              for i in range(n_cnn_layers):\n",
        "                x = self.layers[i](x)\n",
        "                out = x\n",
        "                hidden_layer_outputs.append(out)\n",
        "                x = out\n",
        "              x = self.pool(x)\n",
        "              x = torch.flatten(x, 1)\n",
        "              \n",
        "              for i in range(self.n_hidden_layers-n_cnn_layers):\n",
        "                \n",
        "                x = self.layers[i+n_cnn_layers](x)\n",
        "                out = x\n",
        "                hidden_layer_outputs.append(out)\n",
        "                x = out\n",
        "              # x = self.layers[0](x)\n",
        "              \n",
        "              # hidden_layer_outputs.append(x)\n",
        "\n",
        "\n",
        "              # x = torch.flatten(x, 1)\n",
        "\n",
        "              # x = self.layers[1](x)\n",
        "              # hidden_layer_outputs.append(x)\n",
        "\n",
        "              # x = self.layers[2](x)\n",
        "              # hidden_layer_outputs.append(x)\n",
        "              # # x = self.pool(self.layers[2](x))\n",
        "              # # hidden_layer_outputs.append(x)\n",
        "              # # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "              \n",
        "              # x = self.layers[3](x)\n",
        "             \n",
        "              # hidden_layer_outputs.append(x)\n",
        "            \n",
        "\n",
        "        return hidden_layer_outputs\n",
        "\n",
        "\n",
        "\n",
        "class NPVNeuralNetwork(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim, n_hidden_layers, n_neurons, bias):\n",
        "        super(NPVNeuralNetwork, self).__init__()\n",
        "  \n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.bias = bias\n",
        "        if is_DNN:\n",
        "          self.layers = nn.ModuleList([nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device)])\n",
        "\n",
        "          for i in range(self.n_hidden_layers-1):\n",
        "            self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "        else:\n",
        "\n",
        "          self.pool = nn.AvgPool2d(32, 32)\n",
        "          filter_size = 40\n",
        "          kernel_size = 5\n",
        "          self.layers = nn.ModuleList([nn.Conv2d(3, filter_size, kernel_size, padding = 'same').to(device)])\n",
        "\n",
        "          for i in range(n_cnn_layers-1):\n",
        "            self.layers.append(nn.Conv2d(filter_size, filter_size, kernel_size, padding = 'same').to(device))\n",
        "          self.layers.append(nn.Linear(40, self.n_neurons).to(device))\n",
        "          for i in range(self.n_hidden_layers-n_cnn_layers-1):\n",
        "              self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "\n",
        "          self.layers.append(nn.Linear(self.n_neurons, out_dim).to(device))\n",
        "        \n",
        "        if is_DNN:\n",
        "          self.layers.append(nn.Linear( self.n_neurons, out_dim, bias = self.bias).to(device))\n",
        "\n",
        "        self.gate = Gate(beta = 4)\n",
        "      \n",
        "    def forward(self, x,act_type, gating_mask):\n",
        "        hidden_layer_outputs = []\n",
        "        if is_DNN:\n",
        "          for i in range(len(self.layers)-1):\n",
        "            x = self.layers[i](x)\n",
        "            \n",
        "            out = self.gate(act_type, x, i, gating_mask)\n",
        "            \n",
        "            hidden_layer_outputs.append(out)\n",
        "            x = out\n",
        "          x = self.layers[len(self.layers)-1](x)\n",
        "          #print(\"debug 2\", x.shape, x)\n",
        "        else:\n",
        "          for i in range(n_cnn_layers):\n",
        "            x = self.layers[i](x)\n",
        "            x = self.gate(act_type, x, i, gating_mask)\n",
        "            hidden_layer_outputs.append(x)\n",
        "\n",
        "          x = self.pool(x)\n",
        "          x = torch.flatten(x, 1)\n",
        "          \n",
        "          for i in range(self.n_hidden_layers-n_cnn_layers):\n",
        "            \n",
        "            x = self.layers[i+n_cnn_layers](x)\n",
        "            x = self.gate(act_type, x, i+n_cnn_layers, gating_mask)\n",
        "            hidden_layer_outputs.append(x)\n",
        "            \n",
        "          x = self.layers[-1](x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # x = self.layers[0](x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.gate(act_type, x, 0, gating_mask)\n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # x = torch.flatten(x, 1)\n",
        "          # x = self.layers[1](x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.gate(act_type, x, 1, gating_mask)\n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # x = self.layers[2](x)\n",
        "          \n",
        "          # x = self.gate(act_type, x, 2, gating_mask)\n",
        "          \n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.layers[3](x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.gate(act_type, x, 3, gating_mask)\n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "          # # x = self.layers[3](x)\n",
        "          # # x = self.gate(act_type, x,3, gating_mask)\n",
        "          # # hidden_layer_outputs.append(x)\n",
        "          \n",
        "          \n",
        "         \n",
        "          # x = self.layers[4](x)\n",
        "        return x,  hidden_layer_outputs\n",
        "\n",
        "\n",
        "def apply_gate(beta, idx, gating_mask):\n",
        "  out = beta*(gating_mask[idx])\n",
        "  \n",
        "  return out\n",
        "  \n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, beta):\n",
        "        super(Gate,self).__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self,act_type, x, idx, gating_mask):\n",
        "      #Soft Relu\n",
        "      if act_type == 'soft':\n",
        "        return torch.mul(x,torch.sigmoid(apply_gate(self.beta, idx,gating_mask)))\n",
        "      elif act_type == 'hard':\n",
        "      #Hard Relu\n",
        "        temp = torch.sign(gating_mask[idx])\n",
        "        temp[temp <= 0] = 0\n",
        "        return torch.mul(x,temp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNXo7wMRooPI"
      },
      "source": [
        "#NPF, NPV Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "Er2qEs6hbLH3"
      },
      "outputs": [],
      "source": [
        "def get_metrics_hlo(train_dl_npf, train_dl_npv, test_dl_npf, test_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type , is_training):\n",
        "  #hidden_layer_outputs is computed for mini_dataloader(for each input)\n",
        "  _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "  predictions, hidden_layer_outputs, test_loss, test_acc = evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type)\n",
        "  return train_acc, train_loss, test_acc, test_loss, predictions, hidden_layer_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "3Vfo8ATFok4o"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_decoupled(X1_dataloader,X2_dataloader, npf_model, npv_model, loss_fn, optimizer,act_type, test_dl_npf, test_dl_npv,mini_dl, state_info = None):\n",
        "    size = len(X1_dataloader.dataset)\n",
        "    num_batches = len(X1_dataloader)\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    state_info_over_batches = []\n",
        "    hidden_layer_outs_over_batches = []\n",
        "    predictions_over_batches = []\n",
        "    hyp_over_batches = []\n",
        "\n",
        "    \n",
        "    for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "        X1, y1 = X1.to(device), y1.to(device)\n",
        "        X2, y2 = X2.to(device), y2.to(device)\n",
        "        \n",
        "        npf_model_hidden_layer_outs = npf_model(X1)\n",
        "        pred, npv_model_hidden_layer_outs = npv_model(X2,act_type, npf_model_hidden_layer_outs)\n",
        "        #print(\"Debug1 \", pred.shape)\n",
        "        # print(pred.dtype,type(y1.float), y1.float.dtype)\n",
        "        if not is_classification:\n",
        "          y1 = torch.unsqueeze(y1, 1)\n",
        "        loss = loss_fn(pred, y1)\n",
        "        train_loss += loss.item()\n",
        "        correct += (pred.softmax(1).argmax(1) == y1).type(torch.float).sum().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if batch%10 == 0:\n",
        "        #     #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        #     loss, current = loss.item(), batch * len(X1)\n",
        "        #     print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "        \n",
        "        if int(state_info['epoch']) <= step_stop_point and (batch+1) % step_stepsize == 0:\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc = evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(X1_dataloader,X2_dataloader,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'step' : batch+1})\n",
        "        \n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = None, model_protocol_type = state_info['model_protocol_type'],run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_over_batches, state_info_container = state_info_over_batches, predictions_container = predictions_over_batches,hyp_container = hyp_over_batches,\n",
        "            hidden_layer_outputs = hidden_layer_outputs, state_info = state_info, predictions = predictions, hyp = hyp\n",
        "            )\n",
        "          # if int(state_info['run']) < 2:\n",
        "            # plot_hyperplanes(npf_model, for_layers = 5 ,model_protocol_type = state_info['model_protocol_type'] ,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'])\n",
        "    train_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"End of Epoch {state_info['epoch']}Train : \\n Accuracy = {(100*correct):>0.1f}, Loss =  {train_loss:>7f}\")\n",
        "    return predictions_over_batches, hidden_layer_outs_over_batches, state_info_over_batches, hyp_over_batches\n",
        "\n",
        "def evaluate_decoupled(X1_dataloader, X2_dataloader,mini_dl, npf_model, npv_model, loss_fn,act_type, is_training = False):\n",
        "    size = len(X1_dataloader.dataset)\n",
        "    num_batches = len(X1_dataloader)\n",
        "    npf_model.eval()\n",
        "    npv_model.eval()\n",
        "    loss, correct = 0, 0\n",
        "    X_ones = torch.ones(500,2).to(device)\n",
        "    with torch.no_grad():\n",
        "        for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "            X1, y1 = X1.to(device), y1.to(device)\n",
        "            X2, y2 = X2.to(device), y2.to(device)\n",
        "            npf_model_hidden_layer_outs = npf_model(X1)\n",
        "            pred, npv_model_hidden_layer_outs = npv_model(X2,act_type, npf_model_hidden_layer_outs)\n",
        "            if not is_classification:\n",
        "              y1 = torch.unsqueeze(y1, 1)\n",
        "            loss += loss_fn(pred, y1).item()\n",
        "            \n",
        "            correct += (pred.softmax(1).argmax(1) == y1).type(torch.float).sum().item()\n",
        "    loss /= num_batches\n",
        "    correct /= size\n",
        "    if not is_training:\n",
        "    # print(f\"Test : \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")\n",
        "    # with torch.no_grad():\n",
        "    #     for X, y in mini_dl:\n",
        "    #         X, y = X.to(device), y.to(device)\n",
        "    #         pred , out = model(X.float())\n",
        "      with torch.no_grad():\n",
        "          for X in mini_dl:\n",
        "              X = X.to(device)\n",
        "              npf_model_hidden_layer_outs = npf_model(X)\n",
        "              _, npv_model_hidden_layer_outs = npv_model(X,act_type, npf_model_hidden_layer_outs)\n",
        "        \n",
        "    npf_model_hidden_layer_outs = [x.clone().detach().to('cpu').numpy() for x in npf_model_hidden_layer_outs]\n",
        "    temp_var = \"Training\" if is_training else \"Test\"\n",
        "    print(f\"Loss over all {temp_var} data : {loss:>5f} \\n\")\n",
        "    return pred, npf_model_hidden_layer_outs, loss, correct*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Az41tOliuFWj",
        "outputId": "cdd2949b-1268-4fa5-eff3-489c14607298"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nParameter containing:\\ntensor([[-1., -2.],\\n        [ 1., -1.]], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([[2., 2.],\\n        [1., 0.]], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([[ 2.,  0.],\\n        [ 2., -2.]], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([[-2., -1.],\\n        [ 0., -2.]], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([[-2., -2.]], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([ 2., -1.], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([-1.,  2.], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([1., 0.], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([0., 0.], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([2.], device='cuda:0', requires_grad=True)\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 267
        }
      ],
      "source": [
        "# npf_model = NPFNeuralNetwork(inp_dim = 2, out_dim = 1, n_hidden_layers = 3, n_neurons = 2, protocol_type = 'DLGN', bias = True).to(device)\n",
        "# npv_model = NPVNeuralNetwork(inp_dim = 2, out_dim = 1,n_hidden_layers = 3, n_neurons = 2, bias = True).to(device)\n",
        "# print(npf_model.layers[0].weight,npf_model.layers[0].bias,  npv_model.layers[0].weight, npv_model.layers[0].bias)\n",
        "# print(npf_model.layers[1].weight,npf_model.layers[1].bias,  npv_model.layers[1].weight,npv_model.layers[1].bias )\n",
        "# print( npv_model.layers[2].weight, npv_model.layers[2].bias)\n",
        "# set_seed(seed = 2022)\n",
        "# npf_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npf_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npf_model.layers[0].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "# npf_model.layers[1].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "\n",
        "# set_seed(seed = 2021)\n",
        "# npv_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npv_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npv_model.layers[2].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (1,2)).float().to(device))\n",
        "# npv_model.layers[0].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "# npv_model.layers[1].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "# npv_model.layers[2].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (1,)).float().to(device))\n",
        "\n",
        "# print(npf_model.layers[0].weight, npv_model.layers[0].weight)\n",
        "# print(npf_model.layers[1].weight, npv_model.layers[1].weight)\n",
        "# print( npv_model.layers[2].weight)\n",
        "\n",
        "# print(npf_model.layers[0].bias, npv_model.layers[0].bias)\n",
        "# print(npf_model.layers[1].bias, npv_model.layers[1].bias)\n",
        "# print( npv_model.layers[2].bias)\n",
        "'''\n",
        "Parameter containing:\n",
        "tensor([[-1., -2.],\n",
        "        [ 1., -1.]], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([[2., 2.],\n",
        "        [1., 0.]], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([[ 2.,  0.],\n",
        "        [ 2., -2.]], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([[-2., -1.],\n",
        "        [ 0., -2.]], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([[-2., -2.]], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([ 2., -1.], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([-1.,  2.], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([1., 0.], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([0., 0.], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([2.], device='cuda:0', requires_grad=True)\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "-aiIWr_FyIZg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def run_npf_npv_model(train_dl_npf ,train_dl_npv , test_dl_npf, test_dl_npv,mini_dl, protocol = 'DGN',learning_type = 'BOTH',act_type = 'soft',inp_dim = None, out_dim = None, n_h_l = 5,n_n = 32, n_runs = 5, n_epochs = 250,lr = 5e-5, bias = True, epsilon = 1):\n",
        "\n",
        "    model_protocol_type = protocol\n",
        "    n_hidden_layers = n_h_l\n",
        "    n_neurons = n_n\n",
        "\n",
        "    state_info = {\"model_protocol_type\" : model_protocol_type,\"n_hidden_layers\":n_hidden_layers, \"n_neurons\" : n_neurons}\n",
        "    hidden_layer_outs_5_runs = []\n",
        "    state_info_5_runs = []\n",
        "    predictions_5_runs = []\n",
        "    hyp_5_runs = []\n",
        "    act_type = act_type\n",
        "    for run in range(n_runs):\n",
        "      set_seed(run + 10)\n",
        "      #Model Init\n",
        "      npf_model = NPFNeuralNetwork(inp_dim, out_dim, n_hidden_layers, n_neurons, model_protocol_type, bias).to(device)\n",
        "      npv_model = NPVNeuralNetwork(inp_dim, out_dim,n_hidden_layers, n_neurons, bias).to(device)\n",
        "      # torch.save(npv_model.state_dict(), \"npv_init_weights\")\n",
        "      #tmep code for ICLR toy example\n",
        "      # set_seed(seed = 2022)\n",
        "      # npf_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npf_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npf_model.layers[0].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "      # npf_model.layers[1].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "      # set_seed(seed = 2021)\n",
        "      # npv_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npv_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npv_model.layers[2].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (1,2)).float().to(device))\n",
        "      # npv_model.layers[0].bias = nn.Parameter(torch.randint(low = 0, high = 1, size = (2,)).float().to(device))\n",
        "      # npv_model.layers[1].bias = nn.Parameter(torch.randint(low = 0, high = 1, size = (2,)).float().to(device))\n",
        "      # npv_model.layers[2].bias = nn.Parameter(torch.randint(low = 0, high = 1, size = (1,)).float().to(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      loss_fn = nn.CrossEntropyLoss() if is_classification else nn.MSELoss() \n",
        "      print(learning_type)\n",
        "      \n",
        "      if learning_type == 'BOTH' or learning_type == 'BOTH_ONPV':\n",
        "      \n",
        "        optimizer = torch.optim.Adam([\n",
        "                        {'params': npf_model.parameters()},\n",
        "                        {'params': npv_model.parameters()}],\n",
        "                        lr = lr) \n",
        "      elif learning_type == 'ONPF':\n",
        "        optimizer = torch.optim.Adam([\n",
        "                        {'params': npf_model.parameters()}],\n",
        "                        lr = lr)\n",
        "      elif learning_type == 'ONPV':\n",
        "        optimizer = torch.optim.Adam([\n",
        "                        {'params': npv_model.parameters()}],\n",
        "                        lr = lr)\n",
        "      # optimizer = torch.optim.Adam([\n",
        "      #                 {'params': npf_model.parameters()},\n",
        "      #                               ],\n",
        "      #                 lr = 3e-6)\n",
        "      #Make the non linear layer of NPF model untrainable\n",
        "      #NOTE: Unchecked for different hidden layers(Currently working for n_h_l = 5)\n",
        "      if model_protocol_type == 'DGN-DLGN-SF':\n",
        "        for idx, param in enumerate(npf_model.parameters()):\n",
        "          if idx % 4 == 0 or idx-1 % 4 == 0:\n",
        "            param.requires_grad = False\n",
        "      if learning_type == 'BOTH_ONPV':\n",
        "        act_type = 'soft'\n",
        "      #Local Variable Init\n",
        "      hidden_layer_outs_run = []\n",
        "      state_info_run = []\n",
        "      predictions_run = []\n",
        "      hyp_run = []\n",
        "      epochs = n_epochs\n",
        "\n",
        "      #Evaluate before training starts\n",
        "      # all_per_sample_gradients = compute_pred_gradient(dl_one = train_dl_npf,model = [npf_model, npv_model],act_type = act_type, optimizer = optimizer)\n",
        "      predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl ,npf_model, npv_model, loss_fn, act_type)\n",
        "      _, _, train_loss, train_acc = evaluate_decoupled(train_dl_npf, train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "      state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "      state_info.update({'run' : run+1, 'epoch' : 0,'step' : 0,'learning_status' : 'UnLearned'})\n",
        "      state_info = format_state_info(state_info)\n",
        "      hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "\n",
        "      routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "              hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "              )\n",
        "      \n",
        "      if plot_hyperplanes_figures == True and run < 1:\n",
        "          plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          \n",
        "          # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "      for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : None,'learning_status' : 'Learned'})\n",
        "        predictions_batches, hidden_layer_outs_batches, state_info_batches, hyp_over_batches = train_decoupled(train_dl_npf,\n",
        "                                                                                             train_dl_npv,\n",
        "                                                                                             npf_model,\n",
        "                                                                                             npv_model,\n",
        "                                                                                             loss_fn,\n",
        "                                                                                             optimizer,\n",
        "                                                                                             act_type,\n",
        "                                                                                             test_dl_npf = test_dl_npf,\n",
        "                                                                                             test_dl_npv = test_dl_npv,\n",
        "                                                                                             mini_dl = mini_dl,\n",
        "                                                                                             state_info = state_info)\n",
        "        \n",
        "        if int(state_info['epoch']) <= step_stop_point: \n",
        "            # hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "                    hidden_layer_outputs = hidden_layer_outs_batches, state_info = state_info_batches, predictions = predictions_batches, hyp = hyp_over_batches\n",
        "                    )\n",
        "        \n",
        "        elif int(state_info['epoch']) <= 10 :\n",
        "            \n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "            _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "            \n",
        "        elif int(state_info['epoch']) <= 200 and (int(state_info['epoch']) % 10) == 0:\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          # if plot_hyperplanes_figures == True and run < 10:\n",
        "          #   plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "           \n",
        "          \n",
        "        elif int(state_info['epoch']) <= 1000 and (int(state_info['epoch']) % 100) == 0 :\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "      \n",
        "          \n",
        "        elif int(state_info['epoch']) <= 20000 and (int(state_info['epoch']) % 1000) == 0 :\n",
        "          \n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "\n",
        "        elif int(state_info['epoch']) <= 5000 and int(state_info['epoch']) >4000 and (int(state_info['epoch']) % 100) == 0 :\n",
        "          \n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "          \n",
        "        if learning_type == 'BOTH_ONPV' and int(state_info['epoch']) == 3:\n",
        "            for idx, param in enumerate(npf_model.parameters()):\n",
        "                param.requires_grad = False\n",
        "            act_type = 'soft'\n",
        "        print(f\"End of Epoch {state_info['epoch']}Test : \\n Accuracy = {(test_acc):>0.1f}, Loss =  {test_loss:>5f}\")\n",
        "        _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "        # if epoch == 20000:\n",
        "        \n",
        "        #   break\n",
        "        if train_loss <= train_loss_stopping_criteria:\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            \n",
        "          break\n",
        "\n",
        "      state_info_5_runs.append(state_info_run)\n",
        "      hidden_layer_outs_5_runs.append(hidden_layer_outs_run)\n",
        "      predictions_5_runs.append(predictions_run)\n",
        "      hyp_5_runs.append(hyp_run)\n",
        "    return hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs, hyp_5_runs\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsxDmHwDl8Sb"
      },
      "source": [
        "#Kernel Plotting Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "aJVjibIx240t"
      },
      "outputs": [],
      "source": [
        "def get_start_point(epoch_stop_point, epoch_stepsize):\n",
        "  for i in range(epoch_stop_point+1, 500):\n",
        "    if i % epoch_stepsize == 0:\n",
        "      return i\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "kMqXlCDtRVt_"
      },
      "outputs": [],
      "source": [
        "# from pyparsing.util import col\n",
        "#-------------------------------------------------------Plotting Functions--------------------------------------------------------\n",
        "\n",
        "#For step wise analysis\n",
        "def plot_kernel_values_stepwise(kernel_name, kernel_5_runs,state_info_5_runs, x_idxs_pair):\n",
        "  kernel_map = {'K1':0, 'K2':1, 'K3':2,'K4':3, 'K5':4, 'K': 5}\n",
        "  # labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "  kernel_idx = kernel_map[kernel_name]\n",
        "\n",
        "  f, axes = plt.subplots(1,len(kernel_5_runs),figsize = (7*len(kernel_5_runs), 7))\n",
        "  if len(kernel_5_runs) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  plt.suptitle(f'{kernel_name}_step',ha = 'center', fontsize = 15)\n",
        "\n",
        "  for run in range(len(kernel_5_runs)):\n",
        "    for idx, (row, col) in enumerate(x_idxs_pair):\n",
        "      values = []\n",
        "      \n",
        "      for epoch in range(len(kernel_5_runs[run])):\n",
        "        for step in range(len(kernel_5_runs[run][epoch])):\n",
        "          values.append(kernel_5_runs[run][epoch][step][kernel_idx][row,col])\n",
        "          \n",
        "        if epoch == step_stop_point:\n",
        "          break\n",
        "      if idx >= len(x_idxs_pair)/2:\n",
        "        #high freq\n",
        "        linestyle = 'solid'\n",
        "        color = 'red'\n",
        "      else:\n",
        "        #low freq\n",
        "        linestyle = 'dashed'\n",
        "        color = 'blue'\n",
        "      x = [0] + list(np.arange(step_stepsize, len(values)*step_stepsize, step_stepsize))\n",
        "      axes[run].plot(x,values, linestyle = linestyle, color = color)\n",
        "      axes[run].set_xlabel(f'Run {run+1}', fontsize = 12)\n",
        "      if kernel_name == 'K':\n",
        "        axes[run].set_ylabel(f'Log Scale', fontsize = 12)\n",
        "    line1 = Line2D([0,1],[0,1],linestyle='solid', color='r')\n",
        "    line2 = Line2D([0,1],[0,1],linestyle='dashed', color='b')\n",
        "    axes[run].legend([line1, line2],['high freq', 'low freq'], loc = 'best')\n",
        "    \n",
        "  plt.savefig(f'{kernel_name}_step' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "  plt.show()\n",
        "#For epoch wise analysis\n",
        "def plot_kernel_values_epochwise(kernel_name, kernel_5_runs, state_info_5_runs, x_idxs_pair):\n",
        "  kernel_map = {'K1':0, 'K2':1, 'K3':2,'K4':3, 'K5':4, 'K': 5}\n",
        "  kernel_idx = kernel_map[kernel_name]\n",
        "  # labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "  \n",
        "\n",
        "  f, axes = plt.subplots(1,len(kernel_5_runs),figsize = (7*len(kernel_5_runs), 7))\n",
        "  if len(kernel_5_runs) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  plt.suptitle(f'{kernel_name}_epoch',ha = 'center', fontsize = 15)\n",
        "  \n",
        "  for run in range(len(kernel_5_runs)):\n",
        "    for idx, (row, col) in enumerate(x_idxs_pair):\n",
        "      values = []\n",
        "      state = []\n",
        "      # print(len(kernel_5_runs[run]))\n",
        "      for epoch in range(len(kernel_5_runs[run])):\n",
        "          # print(idx, row, col)\n",
        "          # print(kernel_5_runs[run][epoch])\n",
        "          values.append(kernel_5_runs[run][epoch][-1][kernel_idx][row,col])\n",
        "          state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "      if idx >= len(x_idxs_pair)/2:\n",
        "        #high freq region\n",
        "        linestyle = 'solid'\n",
        "        color = 'red'\n",
        "      else:\n",
        "        #low freq region\n",
        "        linestyle = 'dashed'\n",
        "        color = 'blue'\n",
        "      # x = [0] + list(np.arange(epoch_stepsize, len(values)*epoch_stepsize, epoch_stepsize))\n",
        "      axes[run].plot(state,values, linestyle = linestyle, color = color)\n",
        "      axes[run].set_xlabel(f'Run {run+1}', fontsize = 12)\n",
        "      if kernel_name == 'K':\n",
        "        axes[run].set_ylabel(f'Log Scale', fontsize = 12)\n",
        "    line1 = Line2D([0,1],[0,1],linestyle='solid', color='r')\n",
        "    line2 = Line2D([0,1],[0,1],linestyle='dashed', color='b')\n",
        "    axes[run].legend([line1, line2],['high freq', 'low freq'], loc = 'best')\n",
        "    \n",
        "  plt.savefig(f'{kernel_name}_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "  plt.show()\n",
        "\n",
        "  # Plot Activation graph stepwise\n",
        "def plot_stepwise_activations(hidden_layer_no, hidden_layer_outs_5_runs):\n",
        "  temp = []\n",
        "  y_label = ''\n",
        "  NUM_COLORS = n_neurons\n",
        "  LINE_STYLES = ['solid', 'dashed', 'dashdot', 'dotted']\n",
        "  NUM_STYLES = len(LINE_STYLES)\n",
        "  cm = plt.get_cmap('gist_rainbow')\n",
        "\n",
        "  hidden_layer_no = hidden_layer_no\n",
        "  for run in range(len(hidden_layer_outs_5_runs)):\n",
        "    f, axes = plt.subplots(1, 4,figsize = (7*5,7))\n",
        "    values = []\n",
        "    for epoch in range(len(hidden_layer_outs_5_runs[run])):\n",
        "      for step in range(len(hidden_layer_outs_5_runs[run][epoch])):\n",
        "        sub_h_l_outs = hidden_layer_outs_5_runs[run][epoch][step][hidden_layer_no-1][[x1_idx, x2_idx, x3_idx, x4_idx]]#[:,[x1_idx, x2_idx, x3_idx, x4_idx]]\n",
        "        values.append(sub_h_l_outs)\n",
        "      if epoch == step_stop_point:\n",
        "        break\n",
        "\n",
        "    values = np.array(values)\n",
        "    # x = [0] + list(np.arange(step_stepsize, len(values[0])*step_stepsize, step_stepsize))\n",
        "    plt.suptitle(f'Run {run+1} for Hidden Layer No. {hidden_layer_no}',ha = 'center', fontsize = 15)\n",
        "    for i in range(4):\n",
        "      lines = axes[i].plot(values[:][:,i])\n",
        "      axes[i].set_title(f'x_{i+1}')\n",
        "      axes[i].set_xlabel(f'Step', fontsize = 12)\n",
        "      axes[i].set_ylabel(y_label, fontsize = 12)\n",
        "      # axes[i].set_xticks(np.arange(0,len(values), 1.0))\n",
        "\n",
        "      for j in range(NUM_COLORS):\n",
        "\n",
        "        lines[j].set_color(cm(j//NUM_STYLES*float(NUM_STYLES)/NUM_COLORS))\n",
        "        lines[j].set_linestyle(LINE_STYLES[j%NUM_STYLES])\n",
        "\n",
        "      # axes[i].set_color(cm(i//NUM_STYLES*float(NUM_STYLES)/NUM_COLORS))\n",
        "      # axes[i].set_linestyle(LINE_STYLES[i%NUM_STYLES])\n",
        "    plt.savefig(f'Run_{run+1}_step.png',format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "\n",
        "# Plot Activation graph epochwise\n",
        "def plot_epochwise_activations(hidden_layer_no, hidden_layer_outs_5_runs):\n",
        "  temp = []\n",
        "  y_label = ''\n",
        "\n",
        "  NUM_COLORS = n_neurons\n",
        "  LINE_STYLES = ['solid', 'dashed', 'dashdot', 'dotted']\n",
        "  NUM_STYLES = len(LINE_STYLES)\n",
        "  cm = plt.get_cmap('gist_rainbow')\n",
        "\n",
        "  hidden_layer_no = hidden_layer_no\n",
        "  for run in range(len(hidden_layer_outs_5_runs)):\n",
        "    f, axes = plt.subplots(1, 4,figsize = (7*5,7))\n",
        "    values = []\n",
        "    for epoch in range(len(hidden_layer_outs_5_runs[run])):\n",
        "        sub_h_l_outs = hidden_layer_outs_5_runs[run][epoch][-1][hidden_layer_no-1][[x1_idx, x2_idx, x3_idx, x4_idx]]\n",
        "        values.append(sub_h_l_outs)\n",
        "      \n",
        "    values = np.array(values)\n",
        "    plt.suptitle(f'Run {run+1} for Hidden Layer No. {hidden_layer_no}',ha = 'center', fontsize = 15)\n",
        "    # x = [0] +list(np.arange(1, epoch_stop_point+1 , 1)) +list(np.arange(start_point, n_epochs+1, epoch_stepsize))\n",
        "    for i in range(4):\n",
        "      lines = axes[i].plot(values[:][:,i])\n",
        "      axes[i].set_title(f'x_{i+1}')\n",
        "      \n",
        "      axes[i].set_xlabel(f'Epoch', fontsize = 12)\n",
        "      axes[i].set_ylabel(y_label, fontsize = 12)\n",
        "      axes[i].set_xticks(np.arange(0,len(values), 10.0))\n",
        "\n",
        "      for j in range(NUM_COLORS):\n",
        "        lines[j].set_color(cm(j//NUM_STYLES*float(NUM_STYLES)/NUM_COLORS))\n",
        "        lines[j].set_linestyle(LINE_STYLES[j%NUM_STYLES])\n",
        "    plt.savefig(f'Run_{run+1}_epoch.png',format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# #For step wise loss analysis\n",
        "# def plot_stepwise_loss(state_info_5_runs):\n",
        "#   f, axes = plt.subplots(1, 5,figsize = (7*5,7))\n",
        "#   for run in range(len(state_info_5_runs)):\n",
        "#     values = []\n",
        "    \n",
        "#     for epoch in range(len(state_info_5_runs[run])):\n",
        "#       for step in range(len(state_info_5_runs[run][epoch])):\n",
        "#         train_loss = state_info_5_runs[run][epoch][step]['train_loss']\n",
        "#         test_loss = state_info_5_runs[run][epoch][step]['test_loss']\n",
        "\n",
        "#         values.append([train_loss, test_loss])\n",
        "        \n",
        "#       if epoch == step_stop_point:\n",
        "#         break\n",
        "#     values = np.array(values)\n",
        "#     x = [0] + list(np.arange(step_stepsize, len(values)*step_stepsize, step_stepsize))\n",
        "#     plt.suptitle('Loss v/s step',ha = 'center', fontsize = 15)\n",
        "#     # for i in range(len(values)):\n",
        "#     axes[run].plot(x,values[:,0], label = 'train loss')\n",
        "#     axes[run].plot(x,values[:,1], label = 'test loss')\n",
        "#     axes[run].legend(loc=\"best\")\n",
        "#     axes[run].set_title(f'Run {run+1}')\n",
        "#     axes[run].set_xlabel(f'Step', fontsize = 12)\n",
        "#     axes[run].set_ylabel(f'Loss', fontsize = 12)\n",
        "#       # axes[run].set_xticks(np.arange(0,len(values), 2.0))\n",
        "      \n",
        "\n",
        "#   plt.savefig('loss_step' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#   plt.show()\n",
        "\n",
        "# # For epoch wise loss analysis\n",
        "# def plot_epochwise_loss(state_info_5_runs):\n",
        "#   f, axes = plt.subplots(1, 5,figsize = (7*5,7))\n",
        "#   for run in range(len(state_info_5_runs)):\n",
        "#     values = []\n",
        "#     state = []\n",
        "#     for epoch in range(len(state_info_5_runs[run])):\n",
        "#       train_loss = state_info_5_runs[run][epoch][-1]['train_loss']\n",
        "#       test_loss = state_info_5_runs[run][epoch][-1]['test_loss']\n",
        "#       values.append([train_loss, test_loss])\n",
        "#       state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "      \n",
        "#     values = np.array(values)\n",
        "#     # x = [0] +list(np.arange(1, epoch_stop_point+1 , 1)) +list(np.arange(start_point, n_epochs+1, epoch_stepsize))\n",
        "#     plt.suptitle('Loss v/s epoch',ha = 'center', fontsize = 15)\n",
        "#     # for i in range(len(values)):\n",
        "#     axes[run].plot(state,values[:,0], label = 'train loss')\n",
        "#     axes[run].plot(state,values[:,1], label = 'test loss')\n",
        "#     axes[run].legend(loc=\"best\")\n",
        "#     axes[run].set_title(f'Run {run+1}')\n",
        "#     axes[run].set_xlabel(f'Epoch', fontsize = 12)\n",
        "#     axes[run].set_ylabel(f'Loss', fontsize = 12)\n",
        "#       # axes[run].set_xticks(np.arange(0,len(values), 10.0))\n",
        "\n",
        "#   plt.savefig('loss_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#   plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "rvFtZzCflhGB"
      },
      "outputs": [],
      "source": [
        "def plot_routine(state_info_5_runs, is_stepwise,x_label, train_quant, test_quant,supertitle, figname):\n",
        "  f, axes = plt.subplots(1, n_runs,figsize = (7*n_runs,7))\n",
        "  if n_runs == 1:\n",
        "    axes = [axes]\n",
        "  for run in range(len(state_info_5_runs)):\n",
        "    values = []\n",
        "    state = []\n",
        "    for epoch in range(len(state_info_5_runs[run])):\n",
        "      if is_stepwise:\n",
        "        for step in range(len(state_info_5_runs[run][epoch])):\n",
        "          train_q = state_info_5_runs[run][epoch][step][train_quant]\n",
        "          test_q = state_info_5_runs[run][epoch][step][test_quant]\n",
        "          values.append([train_q, test_q])\n",
        "          state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "        \n",
        "        if epoch == step_stop_point:\n",
        "          break\n",
        "      else:\n",
        "        \n",
        "        train_q = state_info_5_runs[run][epoch][-1][train_quant]\n",
        "        test_q = state_info_5_runs[run][epoch][-1][test_quant]\n",
        "        values.append([train_q, test_q])\n",
        "        state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "    \n",
        "    values = np.array(values)\n",
        "    if is_stepwise:\n",
        "      x = [0] + list(np.arange(step_stepsize, len(values)*step_stepsize, step_stepsize))\n",
        "    else:\n",
        "      x = state\n",
        "    plt.suptitle(supertitle,ha = 'center', fontsize = 15)\n",
        "    # for i in range(len(values)):\n",
        "    axes[run].plot(x,values[:,0], 'b-', label = train_quant)\n",
        "    if test_quant == 'test_acc' or test_quant == 'test_loss':\n",
        "      axes[run].plot(x,values[:,1],'r-', label = test_quant)\n",
        "    axes[run].annotate(str(values[-1,0]),xy=(x[-1]-.5,values[-1,0]))\n",
        "    axes[run].annotate(str(values[-1,1]),xy=(x[-1]-.5,values[-1,1]))\n",
        "    axes[run].legend(loc=\"best\")\n",
        "    axes[run].set_title(f'Run {run+1}')\n",
        "    axes[run].set_xlabel(f'{x_label}', fontsize = 12)\n",
        "    # axes[run].set_ylabel(f'Loss', fontsize = 12)\n",
        "      # axes[run].set_xticks(np.arange(0,len(values), 10.0))\n",
        "\n",
        "  plt.savefig(figname + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_stepwise_loss(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = True,x_label = 'Step', train_quant = 'train_loss', test_quant = 'test_loss',supertitle = 'Loss v/s Step', figname='loss_step')\n",
        "def plot_epochwise_loss(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = False,x_label = 'Epoch', train_quant = 'train_loss', test_quant = 'test_loss',supertitle = 'Loss v/s Epoch', figname='loss_epoch')\n",
        "def plot_stepwise_acc(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = True,x_label = 'Step', train_quant = 'train_acc', test_quant = 'test_acc',supertitle = 'Acc v/s Step', figname='acc_step')\n",
        "def plot_epochwise_acc(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = False,x_label = 'Epoch', train_quant = 'train_acc', test_quant = 'test_acc',supertitle = 'Acc v/s Epoch', figname='acc_epoch')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eigen_vals_kernels = []"
      ],
      "metadata": {
        "id": "Ak5umD8_0HcU"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "iCAAjeQX1EhA"
      },
      "outputs": [],
      "source": [
        "def find_nearest(array, value):\n",
        "    array = np.asarray(array)\n",
        "    idx = (np.abs(array - value)).argmin()\n",
        "    return idx\n",
        "\n",
        "def get_4_inp_idxs(X_sorted, point1_degrees = 40, point2_degrees = 300, diff_in_degrees = 10):\n",
        "\n",
        "  temp = np.array([(angle + 2*np.pi)*180/np.pi if angle < 0 else angle*180/np.pi for angle in np.arctan2(X_sorted[:,1],X_sorted[:,0]) ], dtype = int)\n",
        "  value1 = point1_degrees\n",
        "  x1_idx = find_nearest(temp, value1)\n",
        "  x2_idx = find_nearest(temp, value1+diff_in_degrees)\n",
        "\n",
        "  value2 = point2_degrees\n",
        "  x3_idx = find_nearest(temp, value2)\n",
        "  x4_idx = find_nearest(temp, value2+diff_in_degrees)\n",
        "  return x1_idx, x2_idx, x3_idx, x4_idx\n",
        "\n",
        "def get_kernels(protocol, hidden_layer_outputs):\n",
        "  hidden_layer_outputs = copy.deepcopy(hidden_layer_outputs)\n",
        "  n_hidden_layer = len(hidden_layer_outputs)\n",
        "  for i in range(n_hidden_layer):\n",
        "    # print(np.array(hidden_layer_outputs[i]).shape)\n",
        "    if protocol == \"MLP\":\n",
        "      temp = np.sign(hidden_layer_outputs[i])\n",
        "    else:\n",
        "      temp = torch.sigmoid(torch.tensor(hidden_layer_outputs[i]))\n",
        "      temp = np.array(temp)\n",
        "    temp[temp <= 0] = 0\n",
        "    # print(temp)\n",
        "    hidden_layer_outputs[i] = temp#.detach().to(\"cpu\"))\n",
        "\n",
        "  hidden_layer_kernels = []\n",
        "  for i in range(n_hidden_layer):\n",
        "    \n",
        "    \n",
        "    hidden_layer_kernels.append(np.matmul(hidden_layer_outputs[i],hidden_layer_outputs[i].T))\n",
        "\n",
        "  lambda_matrix = hidden_layer_kernels[0]\n",
        "  for i in range(1,n_hidden_layer):\n",
        "    lambda_matrix = np.multiply(lambda_matrix, hidden_layer_kernels[i])\n",
        "  # lambda_matrix = np.log10(lambda_matrix+1)\n",
        "  kernels = hidden_layer_kernels + [lambda_matrix]\n",
        "  \n",
        "  #Normalizing overlap matrix by its trace\n",
        "  # eigen_vals = eigh(lambda_matrix, eigvals_only = True)\n",
        "  # lambda_matrix = lambda_matrix/np.sum(eigen_vals)\n",
        "  \n",
        "  return [lambda_matrix]#kernels\n",
        "def get_YKYs(kernels):\n",
        "  \n",
        "  # YKYs = []\n",
        "  # for i in range(len(kernels)):\n",
        "  #   YKY = np.matmul(np.matmul(Y_sorted.T, kernels[i]), Y_sorted)\n",
        "  #   eigen_vals = eigh(kernels[i], eigvals_only = True)\n",
        "  #   YKYs.append(str(round(YKY[0][0]/eigen_vals[-1], 2)))\n",
        "  YKYs = ['NA']*len(kernels)\n",
        "  return YKYs\n",
        "\n",
        "def build_kernels(protocol, hidden_layer_outs_5_runs):\n",
        "  h_l_o = hidden_layer_outs_5_runs\n",
        "  kernel_5_runs = []\n",
        "  # start_end = [0, -1]\n",
        "  for run in range(len(h_l_o)):\n",
        "      kernel_epochs = []\n",
        "      for epoch in range(len(h_l_o[run])):\n",
        "      # for epoch in start_end:\n",
        "        kernel_step = []\n",
        "        for step in range(len(h_l_o[run][epoch])):\n",
        "        # for step in [-1]:\n",
        "          kernels = get_kernels(protocol, h_l_o[run][epoch][step])\n",
        "          \n",
        "          kernel_step.append(kernels)\n",
        "        kernel_epochs.append(kernel_step)\n",
        "      kernel_5_runs.append(kernel_epochs)\n",
        "  return kernel_5_runs\n",
        "# def build_kernels(protocol, hidden_layer_outs_5_runs, state_info_5_runs):\n",
        "#   h_l_o = hidden_layer_outs_5_runs\n",
        "#   kernel_5_runs = []\n",
        "#   instance = 100\n",
        "  \n",
        "#   s_i = state_info_5_runs\n",
        "#   for run in range(1):\n",
        "#       kernel_epochs = []\n",
        "#       for epoch in range(len(h_l_o[run])):\n",
        "#         kernel_step = []\n",
        "#         if int(state_info_5_runs[run][epoch][-1]['epoch'])>= 200:\n",
        "#           for step in range(len(h_l_o[run][epoch])):\n",
        "            \n",
        "#             kernels = get_kernels(protocol, h_l_o[run][epoch][step])\n",
        "#             for mode in range(num_modes):   \n",
        "            \n",
        "#               curr_mode_kernels = np.array(kernels)[:][:,mode*instance : (mode+1)*instance][ :,:,mode*instance : (mode+1)*instance]\n",
        "              \n",
        "#               YKYs = get_YKYs(curr_mode_kernels)\n",
        "#               plot_heatmap(curr_mode_kernels,YKYs, s_i[run][epoch][step], all_input = True, save_fig = True, show_fig = False, mode = mode+1)\n",
        "#               del curr_mode_kernels\n",
        "#               gc.collect()\n",
        "#             del kernels\n",
        "#             gc.collect()\n",
        "#     #     kernel_step.append(kernels)\n",
        "#       #   kernel_epochs.append(kernel_step)\n",
        "#       # kernel_5_runs.append(kernel_epochs)\n",
        "#   return kernel_5_runs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "j-Y4JNYHKQ8h"
      },
      "outputs": [],
      "source": [
        "# build_kernels(protocol, hidden_layer_outs_5_runs, state_info_5_runs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "iAszjRQsoDG2"
      },
      "outputs": [],
      "source": [
        "def plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  start_end = [0, 3, -1]\n",
        "  for run in range(1):\n",
        "  # for run in [0, 2]:\n",
        "    # run = 2\n",
        "    # for epoch in range(len(kernel_5_runs[run])):\n",
        "    for epoch in start_end:\n",
        "      # for step in range(len(kernel_5_runs[run][epoch])):\n",
        "      for step in [-1]:\n",
        "        # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "        kernels = kernel_5_runs[run][epoch][step]\n",
        "        YKYs = get_YKYs(kernels)\n",
        "        plot_heatmap(kernels,YKYs, s_i[run][epoch][step], all_input = for_all_inputs, save_fig = save_fig, show_fig = show_fig)\n",
        "      if (not for_all_inputs) and epoch == 2:\n",
        "        print(run, epoch)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s_i = state_info_5_runs\n",
        "# start_end = [0, 3,4, 5, 6, 7 ,8, -1]\n",
        "# for run in range(1):\n",
        "# # for run in [0, 2]:\n",
        "#   # run = 2\n",
        "#   # for epoch in range(len(kernel_5_runs[run])):\n",
        "#   for epoch in start_end:\n",
        "#     # for step in range(len(kernel_5_runs[run][epoch])):\n",
        "#     for step in [-1]:\n",
        "#       print(s_i[run][epoch][step])"
      ],
      "metadata": {
        "id": "RY7CuMJGn86T"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "Wu_1uUR57hm0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', \n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "\n",
        "def plot_y_prediction(Y_sorted, predictions_run):\n",
        "  init_notebook_mode(connected=True)\n",
        "\n",
        "  #from plotly sliders\n",
        "  data = [dict(\n",
        "          visible = False,\n",
        "          line=dict(color='#00CED1', width=2),\n",
        "          name = ' = '+str(\"step\"),\n",
        "          x = np.arange(0,500,1),\n",
        "          y = d.to('cpu').flatten(),\n",
        "          #  y =Y_sorted.flatten()\n",
        "        ) for d in predictions_run]\n",
        "  data[10]['visible'] = True\n",
        "\n",
        "  data2 = [dict(\n",
        "          visible = False,\n",
        "          line=dict(color='#00CED1', width=2),\n",
        "          name = ' = '+str(\"step\"),\n",
        "          x = np.arange(0,500,1),\n",
        "          y = Y_sorted.flatten(),\n",
        "        ) for d in predictions_run]\n",
        "  data[10]['visible'] = True\n",
        "\n",
        "  #configure added to visualize in colab\n",
        "  configure_plotly_browser_state()\n",
        "\n",
        "  steps = []\n",
        "  for i in range(len(data)):\n",
        "      step = dict(\n",
        "          method = 'restyle',  \n",
        "          args = ['visible', [False] * len(data)],\n",
        "      )\n",
        "      step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
        "      steps.append(step)\n",
        "\n",
        "  sliders = [dict(\n",
        "      active = 10,\n",
        "      currentvalue = {\"prefix\": \"Frequency: \"},\n",
        "      pad = {\"t\": 50},\n",
        "      steps = steps\n",
        "  )]\n",
        "\n",
        "  layout = dict(sliders=sliders)\n",
        "\n",
        "  fig = dict(data=data, layout=layout)\n",
        "\n",
        "  fig2 = dict(data=data2, layout=layout)\n",
        "  configure_plotly_browser_state()\n",
        "\n",
        "  iplot(fig, filename='Sine Wave Slider')\n",
        "  iplot(fig2, filename='Sine Wave Slider')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQBvVtdXmGid"
      },
      "source": [
        "#Plot Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "B5J4Z3ml63Du"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "Y-lH0wiPbaZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "e8f87939-432c-4e8f-902a-b134ef9737c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot_labels = ['ReLU', 'DLGN-BOTH-PWC','DLGN-ONPV-PWC', 'DLGN-BOTH-ONPV-PWC']\n",
        "# plot_label = 2\n",
        "# plot_predictions(None, state_collections[plot_label][1], state_collections[plot_label][0],for_all_inputs = True, save_fig = True, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_predictions(None, state_collections[0][1], state_collections[0][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[1][1], state_collections[1][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[2][1], state_collections[2][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[3][1], state_collections[3][0],for_all_inputs = True, save_fig = False, show_fig = True)"
      ],
      "metadata": {
        "id": "92YyDda2kS-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "id": "J37AGK_2f18f"
      },
      "outputs": [],
      "source": [
        "def plot_predictions_old(mode, predictions_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  test_data = test_data_curr\n",
        "  test_labels = test_labels_curr\n",
        "  num_data = len(test_data)\n",
        "  # mode = mode\n",
        "  # size_low_mode = num_data/2/20\n",
        "  # if mode == 21:\n",
        "  #   strt = int(num_data/2)\n",
        "  #   end = num_data\n",
        "  #   reqd_mode  = test_data[strt:end,-2:]\n",
        "  # else:\n",
        "  #   strt = int((mode-1)*size_low_mode)\n",
        "  #   end = int(mode*size_low_mode)\n",
        "  #   reqd_mode  = test_data[strt:end,(mode-1)*2:mode*2]\n",
        "  start_end = [0, 3, -1]\n",
        "  for run in range(1):\n",
        "    # for epoch in range(len(predictions_5_runs[run])):\n",
        "    for epoch in start_end:\n",
        "      # for step in range(len(predictions_5_runs[run][epoch])):\n",
        "      for step in [-1]:\n",
        "        fig, ax = plt.subplots(1,2, figsize = (10, 5))\n",
        "        # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "        preds = predictions_5_runs[run][epoch][step]\n",
        "        # preds =  preds.softmax(1).argmax(1).detach().to('cpu').numpy()\n",
        "        preds =  preds.detach().to('cpu').numpy()\n",
        "        angle = np.arange(0, 360, 360/500)\n",
        "        ax[1].plot(angle, preds)\n",
        "        ax[0].plot(angle, test_labels)\n",
        "        \n",
        "       \n",
        "        # ax[0].scatter(reqd_mode[:,0],reqd_mode[:,1],c =  test_labels[strt:end])\n",
        "        # ax[1].scatter(reqd_mode[:,0],reqd_mode[:,1],c = preds[strt:end])\n",
        "        # curr_acc = accuracy_score(test_labels[strt:end], preds[strt:end])\n",
        "        curr_acc = 'NA'\n",
        "        state_info = state_info_5_runs[run][epoch][step]\n",
        "        title = state_info['model_protocol_type']+','+'Run=' + str(state_info[\"run\"])+',' +'Epoch = '+str(state_info['epoch'])+','+'step='+ str(state_info['step']) +',' + \"tr_loss=\" + str(state_info['train_loss']) +',' +'tr_acc='+str(state_info['train_acc'])+','+'te_acc='+str(state_info['test_acc'])+','+'curr_mode_acc='+str(curr_acc)\n",
        "        plt.suptitle(title)\n",
        "\n",
        "        if show_fig:\n",
        "          plt.show()\n",
        "        if save_fig:\n",
        "          fig.savefig(f'Preds({title}).png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        "        \n",
        "        plt.close(fig)\n",
        "\n",
        "def plot_predictions(mode, predictions_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  test_data = test_data_curr\n",
        "  test_labels = test_labels_curr\n",
        "  num_data = len(test_data)\n",
        "  \n",
        "  start_end = [0, 3, -1]\n",
        "  plot_labels = ['Initial Epoch', 'Mid($3^{rd}$) epoch', 'Final($8000^{th}$) epoch']\n",
        "  plot_labels_name = ['ReLU', 'DLGN-BOTH-PWC','DLGN-ONPV-PWC', 'DLGN-BOTH-ONPV-PWC']\n",
        "  fig, ax = plt.subplots(1,1, figsize = (10, 5))\n",
        "  \n",
        "  angle = np.arange(0, 360, 360/500)\n",
        "  ax.plot(angle, test_labels, 'black', label = 'True label function', linewidth = 1.5)\n",
        "  for run in range(1):\n",
        "    # for epoch in range(len(predictions_5_runs[run])):\n",
        "    for idx, epoch in enumerate(start_end):\n",
        "      # for step in range(len(predictions_5_runs[run][epoch])):\n",
        "      for step in [-1]:\n",
        "        \n",
        "        # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "        preds = predictions_5_runs[run][epoch][step]\n",
        "        # preds =  preds.softmax(1).argmax(1).detach().to('cpu').numpy()\n",
        "        preds =  preds.detach().to('cpu').numpy()\n",
        "       \n",
        "        ax.plot(angle, preds,linestyle='dashed', label = plot_labels[idx], linewidth = 2.5 )\n",
        "       \n",
        "        \n",
        "       \n",
        "        # ax[0].scatter(reqd_mode[:,0],reqd_mode[:,1],c =  test_labels[strt:end])\n",
        "        # ax[1].scatter(reqd_mode[:,0],reqd_mode[:,1],c = preds[strt:end])\n",
        "        # curr_acc = accuracy_score(test_labels[strt:end], preds[strt:end])\n",
        "  \n",
        "  plt.xlabel('angle in radians')\n",
        "  plt.xticks([0, 180, 360], ['0', '$\\pi$', '$2\\pi$'])\n",
        "  title = f'{plot_labels_name[plot_label]} Predictions'\n",
        "  # plt.suptitle(title)\n",
        "  plt.legend()\n",
        "  if show_fig:\n",
        "    plt.show()\n",
        "  if save_fig:\n",
        "    fig.savefig(f'Preds({title}).pdf',format = 'pdf', bbox_inches='tight', dpi = 200)\n",
        "  \n",
        "  plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {
        "id": "hpXbBBPVw3Sx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "00a7f675-5e18-4f24-fd2e-e1167c1d5674"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot_loss_iclr( save_fig = True, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state_collections[0] = state_collections[-1]"
      ],
      "metadata": {
        "id": "aknBkeelwe79"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {
        "id": "uW_PYbtevFX0"
      },
      "outputs": [],
      "source": [
        "def plot_loss_iclr( save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  test_data = test_data_curr\n",
        "  test_labels = test_labels_curr\n",
        "  num_data = len(test_data)\n",
        "  \n",
        "  \n",
        "  plot_labels = ['ReLU', 'DLGN','DLGN-Fixed-Gating', 'DLGN-Hybrid']\n",
        "  fig, ax = plt.subplots(1,1, figsize = (10, 5))\n",
        "  \n",
        "  \n",
        "  for i in range(4):\n",
        "    loss = []\n",
        "    x = []\n",
        "    for run in range(1):\n",
        "      for epoch in range(len(state_collections[i][0][run])):\n",
        "        for step in [-1]:\n",
        "        # for step in range(len(predictions_5_runs[run][epoch])):\n",
        "        \n",
        "          \n",
        "          # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "          x.append(int(state_collections[i][0][run][epoch][step]['epoch']))\n",
        "          loss.append(state_collections[i][0][run][epoch][step]['train_loss'])\n",
        "          # preds =  preds.softmax(1).argmax(1).detach().to('cpu').numpy()\n",
        "          \n",
        "        \n",
        "    ax.plot(x, loss, label = plot_labels[i] )\n",
        "        \n",
        "          \n",
        "        \n",
        "          # ax[0].scatter(reqd_mode[:,0],reqd_mode[:,1],c =  test_labels[strt:end])\n",
        "          # ax[1].scatter(reqd_mode[:,0],reqd_mode[:,1],c = preds[strt:end])\n",
        "          # curr_acc = accuracy_score(test_labels[strt:end], preds[strt:end])\n",
        "    \n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('MSE loss')\n",
        "  title = 'Loss curves'\n",
        "  plt.suptitle(title)\n",
        "  plt.legend()\n",
        "  if show_fig:\n",
        "    plt.show()\n",
        "  if save_fig:\n",
        "    fig.savefig(f'Loss({title}).pdf',format = 'pdf', bbox_inches='tight', dpi = 200)\n",
        "  \n",
        "  plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkjaTiLSmMUT"
      },
      "source": [
        "#*CNN* routine funtions for generting hyperplanes and local usefulness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "iB2mbIIVuX_n"
      },
      "outputs": [],
      "source": [
        "def multiply_higher_dimensional_matrices(a, b):\n",
        "  a_shape, b_shape = a.size(), b.size()\n",
        "  ab = []\n",
        "  \n",
        "  for r in range(a_shape[0]):\n",
        "    temp = []\n",
        "    for c in range(b_shape[1]):\n",
        "      temp.append(torch.sum(torch.matmul(a[r,:], b[:,c]), axis = 0))\n",
        "    temp = torch.stack(temp)\n",
        "    ab.append(temp)\n",
        "  return torch.stack(ab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "9WOZquNTLtuW"
      },
      "outputs": [],
      "source": [
        "def convert_cnn_weights(layer_weights, layer_biases, protocol_type = 'DLGN'):\n",
        "  l_w, l_b = get_cnn_flattend_weights(torch.tensor(layer_weights[0]).to(device)), get_cnn_flattend_bias(torch.tensor(layer_biases[0]).to(device))\n",
        "  l_ws = [l_w]\n",
        "  l_bs = [l_b]\n",
        "  \n",
        "  for i in range(1, n_hidden_layers):\n",
        "    \n",
        "    curr_l_w =  torch.tensor(layer_weights[i]).to(device)\n",
        "    curr_l_b = torch.tensor(layer_biases[i]).to(device)\n",
        "    \n",
        "    if protocol_type == 'DLGN':\n",
        "\n",
        "        if len(curr_l_w.shape) > 2: \n",
        "          flattend_curr_l_w = get_cnn_flattend_weights(curr_l_w)\n",
        "          l_w = torch.matmul(input = flattend_curr_l_w ,other = l_w) #multiply_higher_dimensional_matrices(curr_layer_weight, layer_weight)\n",
        "          l_b =  torch.matmul(input= flattend_curr_l_w, other =  l_b) + get_cnn_flattend_bias(curr_l_b)\n",
        "        else:\n",
        "          l_w = torch.matmul(input= curr_l_w, other =  l_w)\n",
        "          l_b = torch.matmul(input= curr_l_w, other =  l_b) + curr_l_b\n",
        "\n",
        "    elif protocol_type == 'DLGN-SF':\n",
        "      layer_weight = curr_layer_weight\n",
        "      layer_bias = curr_layer_bias\n",
        "\n",
        "    \n",
        "    l_ws.append(l_w)\n",
        "    l_bs.append(l_b)\n",
        "  return l_ws, l_bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "YJMpGRE8heqa"
      },
      "outputs": [],
      "source": [
        "def get_cnn_flattend_weights(layer_weight):\n",
        "  \n",
        "  kernel_size = 5\n",
        "  filter_size = 8\n",
        "  ks = kernel_size\n",
        "  padding = int((ks - 1)/2)\n",
        "  start,end = 0, 32#-padding, 32-int(ks/2)   \n",
        "  hyperplanes = [ [] for i in range(filter_size)]\n",
        "  count = 0\n",
        "  \n",
        "  for i in range(start, end, 1):\n",
        "    for j in range(start, end, 1):\n",
        "      map_idx = []\n",
        "      for fs_i in range(ks):\n",
        "        for fs_j in range(ks):\n",
        "          map_idx.append( 36 * (i+fs_i) + j+fs_j )#(i+fs_i,j+fs_j))\n",
        "      for filter in range(filter_size):\n",
        "        # print(f\"filter n { filter}\")\n",
        "        for channel_n in range(layer_weight.shape[1]):\n",
        "          # print(f\"channel n { channel_n}\")\n",
        "          channel =  torch.zeros(size = (36*36,)).to(device)\n",
        "          channel[map_idx] = layer_weight[filter][channel_n].flatten()\n",
        "          channel = channel.reshape((36, 36))\n",
        "          \n",
        "          channel = channel[2:-2, 2:-2]\n",
        "          channel = channel.flatten()\n",
        "          \n",
        "          if channel_n == 0:\n",
        "            temp = channel\n",
        "            \n",
        "          else:\n",
        "            temp = torch.cat((temp, channel))\n",
        "        hyperplanes[filter].append(temp)\n",
        "        \n",
        "      \n",
        "  for idx in range(len(hyperplanes)):\n",
        "    hyperplanes[idx] = torch.stack(hyperplanes[idx]).to(device)\n",
        "  \n",
        "  hyperplanes = torch.stack(hyperplanes).to(device)\n",
        "\n",
        "  # hyperplanes = torch.cat([torch.tensor(x for x in hyperplanes], dim=0)\n",
        "  hyperplanes = hyperplanes.reshape((-1, hyperplanes.shape[-1]))\n",
        "  return hyperplanes\n",
        "\n",
        "def get_cnn_flattend_bias(bias):\n",
        "  return bias.repeat(32*32,1).T.flatten().float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "qac5xEWvgL-s"
      },
      "outputs": [],
      "source": [
        "# a, b = unknown(weights_biases_all_runs, .05, test_data_curr, test_labels_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "N0LLheNdSirl"
      },
      "outputs": [],
      "source": [
        "# plot_hyp_posneg(posneg_pairs_all_runs = a, epsilon = .05)\n",
        "# plot_hyp_posneg_ratio(posneg_pairs_all_runs = a, epsilon = .05)\n",
        "# plot_hyp_local_usefulness(local_usefulness_all_runs = b, epsilon = .05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "99ZgmhfMqAMa"
      },
      "outputs": [],
      "source": [
        "# layer_weights = weights_biases_all_runs[0][0][0][0]\n",
        "# layer_biases = weights_biases_all_runs[0][0][0][1]\n",
        "# for i in range(n_hidden_layers):\n",
        "#   temp = get_cnn_flattend_weights(layer_weights[i])\n",
        "#   print(temp.dtype)\n",
        "#   print(get_cnn_flattend_bias(layer_biases[i]).dtype)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "qDPQ9-SYklDB"
      },
      "outputs": [],
      "source": [
        "# def unknown_routine(layer_weights, layer_biases, data, i, temp_data):\n",
        "#   if is_DNN:\n",
        "#         weight_mags = np.linalg.norm(layer_weights[i], axis = 1)\n",
        "#         weight_mags = np.expand_dims(weight_mags, axis =1)\n",
        "#         num = np.absolute(np.matmul(layer_weights[i], data.T) + np.expand_dims(layer_biases[i], axis = 1))\n",
        "#         d = num / weight_mags #n_nodes x n_examples\n",
        "#         d = d.T # n_examples x n_nodes \n",
        "#         # d = np.absolute(np.multiply(np.expand_dims(inp_plt_x, axis = 1), np.expand_dims(layer_slopes[i], axis = 0)) + np.expand_dims(inp_plt_y*(-1), axis = 1) + np.expand_dims(layer_intercepts[i], axis = 0)) / np.expand_dims(np.sqrt(layer_slopes[i]**2 + 1), axis = 0)\n",
        "#   else:\n",
        "#     l_weights = get_cnn_flattend_weights(layer_weights[i])\n",
        "#     weight_mags = np.linalg.norm(l_weights, axis = -1)\n",
        "#     weight_mags = np.expand_dims(weight_mags, axis =-1)\n",
        "#     num = np.absolute(np.matmul(l_weights, temp_data.T) + np.expand_dims(layer_biases[i], axis = (1,2)))\n",
        "#     d = num / weight_mags #n_nodes x n_examples\n",
        "#     d = np.reshape(d, (-1, temp_data.shape[0]))\n",
        "#     d = d.T #n_examples x n_nodes\n",
        "#     # d = d[:,np.random.randint(0, d.shape[1], size = (500,))]\n",
        "#   return d\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "alCUaO77Ozz0"
      },
      "outputs": [],
      "source": [
        "# mask = torch.ones((2000)).long()\n",
        "# dat = torch.rand((2000, 3072))\n",
        "# dat[mask,:].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "pzTIkLg-Oy08"
      },
      "outputs": [],
      "source": [
        "# print(bool_mask[:,j].shape)\n",
        "#           print(temp_data[bool_mask[:,j], :].T.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "rMMBi0ADoTfJ"
      },
      "outputs": [],
      "source": [
        "# posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs, .015, test_data_curr, test_labels_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "0ZD8G3lLCMkx"
      },
      "outputs": [],
      "source": [
        "# a, b = unknown(weights_biases_all_runs, .25, test_data_curr, test_labels_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "gmYi4aZXOl82"
      },
      "outputs": [],
      "source": [
        "def  get_hyperplanes_params(model,protocol_type,  for_layers, for_mode):\n",
        "  layer_weights = []\n",
        "  layer_slopes = []\n",
        "  layer_intercepts = []\n",
        "  layer_biases = []\n",
        "  # if for_mode == 1:\n",
        "  #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,0:2]\n",
        "  # elif for_mode == 21:\n",
        "  #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,-2:]\n",
        "  layer_weight = model.layers[0].weight.detach().clone()\n",
        "  layer_bias = model.layers[0].bias.detach().clone()\n",
        "  if is_DNN:\n",
        "    pass\n",
        "    # layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "    # layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "    # layer_slopes.append(layer_slope)\n",
        "    # layer_intercepts.append(layer_intercept)\n",
        "\n",
        "  layer_weights.append(layer_weight)\n",
        "  layer_biases.append(layer_bias)\n",
        "  for i in range(1, for_layers):\n",
        "    curr_layer_weight =  model.layers[i].weight.detach().clone()\n",
        "    curr_layer_bias = model.layers[i].bias.detach().clone()\n",
        "    if protocol_type == 'DLGN':\n",
        "      if is_DNN:\n",
        "        layer_weight = torch.matmul(input= curr_layer_weight, other =  layer_weight)\n",
        "        layer_bias = torch.matmul(input= curr_layer_weight, other =  layer_bias) + curr_layer_bias\n",
        "      else:\n",
        "        layer_weight = curr_layer_weight #multiply_higher_dimensional_matrices(curr_layer_weight, layer_weight)\n",
        "        layer_bias =  curr_layer_bias\n",
        "    elif protocol_type == 'DLGN-SF':\n",
        "      layer_weight = curr_layer_weight\n",
        "      layer_bias = curr_layer_bias\n",
        "\n",
        "    if is_DNN:\n",
        "      pass\n",
        "      # layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "      # layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "    \n",
        "      # layer_slopes.append(layer_slope)\n",
        "      # layer_intercepts.append(layer_intercept)\n",
        "\n",
        "    layer_weights.append(layer_weight)\n",
        "    layer_biases.append(layer_bias)\n",
        "  return layer_weights, layer_biases, layer_slopes, layer_intercepts\n",
        "\n",
        "\n",
        "def plot_posneg_hyperplanes(model, for_layers, epsilon, model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "  layer_weights, layer_biases, _, _ = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "  return [layer_weights, layer_biases]\n",
        "\n",
        "\n",
        "def unknown(weights_biases_all_runs, epsilon, data, labels):\n",
        "  step = -1\n",
        "  w_idx, b_idx = 0, 1\n",
        "  \n",
        "  posneg_pairs_all_runs = []\n",
        "  local_usefulness_all_runs = []\n",
        "\n",
        "  for run in range(len(weights_biases_all_runs)):\n",
        "    posneg_pairs_all_epochs = []\n",
        "    local_usefulness_all_epochs = []\n",
        "\n",
        "    for epoch in range(len(weights_biases_all_runs[run])):\n",
        "      posneg_pairs_all_step = []\n",
        "      local_usefulness_all_step = []\n",
        "      for step in range(len(weights_biases_all_runs[run][epoch])):\n",
        "        layer_weights = weights_biases_all_runs[run][epoch][step][w_idx]\n",
        "        layer_biases = weights_biases_all_runs[run][epoch][step][b_idx]\n",
        "        \n",
        "        if not is_DNN:\n",
        "          layer_weights,layer_biases  = convert_cnn_weights(layer_weights, layer_biases, protocol_type = 'DLGN')\n",
        "\n",
        "        posneg_pairs_all_epochs.append(get_posneg_pairs(layer_weights, layer_biases, epsilon, data, labels))\n",
        "        local_usefulness_all_epochs.append(get_local_usefulness(layer_weights, layer_biases, epsilon, data, labels))\n",
        "\n",
        "      \n",
        "    posneg_pairs_all_runs.append(posneg_pairs_all_epochs)\n",
        "    local_usefulness_all_runs.append(local_usefulness_all_epochs)\n",
        "  return posneg_pairs_all_runs, local_usefulness_all_runs\n",
        "\n",
        "def get_posneg_pairs(layer_weights, layer_biases, epsilon, data, labels):\n",
        "  posneg_pairs_all_layers = []\n",
        "  temp_data = data.reshape((data.shape[0], -1))\n",
        "  # for i in range(data.shape[0]):\n",
        "  #   temp_data.append(np.ravel(data[i]))\n",
        "  temp_data = torch.tensor(temp_data, device = device)\n",
        "  \n",
        "  labels = torch.tensor(labels, device = device)\n",
        "  for i in range(n_hidden_layers):     \n",
        "      # print(layer_weights[i].shape)\n",
        "      # debug()\n",
        "      \n",
        "      if is_DNN:\n",
        "        # print(layer_weights[i].size())\n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = 1, keepdim = True)\n",
        "        # print(weight_mags.size())\n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis = 1))\n",
        "        # print(num.size())\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        d = d.T\n",
        "        # print(d.size())\n",
        "      else:\n",
        "        # l_weights = get_cnn_flattend_weights(layer_weights[i])\n",
        "        \n",
        "        # l_weights = torch.tensor(l_weights, device = device).float()\n",
        "        \n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = -1)\n",
        "        \n",
        "        \n",
        "        weight_mags = torch.unsqueeze(weight_mags, axis =-1)\n",
        "        \n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis = 1))\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        # d = torch.reshape(d, (-1, temp_data.size()[0]))\n",
        "        d = d.T #n_examples x n_nodes\n",
        "        \n",
        "        # d = d[:,np.random.randint(0, d.shape[1], size = (500,))]\n",
        "      \n",
        "      \n",
        "      bool_mask = d <= epsilon\n",
        "      posneg_pairs = []\n",
        "      if is_DNN:\n",
        "        for j in range(d.size()[1]):\n",
        "          data_points_within_epsilon = labels[:,None][bool_mask[:,j]]\n",
        "          n_pos_within_epsilon = torch.sum(data_points_within_epsilon)\n",
        "          n_neg_within_epsilon = torch.tensor(len(data_points_within_epsilon), device = device) - n_pos_within_epsilon\n",
        "          posneg_pairs.append((n_pos_within_epsilon.item(), n_neg_within_epsilon.item()))\n",
        "        posneg_pairs_all_layers.append(posneg_pairs)\n",
        "      else:\n",
        "        for j in range(d.size()[-1]):\n",
        "          \n",
        "          data_points_within_epsilon = labels[:,None][bool_mask[:,j]]\n",
        "         \n",
        "          n_pos_within_epsilon = torch.sum(data_points_within_epsilon)\n",
        "          n_neg_within_epsilon = torch.tensor(len(data_points_within_epsilon), device = device) - n_pos_within_epsilon\n",
        "          posneg_pairs.append((n_pos_within_epsilon.item(), n_neg_within_epsilon.item()))\n",
        "\n",
        "        posneg_pairs_all_layers.append(posneg_pairs)\n",
        "      \n",
        "  return posneg_pairs_all_layers\n",
        "def get_local_usefulness(layer_weights, layer_biases, epsilon, data, labels):\n",
        "  local_usefulness_all = []\n",
        "  signed_labels = torch.tensor(labels, device = device)\n",
        "  signed_labels[signed_labels == 0] = -1 \n",
        "  temp_data = data.reshape((data.shape[0], -1))\n",
        "  temp_data = torch.tensor(temp_data, device = device)\n",
        " \n",
        "  \n",
        "  for i in range(n_hidden_layers):\n",
        "      if is_DNN:\n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = -1, keepdim = True)\n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis =1))\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        # d = torch.reshape(d, (-1, temp_data.shape[0]))\n",
        "        d = d.T #n_examples x n_nodes\n",
        "        # d = np.absolute(np.multiply(np.expand_dims(inp_plt_x, axis = 1), np.expand_dims(layer_slopes[i], axis = 0)) + np.expand_dims(inp_plt_y*(-1), axis = 1) + np.expand_dims(layer_intercepts[i], axis = 0)) / np.expand_dims(np.sqrt(layer_slopes[i]**2 + 1), axis = 0)\n",
        "      else:\n",
        "        # l_weights = get_cnn_flattend_weights(layer_weights[i])\n",
        "        # l_weights = torch.tensor(l_weights, device = device).float()\n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = -1)\n",
        "        weight_mags = torch.unsqueeze(weight_mags, axis =-1)\n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis =1))\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        # d = torch.reshape(d, (-1, temp_data.shape[0]))\n",
        "        d = d.T #n_examples x n_nodes\n",
        "        # d = d[:,np.random.randint(0, d.shape[1], size = (500,))]\n",
        "        \n",
        "        \n",
        "       \n",
        "      local_usefulness_per_node = []\n",
        "      bool_mask = d <= epsilon\n",
        "   \n",
        "      l_u = torch.tensor(0, dtype = torch.float, device = device)\n",
        "      for j in range(d.size()[1]):\n",
        "        if is_DNN:\n",
        "        \n",
        "          wx_b = torch.matmul(layer_weights[i][j], temp_data[bool_mask[:,j]].T) + torch.unsqueeze(layer_biases[i], axis =1)[j]\n",
        "          signed_wx_b = torch.sign(wx_b)\n",
        "          \n",
        "          \n",
        "          temp_labels = (signed_labels[:,None][bool_mask[:,j]]).T\n",
        "         \n",
        "          # print(signed_wx_b.shape, np.multiply(signed_wx_b, labels).shape)\n",
        "          local_usefulness = torch.absolute(torch.sum(torch.multiply(signed_wx_b, temp_labels))/temp_labels.shape[1])\n",
        "          local_usefulness_per_node.append(local_usefulness.item())\n",
        "          \n",
        "        else:\n",
        "          # l_weights = torch.reshape(l_weights, (-1,temp_data.size()[1] ))\n",
        "          \n",
        "          # print(bool_mask[:,j].shape)\n",
        "          # print(temp_data[bool_mask[:,j], :].T.shape)\n",
        "          \n",
        "\n",
        "          # print(layer_weights[i][j].shape, temp_data[bool_mask[:,j]].T.shape, torch.unsqueeze(layer_biases[i], axis =1)[j])\n",
        "          \n",
        "          wx_b = torch.matmul(layer_weights[i][j], temp_data[bool_mask[:,j]].T) + torch.unsqueeze(layer_biases[i], axis =1)[j]\n",
        "          signed_wx_b = torch.sign(wx_b)\n",
        "          \n",
        "          temp_labels = (signed_labels[:,None][bool_mask[:,j]]).T\n",
        "\n",
        "          # print(signed_wx_b.shape, np.multiply(signed_wx_b, labels).shape)\n",
        "          local_usefulness = torch.absolute(torch.sum(torch.multiply(signed_wx_b, temp_labels))/temp_labels.shape[1])\n",
        "          # print(local_usefulness)\n",
        "          # debug()\n",
        "          \n",
        "          local_usefulness_per_node.append(local_usefulness.item())\n",
        "          # if (j != 0 and j%784 == 0):\n",
        "            \n",
        "          #   local_usefulness_per_node.append(l_u.item()/784)\n",
        "          #   l_u = torch.tensor(0, dtype = torch.float, device = device)\n",
        "\n",
        "          # l_u += local_usefulness\n",
        "      \n",
        "      # local_usefulness_per_node = np.reshape(local_usefulness_per_node, (8, -1))\n",
        "      # local_usefulness_per_node = np.partition(local_usefulness_per_node, kth = -800, axis = -1)\n",
        "      # local_usefulness_per_node = local_usefulness_per_node[ -800:]\n",
        "      # local_usefulness_per_node = np.mean(local_usefulness_per_node, axis = 1)\n",
        "      local_usefulness_all.append(local_usefulness_per_node)\n",
        "  return np.array(local_usefulness_all)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M72vt8AmiUl"
      },
      "source": [
        "#Commented previous hyperplane codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "SE_8guUjmnGr"
      },
      "outputs": [],
      "source": [
        "# def plot_posneg_hyperplanessssss(model, for_layers, epsilon, model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "#   layer_weights, layer_biases, _, _ = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "#   # debug()\n",
        "#   if for_mode == 1:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "#   elif for_mode == 21:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,-2], test_data_curr[:,-1]\n",
        "#   n_posneg_pairs_all = []\n",
        "#   for i in range(for_layers):\n",
        "#     # for neuron in model.n_neurons:\n",
        "     \n",
        "#       weight_mags = np.linalg.norm(layer_weights[i], axis = 1)\n",
        "#       weight_mags = np.expand_dims(weight_mags, axis =1)\n",
        "#       num = np.absolute(np.matmul(layer_weights[i], test_data_curr.T) + np.expand_dims(layer_biases[i], axis = 1))\n",
        "#       d = num / weight_mags #n_nodes x n_examples\n",
        "#       d = d.T\n",
        "#       # d = np.absolute(np.multiply(np.expand_dims(inp_plt_x, axis = 1), np.expand_dims(layer_slopes[i], axis = 0)) + np.expand_dims(inp_plt_y*(-1), axis = 1) + np.expand_dims(layer_intercepts[i], axis = 0)) / np.expand_dims(np.sqrt(layer_slopes[i]**2 + 1), axis = 0)\n",
        "    \n",
        "#       # d = np.absolute((layer_slopes[i][neuron]*inp_plt_x + inp_plt_y*(-1) + layer_intercepts[i][neuron]) / np.sqrt(layer_slopes[i][neuron]**2 + 1))\n",
        "#       n_posneg_pairs = []\n",
        "#       for j in range(d.shape[1]):\n",
        "#         bool_mask = d <= epsilon\n",
        "#         n_pos_within_epsilon = np.sum(test_labels_curr[:,np.newaxis][bool_mask[:,j]])\n",
        "#         n_neg_within_epsilon = len(test_labels_curr[:,np.newaxis][bool_mask[:,j]]) - n_pos_within_epsilon\n",
        "#         n_posneg_pairs.append((n_pos_within_epsilon, n_neg_within_epsilon))\n",
        "\n",
        "#       n_posneg_pairs_all.append(d)\n",
        "#   return n_posneg_pairs_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # plot_hyperplanes(npf_model, model_protocol_type + \" \" + model_learning_status)\n",
        "# def plot_hyperplanes(model, for_layers,model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "#   # plot_posneg_hyperplanes(model, for_layers,model_protocol_type = model_protocol_type,run = run,  epoch = epoch, step = step, for_mode = 1)\n",
        "#   # for_layers = 1#len(model.layers)-1\n",
        "#   layer_weights, layer_biases, layer_slopes, layer_intercepts = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "\n",
        "#   if for_mode == 1:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "#   elif for_mode == 21:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,-2], test_data_curr[:,-1]\n",
        "\n",
        "#   fig = plt.figure(figsize=(10*model.n_hidden_layers, 10))\n",
        "#   outer = gridspec.GridSpec(1, for_layers, wspace=0.5, hspace=0.1)\n",
        "\n",
        "#   start, stop = -6.3, 6.3\n",
        "#   x_axis = np.linspace(start = start, stop = stop, num = 2, endpoint=True)\n",
        "\n",
        "#   # f_p, axes_p = plt.subplots(1,for_layers, figsize = (8/1.5 * for_layers, 16/1.5)  )\n",
        "  \n",
        "#   for i in range(for_layers):\n",
        "\n",
        "#     inner = gridspec.GridSpecFromSubplotSpec(8, 4, subplot_spec=outer[i], wspace=0.1, hspace=0.1)\n",
        "    \n",
        "#     # debug()\n",
        "\n",
        "#     # plt.figure(figsize = (10,10))\n",
        "#     #fig, ax = plt.subplots(figsize = (10,10))\n",
        "\n",
        "\n",
        "#     # f, axes = plt.subplots(8, 4, figsize = (8/1.5,16/1.5))\n",
        "#     for r in range(8):\n",
        "#       for c in range(4):\n",
        "#         ax = plt.Subplot(fig, inner[r*4 + c])\n",
        "\n",
        "#         ax.scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1)\n",
        "#         ax.set_xlim((start,stop))\n",
        "#         ax.set_ylim((start,stop))\n",
        "#         y_axis = layer_slopes[i][r*4 + c]*x_axis + layer_intercepts[i][r*4 + c]\n",
        "\n",
        "#         #calc #+ve pts and -ve pts around epsilon dist from the hyperplane\n",
        "        \n",
        "\n",
        "\n",
        "#         #plt.plot(x_axis, y_axis)\n",
        "#         ax.plot( x_axis, y_axis, 'r')\n",
        "#         weight_vec = layer_weights[i][r*4 + c]\n",
        "       \n",
        "#         weight_vec_mag = torch.sqrt(torch.sum(torch.square(weight_vec)))\n",
        "#         origin = np.array([[0, 0, 0],[0, 0, 0]])\n",
        "#         ax.quiver( np.array([weight_vec[0]]) / weight_vec_mag ,np.array([weight_vec[1]]) / weight_vec_mag, angles='xy', scale_units='xy', scale=1, color =  ['b'], width = .01, headwidth = 12, headlength = 8)\n",
        "#         ax.set_aspect(True)\n",
        "#         # print(layer_weights[i][r*4 + c])\n",
        "#         # ax.fill_between(x_axis, y_axis, y_axis +  (weight_vec[1]/weight_vec_mag - layer_intercepts[i][r*4 + c]), alpha = .4)\n",
        "#         inter =  (weight_vec[1] / weight_vec_mag) - layer_slopes[i][r*4 + c]* (weight_vec[0] / weight_vec_mag)\n",
        "#         ax.fill_between(x_axis, y_axis,  layer_slopes[i][r*4 + c]*x_axis + inter, alpha = .4)\n",
        "\n",
        "#         # ax.fill_between(x_axis, y_axis, np.array([0, weight_vec[0]]) / weight_vec_mag, alpha = .2)\n",
        "#         fig.add_subplot(ax)\n",
        "   \n",
        "#   fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\")\n",
        "#   fig.savefig('Hyperplanes '+model_protocol_type +', ' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.png\",format = 'png', bbox_inches='tight', dpi = 100)\n",
        "  \n",
        "#   plt.cla()\n",
        "#   plt.clf()\n",
        "#   plt.close('all')\n",
        "\n",
        "#   plot_hyperplanes_all(model, for_layers,model_protocol_type = model_protocol_type,run = run,  epoch = epoch, step = step, for_mode = 1)\n",
        "  \n",
        "# def plot_hyperplanes_all(model, for_layers,model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "#   layer_weights, layer_biases, layer_slopes, layer_intercepts = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "\n",
        "#   if for_mode == 1:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "#   elif for_mode == 21:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,-2], test_data_curr[:,-1]\n",
        "\n",
        "#   x_axis = np.linspace(start = -6.3, stop = 6.3, num = 2, endpoint=True)\n",
        "#   # f_p, axes_p = plt.subplots(1,for_layers, figsize = (8/1.5 * for_layers, 16/1.5)  )\n",
        "#   start, stop = -6.3, 6.3\n",
        "#   fig, ax = plt.subplots(nrows=1, ncols=for_layers, figsize=(5*for_layers,5))\n",
        "#   for i in range(for_layers):\n",
        "#     ax[i].scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1)\n",
        "#     for r in range(8):\n",
        "#       for c in range(4):\n",
        "        \n",
        "#         ax[i].set_xlim((start,stop))\n",
        "#         ax[i].set_ylim((start,stop))\n",
        "        \n",
        "#         # ax[i].set_xlim((-1.2,1.2))\n",
        "#         # ax[i].set_ylim((-1.2,1.2))\n",
        "#         y_axis = layer_slopes[i][r*4 + c]*x_axis + layer_intercepts[i][r*4 + c]\n",
        "#         #plt.plot(x_axis, y_axis)\n",
        "#         ax[i].plot( x_axis, y_axis, 'r')\n",
        "#     # plt.show()\n",
        "#     fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\")\n",
        "#     fig.savefig('Hyperplanes_all '+model_protocol_type +', '+f'Mode={for_mode}' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.png\",format = 'png', bbox_inches='tight', dpi = 100)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRbcS_Wfm7-l"
      },
      "source": [
        "#Plotting functions for hyperplanes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRj7Gjibj_1y",
        "outputId": "4bbc3b51-04c6-4d34-ef8a-376621b7acd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ],
      "source": [
        "a = [2]\n",
        "type(a) == list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "uQrD0K9uhYtE"
      },
      "outputs": [],
      "source": [
        "def plot_hyp_total_posneg(posneg_pairs_all_runs,n_neurons, epsilon, temp_mask = None):  \n",
        "  n_parts = 1\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers #np.array(posneg_pairs_all_runs[0]).shape[1]\n",
        "\n",
        "  fig = plt.figure(figsize=(5*n_parts, n_layers*5))\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "  posneg_id = ['positive', 'negative']\n",
        "  unwraped_posneg  = unwrap_routine(posneg_pairs_all_runs)\n",
        "  subfigs = fig.subfigures(nrows=n_layers, ncols=1)\n",
        "  \n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "        subfig.suptitle(f'Layer {layer_no+1}')\n",
        "\n",
        "        axs = subfig.subplots(nrows=1, ncols = n_parts)\n",
        "      # for inner_row, inner_ax in enumerate(axs):\n",
        "        if n_parts == 1:\n",
        "          axs = [axs]\n",
        "        for i, ax in enumerate(axs):\n",
        "          # (27, 8192, 2)\n",
        "          sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "          sub_negatives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "          vicinity_mask = np.logical_and((sub_positives[0] + sub_negatives[0] )<= n_points_vicinity, (sub_positives[0] + sub_negatives[0]) <= 500)\n",
        "          ratio =  sub_positives / sub_negatives\n",
        "          ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "          mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "          intermediate = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "          intermediate += np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "          \n",
        "          if temp_mask != None:\n",
        "            \n",
        "            ax.plot(xi,  intermediate[:,temp_mask[layer_no]])\n",
        "          else:\n",
        "          # print(intermediate[:,:].shape)\n",
        "            # color = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange']\n",
        "            # for c in range(8):\n",
        "                # ax.plot(xi,  intermediate[:,c*int((8192/8)):(c+1)*int(8192/8)],  color = color[c], alpha = .5)\n",
        "\n",
        "            ax.plot(xi,  intermediate[:,:])\n",
        "          ax.set_xticks(xi,x_axis_global)\n",
        "          plt.setp(ax.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "          ax.set_title(f\"# of {posneg_id[0]} pts,{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "        \n",
        "        plt.gca().set_prop_cycle(None)\n",
        "\n",
        "  title = f'epsilon = {epsilon}, n_vicinity_pts = {n_points_vicinity}, Ratio criteria: pos/neg = {pos_neg_ratio_criteria}, neg/pos = {1/pos_neg_ratio_criteria} '\n",
        "  fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_total_posneg_epsilon = {epsilon}.png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        "def plot_hyp_posneg(posneg_pairs_all_runs,n_neurons,  epsilon, temp_mask = None):  \n",
        "  n_parts = 4\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers #np.array(posneg_pairs_all_runs[0]).shape[1]\n",
        "\n",
        "  fig = plt.figure(figsize=(10*n_parts, n_layers*10))\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "  posneg_id = ['positive', 'negative']\n",
        "  unwraped_posneg  = unwrap_routine(posneg_pairs_all_runs)\n",
        "  subfigs = fig.subfigures(nrows=n_layers, ncols=1)\n",
        " \n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "      subfig.suptitle(f'Layer {layer_no+1}')\n",
        "\n",
        "      axs = subfig.subplots(nrows=2, ncols=4)\n",
        "      for inner_row, inner_ax in enumerate(axs):\n",
        "        for i, ax in enumerate(inner_ax):\n",
        "          # (27, 8192, 2)\n",
        "          sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "          sub_negatives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "          vicinity_mask = sub_positives[0] + sub_negatives[0] >= n_points_vicinity\n",
        "          ratio =  sub_positives / sub_negatives\n",
        "          ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "          \n",
        "          mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "          intermediate = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),inner_row]\n",
        "          # ax.plot(xi,  intermediate[:,mask])\n",
        "          # print(intermediate[:,:].shape)\n",
        "          ax.plot(xi,  intermediate[:,:])\n",
        "          ax.set_xticks(xi,x_axis_global)\n",
        "          plt.setp(ax.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "          ax.set_title(f\"# of {posneg_id[inner_row]} pts,{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "        \n",
        "        plt.gca().set_prop_cycle(None)\n",
        "\n",
        "  title = f'epsilon = {epsilon}, n_vicinity_pts = {n_points_vicinity}, Ratio criteria: pos/neg = {pos_neg_ratio_criteria}, neg/pos = {1/pos_neg_ratio_criteria} '\n",
        "  fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_posneg_epsilon = {epsilon}.png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        " \n",
        "  plot_hyp_total_posneg(posneg_pairs_all_runs,n_neurons_list, epsilon, temp_mask)\n",
        "def plot_hyp_posneg_ratio(posneg_pairs_all_runs,n_neurons, epsilon):  \n",
        "  n_parts = 1\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "\n",
        "  fig = plt.figure(figsize=(10*n_parts, n_layers*5))\n",
        "\n",
        "  posneg_id = ['positive', 'negative']\n",
        "  unwraped_posneg  = unwrap_routine(posneg_pairs_all_runs)\n",
        "  i = 0\n",
        "  # create 3x1 subfigs\n",
        "  subfigs = fig.subfigures(nrows=n_layers, ncols=1)\n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "      subfig.suptitle(f'Layer {layer_no+1}')\n",
        "      axs = subfig.subplots(nrows=1, ncols=1)\n",
        "      sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "      sub_negatives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "      vicinity_mask = sub_positives[0] + sub_negatives[0] >= n_points_vicinity\n",
        "      # print(vicinity_mask.shape)\n",
        "      ratio =  sub_positives / sub_negatives\n",
        "      ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "      mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "      # axs.plot(xi, np.vstack([ratio[0,mask],ratio[:,mask]]))   #shifting 0 bcoz of log scale\n",
        "      # axs.plot(xi,ratio[:,mask])\n",
        "      axs.plot(xi,ratio[:,:])\n",
        "      axs.set_xticks(xi,x_axis_global)\n",
        "      plt.setp(axs.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "      # axs.set_xscale(\"log\")\n",
        "      # axs.set_yscale(\"symlog\")\n",
        "      axs.set_adjustable(\"datalim\")\n",
        "      axs.set_xlabel(\"epochs\")\n",
        "      axs.set_ylabel(\"#pos / #neg \")\n",
        "      axs.set_title(f\"{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "\n",
        "  title = f'epsilon = {epsilon}, n_vicinity_pts = {n_points_vicinity}, Ratio criteria: pos/neg = {pos_neg_ratio_criteria}, neg/pos = {1/pos_neg_ratio_criteria} '\n",
        "  fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_posneg_ratio_epsilon = {epsilon}.png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        "\n",
        "def plot_hyp_local_usefulness(local_usefulness_all_runs,n_neurons, epsilon):  \n",
        "  n_parts = 1\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "  fig = plt.figure(figsize=( n_layers*9,7))\n",
        "  unwraped_posneg  = unwrap_routine(local_usefulness_all_runs)\n",
        "  i = 0\n",
        "  subfigs = fig.subfigures(nrows=1, ncols= n_layers)\n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  temp_mask = []\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "      subfig.suptitle(f'Layer {layer_no+1}')\n",
        "      axs = subfig.subplots(nrows=1, ncols=1)\n",
        "      sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts)]\n",
        "      \n",
        "      \n",
        "      # vicinity_mask = sub_positives[-1, :] >= .4\n",
        "      # temp_mask.append(vicinity_mask)\n",
        "      # sub_positives = sub_positives[:, vicinity_mask]\n",
        "      # ratio =  sub_positives / sub_negatives\n",
        "      # ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "      # mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "      # axs.semilogx([0] + x_axis_global, np.vstack([sub_positives[0,:], sub_positives]))   #shifting 0 bcoz of log scale\n",
        "\n",
        "      # color = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange']\n",
        "      # for c in range(8):\n",
        "      #   axs.plot(xi,  sub_positives[:,c*int((8192/8)):(c+1)*int(8192/8)],  color = color[c], alpha = .5)\n",
        "      axs.plot(xi,  sub_positives)\n",
        "\n",
        "      # axs.set_xscale(\"log\")\n",
        "      # axs.set_yscale(\"log\")\n",
        "      axs.set_xticks(xi,x_axis_global)\n",
        "\n",
        "      axs.set_adjustable(\"datalim\")\n",
        "      plt.setp(axs.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "      axs.set_xlabel(\"epochs\")\n",
        "      axs.set_ylabel(\"local usefulness\")\n",
        "      # axs.set_title(f\"{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "\n",
        "  title = f'epsilon = {epsilon} '\n",
        "  # fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_posneg_local_usefulness = {epsilon}.pdf',format = 'pdf', bbox_inches='tight', dpi = 100)\n",
        "\n",
        "  return temp_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {
        "id": "DBQqrPIPfzOJ"
      },
      "outputs": [],
      "source": [
        "def plot_hyperplanes(weights_biases_all_runs, state_info_5_runs, n_rows, n_cols):\n",
        "  w_idx, b_idx = 0, 1\n",
        "  pseudo_step = -1\n",
        "  first_last = [0,3, -1]\n",
        "  for pseudo_run in range(len(weights_biases_all_runs)):\n",
        "    # for pseudo_epoch in range(len(weights_biases_all_runs[pseudo_run])):\n",
        "    for pseudo_epoch in first_last:\n",
        "      \n",
        "      layer_weights = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][w_idx]\n",
        "      layer_biases = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][b_idx]\n",
        "      \n",
        "      inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "  \n",
        "      fig = plt.figure(figsize = (n_hidden_layers*n_cols*1.5, n_rows*1.5))\n",
        "      outer = gridspec.GridSpec(1, n_hidden_layers, wspace=0.2, hspace=0.1, figure = fig)\n",
        "\n",
        "      # start, stop = -1.2, 1.2\n",
        "      pad = .5\n",
        "      start_x, stop_x = np.min(train_data_curr[:,0])-pad, np.max(train_data_curr[:,0])+pad\n",
        "      start_y, stop_y = np.min(train_data_curr[:,1])-pad, np.max(train_data_curr[:,1])+pad\n",
        "      x_axis = np.linspace(start = start_x, stop = stop_x, num = 2, endpoint=True)\n",
        "\n",
        "      # f_p, axes_p = plt.subplots(1,for_layers, figsize = (8/1.5 * for_layers, 16/1.5)  )\n",
        "      # n_rows, n_cols = 8, 2\n",
        "      for layer_no in range(n_hidden_layers):\n",
        "        inner = gridspec.GridSpecFromSubplotSpec(n_rows, n_cols, subplot_spec=outer[layer_no], wspace=0.1, hspace=0.3)\n",
        "        \n",
        "        ax = plt.Subplot(fig, outer[layer_no])\n",
        "        ax.set_title(\"Layer {}\".format(layer_no+1), fontsize = 18)\n",
        "        ax.axis('off')\n",
        "        fig.add_subplot(ax)\n",
        "        for r in range(n_rows):\n",
        "          for c in range(n_cols):\n",
        "            # ax = plt.Subplot(fig, inner[r*n_cols + c])\n",
        "            ax = fig.add_subplot(inner[r*n_cols + c])\n",
        "           \n",
        "            ax.scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1, alpha = .3)\n",
        "            ax.set_xlim((start_x,stop_x))\n",
        "            # ax.set_ylim((-1.2,4.2))\n",
        "            ax.set_ylim((start_y,stop_y))\n",
        "            layer_slope = -(layer_weights[layer_no][r*n_cols + c,0]/layer_weights[layer_no][r*n_cols + c,1])\n",
        "            layer_intercept = -(layer_biases[layer_no][r*n_cols + c] / layer_weights[layer_no][r*n_cols + c,1])\n",
        "\n",
        "            layer_slope = layer_slope.detach().to('cpu').numpy()\n",
        "            layer_intercept = layer_intercept.detach().to('cpu').numpy()\n",
        "            y_axis = layer_slope*x_axis + layer_intercept\n",
        "\n",
        "            #calc #+ve pts and -ve pts around epsilon dist from the hyperplane\n",
        "            ax.plot( x_axis, y_axis, 'r')\n",
        "            weight_vec = layer_weights[layer_no][r*n_cols + c].detach().to('cpu').numpy()\n",
        "            weight_vec_mag = np.sqrt(np.sum(np.square(weight_vec)))\n",
        "            origin = np.array([[0, 0, 0],[0, 0, 0]])\n",
        "            ax.quiver( np.array([weight_vec[0]]) / weight_vec_mag ,np.array([weight_vec[1]]) / weight_vec_mag, angles='xy', scale_units='xy', scale=1, color =  ['b'], width = .01, headwidth = 12, headlength = 8)\n",
        "            # ax.quiver( 3*np.array([weight_vec[0]]) / weight_vec_mag ,3*np.array([weight_vec[1]]) / weight_vec_mag, angles='xy', scale_units='xy', scale=1, color =  ['b'], width = .01, headwidth = 12, headlength = 8)\n",
        "            ax.set_aspect(True)\n",
        "            # print(layer_weights[i][r*4 + c])\n",
        "            # ax.fill_between(x_axis, y_axis, y_axis +  (weight_vec[1]/weight_vec_mag - layer_intercepts[i][r*4 + c]), alpha = .4)\n",
        "            inter =  (10*weight_vec[1] / weight_vec_mag) - layer_slope* (10*weight_vec[0] / weight_vec_mag)\n",
        "            ax.fill_between(x_axis, y_axis,  layer_slope*x_axis + inter, alpha = .4)\n",
        "            # ax.grid()\n",
        "            if c >= 1:\n",
        "              ax.set_yticks([])\n",
        "            # ax.fill_between(x_axis, y_axis, np.array([0, weight_vec[0]]) / weight_vec_mag, alpha = .2)\n",
        "            # fig.add_subplot(ax)\n",
        "      \n",
        "      run = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"run\"]\n",
        "      epoch = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"epoch\"]\n",
        "      step = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"step\"]\n",
        "      model_protocol_type = state_info_5_runs[0][0][0][\"model_protocol_type\"]\n",
        "      # fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\", fontsize = 8)\n",
        "      fig.savefig('Hyperplanes '+model_protocol_type +', ' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.pdf\",format = 'pdf', bbox_inches='tight', dpi = 100)\n",
        "\n",
        "      plt.cla()\n",
        "      plt.clf()\n",
        "      plt.close('all')\n",
        "\n",
        "\n",
        "def plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs, n_rows, n_cols):\n",
        "  w_idx, b_idx = 0, 1\n",
        "  pseudo_step = -1\n",
        "  first_last = [0,3, -1]\n",
        "  for pseudo_run in range(len(weights_biases_all_runs)):\n",
        "    # for pseudo_epoch in range(len(weights_biases_all_runs[pseudo_run])):\n",
        "    for pseudo_epoch in first_last:\n",
        "      layer_weights = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][w_idx]\n",
        "      layer_biases = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][b_idx]\n",
        "      \n",
        "          \n",
        "      inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "      pad = 2\n",
        "      start_x, stop_x = np.min(train_data_curr[:,0])-pad, np.max(train_data_curr[:,0])+pad\n",
        "      start_y, stop_y = np.min(train_data_curr[:,1])-pad, np.max(train_data_curr[:,1])+pad\n",
        "      x_axis = np.linspace(start = start_x, stop = stop_x, num = 2, endpoint=True)\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=n_hidden_layers, figsize=(5*n_hidden_layers,5))\n",
        "\n",
        "      for layer_no in range(n_hidden_layers):\n",
        "        ax[layer_no].scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1, alpha = .8)\n",
        "        ax[layer_no].set_title(f\"Layer {layer_no+1}\")\n",
        "        ax[layer_no].grid()\n",
        "        for r in range(n_rows):\n",
        "          for c in range(n_cols):\n",
        "            ax[layer_no].set_xlim((start_x,stop_x))\n",
        "            ax[layer_no].set_ylim((start_y,stop_y))\n",
        "            # print(r*n_cols + c, r*n_cols + c)\n",
        "            layer_slope = -(layer_weights[layer_no][r*n_cols + c,0]/layer_weights[layer_no][r*n_cols + c,1])\n",
        "            layer_intercept = -(layer_biases[layer_no][r*n_cols + c] / layer_weights[layer_no][r*n_cols + c,1])\n",
        "            layer_slope = layer_slope.detach().to('cpu').numpy()\n",
        "            layer_intercept = layer_intercept.detach().to('cpu').numpy()\n",
        "            y_axis = layer_slope*x_axis + layer_intercept\n",
        "            ax[layer_no].plot( x_axis, y_axis, 'r')\n",
        "            # ax[layer_no].grid()\n",
        "\n",
        "           \n",
        "            \n",
        "      # print(r, e, s)\n",
        "        run = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"run\"]\n",
        "        epoch = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"epoch\"]\n",
        "        step = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"step\"]\n",
        "        model_protocol_type = state_info_5_runs[0][0][0][\"model_protocol_type\"]\n",
        "        # fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\")\n",
        "        fig.savefig('Hyperplanes_all '+model_protocol_type +', ' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.pdf\",format = 'pdf', bbox_inches='tight', dpi = 100)\n",
        "    \n",
        "      plt.cla()\n",
        "      plt.clf()\n",
        "      plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "gFL83-XRRaLd"
      },
      "outputs": [],
      "source": [
        "# plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs)\n",
        "# plot_hyperplanes(weights_biases_all_runs, state_info_5_runs, n_rows = 8, n_cols = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbqVgGWLnG1X"
      },
      "source": [
        "#Saving and downloading funtions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "bMcSnqDY2dD4"
      },
      "outputs": [],
      "source": [
        "def save_results(file_name):\n",
        "  os.system(f\"zip -R {file_name}.zip '*.png' '*.pdf'\")\n",
        "  os.system('mkdir All_Protocols')\n",
        "  os.system(f'mv {file_name}.zip /content/All_Protocols/')\n",
        "  os.system(\"rm *\")\n",
        "\n",
        "def download_results():\n",
        "  path = '/content/All_Protocols/'\n",
        "  for f in os.listdir(path):\n",
        "    files.download(f'{path}{f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "9d7Nbr4dDTe1"
      },
      "outputs": [],
      "source": [
        "# def get_input_idxs(Y_sorted):\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   low_freq_idxs = sorted(rng.choice(np.arange(0, int(len(Y_sorted)/2)),10, replace = False))\n",
        "#   high_freq_idxs = sorted(rng.choice(np.arange(int(len(Y_sorted)/2), len(Y_sorted)),10, replace = False))\n",
        "#   x_idxs = low_freq_idxs + high_freq_idxs\n",
        "#   x_idxs_pair = [(idx, idx) for idx in x_idxs]\n",
        "#   return x_idxs_pair\n",
        "\n",
        "# def get_input_idxs(size):\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   low_freq_idxs = sorted(rng.choice(np.arange(0,int(size/num_modes)*20),10, replace = False))\n",
        "#   high_freq_idxs = sorted(rng.choice(np.arange(20*int(size/num_modes), num_modes*int(size/num_modes)),10, replace = False))\n",
        "#   x_idxs = low_freq_idxs + high_freq_idxs\n",
        "#   x_idxs_pair = [(idx, idx) for idx in x_idxs]\n",
        "#   return x_idxs_pair\n",
        "\n",
        "def get_input_idxs(size):\n",
        "  rng = np.random.default_rng(0)\n",
        "  low_freq_idxs = sorted(rng.choice(np.arange(0,250),50, replace = False))\n",
        "  high_freq_idxs = sorted(rng.choice(np.arange(250, 500),50, replace = False))\n",
        "  x_idxs = low_freq_idxs + high_freq_idxs\n",
        "  x_idxs_pair = [(idx, idx) for idx in x_idxs]\n",
        "  return x_idxs_pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REhPzmLCncJx"
      },
      "source": [
        "#Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {
        "id": "GEzA8qRI1w5j"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"binary_cifar_dataset\"\n",
        "#########NOTE:change start stop axis for hyperplane figs############\n",
        "dataset_func, dataset_args = None, None\n",
        "is_DNN = True\n",
        "is_classification = True\n",
        "if dataset_name == 'parabola_dataset':\n",
        "  dataset_func = parabola_dataset\n",
        "elif dataset_name == 'L1_dataset':\n",
        "  dataset_func = L1_dataset\n",
        "elif dataset_name == 'boxed_dataset':\n",
        "  dataset_func = boxed_dataset\n",
        "elif dataset_name == 'boxed_complex_dataset':\n",
        "  dataset_func = boxed_complex_dataset\n",
        "elif dataset_name == 'binary_mnist_dataset':\n",
        "  dataset_func = binary_mnist_dataset\n",
        "elif dataset_name == 'binary_cifar_dataset':\n",
        "  dataset_func = binary_cifar_dataset\n",
        "  is_DNN = False\n",
        "  dataset_args = {'flattened' : False, 'is_binary' : False}\n",
        "elif dataset_name == 'union_dataset':\n",
        "  dataset_func = union_dataset\n",
        "elif dataset_name == 'circle_dataset':\n",
        "  dataset_func = circle_dataset\n",
        "  is_classification = False\n",
        "  dataset_args = {'n_data' : 500,'a' : 1, 'b' : 9, 'plot' : True }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "686a44e2edd148e596f3d2836bd5b88a",
            "95c8d1ea770c4a8a9578cc3a15a2ec84",
            "197e23508eef4c37a67229d0e81edb3b",
            "2357a0131edb45a2aa4a05564695f737",
            "78cc0b5462674ec9aabab915143155f9",
            "efaea7d7c227497a903f3d7463816c99",
            "9ba16719fd0744818bab3f00697e23ef",
            "ecba25841d9a4671805468c875ce058c",
            "7eb5c4d2627a4385b3da33dad19e0ae5",
            "9859e14274d64910a150cc5599f1572d",
            "1e186330d1414d789532c5f834a0beca"
          ]
        },
        "id": "Q2lsXg6s2saU",
        "outputId": "54da4623-9530-4aae-f0b7-dde79d4b0520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "686a44e2edd148e596f3d2836bd5b88a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /CIFAR10/cifar-10-python.tar.gz to /CIFAR10\n",
            "Files already downloaded and verified\n",
            "(50000, 3, 32, 32) (50000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1563"
            ]
          },
          "metadata": {},
          "execution_count": 422
        }
      ],
      "source": [
        "seed = 1020 #1010 for all\n",
        "set_seed(seed = seed)\n",
        "\n",
        "\n",
        "train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels = dataset_func(dataset_args)\n",
        "n_data, inp_dim, n_classes = train_data_curr.shape[0], train_data_curr.shape[1], len(np.unique(train_labels_curr))\n",
        "\n",
        "# mini = get_mini_multi_modes_data(test_data_curr,num_modes = 6, n_examples_per_mode = 10)\n",
        "\n",
        "train_dataloader, test_dataloader, train_dl_PWC, test_dl_PWC = get_dataloaders(train_data_curr, test_data_curr, train_labels_curr, test_labels_curr, batch_size = 32)\n",
        "mini_dl = get_mini_multi_mode_dataloaders(mini_test_data, batch_size = len(mini_test_data))\n",
        "x_idxs_pair = get_input_idxs(len(mini_test_data))\n",
        "n_batches = len(train_dataloader)\n",
        "n_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u95Ct6WJoL_e"
      },
      "source": [
        "#CNN model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "dDdyDeiDkgbl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def compute_pred_gradient(dl_one,label, model,act_type, optimizer):\n",
        "  all_per_sample_gradients = []\n",
        "\n",
        "  for s in range(dl_one.size()[0]):\n",
        "    \n",
        "    x, y = dl_one[s], label[s]\n",
        "    X, y = x.to(device), y.to(device)\n",
        "    \n",
        "    X = torch.unsqueeze(X, axis = 0)\n",
        "    \n",
        "    pred = model(X.float())\n",
        "    class_grad = []\n",
        "    \n",
        "    # optimizer.zero_grad()\n",
        "    for i in range(pred.size()[-1]):\n",
        "      \n",
        "      pred[:,i].backward(retain_graph=True)\n",
        "      para = []\n",
        "      for p in model.parameters():\n",
        "        if p.grad != None:\n",
        "          para.append(p.grad.flatten().detach().clone())\n",
        "\n",
        "      # for p in para:\n",
        "      #   print(p.size())\n",
        "      \n",
        "      class_grad.append(torch.hstack(para))\n",
        "    \n",
        "    per_sample_gradients = torch.vstack(class_grad)\n",
        "\n",
        "      \n",
        "  \n",
        "    all_per_sample_gradients.append(per_sample_gradients)\n",
        "    optimizer.zero_grad()\n",
        "  all_per_sample_gradients = torch.stack(all_per_sample_gradients)\n",
        "  \n",
        "  return all_per_sample_gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "-Wxnn4oYAJ66"
      },
      "outputs": [],
      "source": [
        "class Linear_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.out_dim = 10\n",
        "        for i in range(self.out_dim):\n",
        "          self.layers.append(nn.Linear(511362, 1))\n",
        "          # self.layers.append(nn.Linear(28354, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = []\n",
        "      # print(x.size())\n",
        "      \n",
        "      for i in range(self.out_dim):\n",
        "        \n",
        "        out.append( self.layers[i](x[:,i,:]))\n",
        "\n",
        "      return torch.hstack(out)\n",
        "linear_classifier = Linear_Classifier()\n",
        "linear_classifier.to(device)\n",
        "\n",
        "linear_criterion = nn.CrossEntropyLoss()\n",
        "linear_optimizer = optim.Adam(linear_classifier.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "FOjn1meMPZzd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# train_acc, test_acc = [], []\n",
        "# for epoch in range(1000):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "        \n",
        "#         data_grad = compute_pred_gradient(dl_one = data[0],label = data[1], model = net,act_type = None, optimizer = optimizer)\n",
        "       \n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         # inputs, labels = data\n",
        "#         inputs, labels = data_grad, data[1].to(device)\n",
        "#         # zero the parameter gradients\n",
        "#         linear_optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = linear_classifier(inputs)\n",
        "        \n",
        "            \n",
        "        \n",
        "#         loss = linear_criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         linear_optimizer.step()\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "        \n",
        "#         if i+1 % 100 == 0:    # print every 2000 mini-batches\n",
        "#           print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "#           running_loss = 0.0\n",
        "#     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} ')\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "  \n",
        "# #     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "# #     with torch.no_grad():\n",
        "# #         for data in test_dataloader:\n",
        "# #             images, labels = data\n",
        "# #             # calculate outputs by running images through the network\n",
        "# #             images, labels = images.to(device), labels.to(device)\n",
        "# #             outputs = linear_classifier(images)\n",
        "# #             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "# #             # debug()\n",
        "# #             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "# #             total += labels.size(0)\n",
        "# #             correct += (predicted == labels).sum().item()\n",
        "# #     test_acc.append(100 * correct / total)\n",
        "# #     print(f'Accuracy of the network on the {total} test images: {100 * correct / total} ')\n",
        "# #     correct = 0\n",
        "# #     total = 0\n",
        "    \n",
        "# #     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "# #     with torch.no_grad():\n",
        "# #         for data in train_dataloader:\n",
        "# #             images, labels = data\n",
        "# #             # calculate outputs by running images through the network\n",
        "# #             images, labels = images.to(device), labels.to(device)\n",
        "# #             outputs = linear_classifier(images)\n",
        "# #             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "# #             # debug()\n",
        "# #             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "# #             total += labels.size(0)\n",
        "# #             correct += (predicted == labels).sum().item()\n",
        "# #     train_acc.append(100 * correct / total)\n",
        "# #     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} %')\n",
        "    \n",
        "# # print('Finished Training')\n",
        "# # print(max(train_acc), max(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "U7q3TVstj3qV"
      },
      "outputs": [],
      "source": [
        "# torch.save({\n",
        "#             'modelA_state_dict': linear_classifier.state_dict(),\n",
        "#             'modelB_state_dict': net.state_dict(),\n",
        "#             'optimizerA_state_dict': linear_optimizer.state_dict(),\n",
        "#             'optimizerB_state_dict': optimizer.state_dict(),\n",
        "#             'running_loss' : running_loss,\n",
        "#             'acc' : 100 * correct / total\n",
        "#             }, 'NTK_regression.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "dvN6leCHPACx"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load('NTK_regression.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "ZwhTWkwBCJs5"
      },
      "outputs": [],
      "source": [
        "# linear_classifier.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "# # linear_optimizer.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "# net.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizerB_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "tRlFr1Rqs21F"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AvgPool2d(32, 32)\n",
        "        self.n_filters = 40\n",
        "        self.conv1 = nn.Conv2d(3, self.n_filters, 5, padding = 'same')\n",
        "        self.conv2 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "        self.conv3 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "        self.conv4 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "        # self.conv1 = nn.Linear(784, 32)\n",
        "        # self.conv2 =  nn.Linear(32, 32)\n",
        "        # self.conv3 =  nn.Linear(32, 32)\n",
        "        # self.conv4 =  nn.Linear(32, 32)\n",
        "        self.fc1 = nn.Linear(40, 32)\n",
        "        # self.fc2 = nn.Linear(64, 2)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        # self.pool = nn.MaxPool2d(2, 2)\n",
        "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "        # self.fc2 = nn.Linear(120, 84)\n",
        "        # self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.pool(F.relu(self.conv1(x)))\n",
        "        # x = self.pool(F.relu(self.conv2(x)))\n",
        "        # x = self.pool(F.relu(self.conv3(x)))\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # # x = F.relu(self.fc2(x))\n",
        "        # x = self.fc3(x)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        \n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # x = self.pool(self.conv1(x))\n",
        "        # x = self.pool(self.conv2(x))\n",
        "        # x = self.pool(self.conv3(x))\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        # x = self.fc1(x)\n",
        "        # # x = F.relu(self.fc2(x))\n",
        "        # x = self.fc3(x)\n",
        "\n",
        "        # x = self.conv1(x)\n",
        "        # x = self.conv2(x)\n",
        "        # x = self.conv3(x)\n",
        "        # x = self.pool(x)\n",
        "        # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        # x = self.fc1(x)\n",
        "        # # x = self.fc2(x)\n",
        "        # x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gylTOEYu_wn",
        "outputId": "b833851b-b573-4a79-f05b-c60c955afece"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 13 12:19:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    47W / 400W |   1374MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# train_acc, test_acc = [], []\n",
        "# for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "               \n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         # inputs, labels = data\n",
        "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "        \n",
        "            \n",
        "        \n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "        \n",
        "#         if i+1 % 100 == 0:    # print every 2000 mini-batches\n",
        "#           print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "#           running_loss = 0.0\n",
        "#     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} ')\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "  \n",
        "#     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "#     with torch.no_grad():\n",
        "#         for data in test_dataloader:\n",
        "#             images, labels = data[0], data[1]\n",
        "#             # calculate outputs by running images through the network\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = net(images)\n",
        "#             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "#             # debug()\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#     test_acc.append(100 * correct / total)\n",
        "#     print(f'Accuracy of the network on the {total} test images: {100 * correct / total} ')\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "    \n",
        "# #     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "# #     with torch.no_grad():\n",
        "# #         for data in train_dataloader:\n",
        "# #             images, labels = data\n",
        "# #             # calculate outputs by running images through the network\n",
        "# #             images, labels = images.to(device), labels.to(device)\n",
        "# #             outputs = linear_classifier(images)\n",
        "# #             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "# #             # debug()\n",
        "# #             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "# #             total += labels.size(0)\n",
        "# #             correct += (predicted == labels).sum().item()\n",
        "# #     train_acc.append(100 * correct / total)\n",
        "# #     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} %')\n",
        "    \n",
        "# # print('Finished Training')\n",
        "# # print(max(train_acc), max(test_acc))\n"
      ],
      "metadata": {
        "id": "4roFN9PahFXR"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XMjOfpnusU_",
        "outputId": "8490d412-a054-44bd-e6a3-834bada5b297"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 13 12:19:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    46W / 400W |   1374MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHVRuUJEnp0c"
      },
      "source": [
        "#Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XvUBVBUvob2Y",
        "outputId": "88f857f9-b96b-4074-8c2e-8a7af8e3a0ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMLP, DLGN, DLGN_ONPV, DLGN_BOTH_ONPV\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 314
        }
      ],
      "source": [
        "'''\n",
        "1. lr = 5e-3 for boxed and circle data set\n",
        "2. build_kernels(...): \n",
        "    a. Now returns only overlap matrix\n",
        "    b. Normalized overlap matrix by its trace\n",
        "3. plot_kernels(...): function changed to plot kernels at only critical point \n",
        "4. beta changedto 20\n",
        "5. run_npf_npv(...): changed jump in 100 to 200 epochs to 10 \n",
        "6. To save memory while running CIFAR 10\n",
        "    a. plot_hypposneg_(..) returns None\n",
        "    b. routine(..) funtion does not store in container\n",
        "\n",
        "#---Hyper params------\n",
        "\n",
        "\n",
        "Cirle Dataset:\n",
        "lr = 5e-3\n",
        "batch_size = 32\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 16\n",
        "\n",
        "Boxed Dataset:\n",
        "lr = 5e-4\n",
        "batch_size = 32\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 16\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "MLP, DLGN, DLGN_ONPV, DLGN_BOTH_ONPV\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r *"
      ],
      "metadata": {
        "id": "m3mzlqMBSMBo"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "DuS_Dxsje6fC"
      },
      "outputs": [],
      "source": [
        "state_collections = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsX46dmH3Utm",
        "outputId": "5393b4ff-ff70-4883-f513-a436bc10e1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOTH\n",
            "Loss over all Test data : 2.306448 \n",
            "\n",
            "Loss over all Training data : 2.306449 \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss over all Test data : 2.201569 \n",
            "\n",
            "Loss over all Training data : 2.203575 \n",
            "\n",
            "Loss over all Test data : 2.246145 \n",
            "\n",
            "Loss over all Training data : 2.245615 \n",
            "\n",
            "Loss over all Test data : 2.155287 \n",
            "\n",
            "Loss over all Training data : 2.157915 \n",
            "\n",
            "Loss over all Test data : 2.046087 \n",
            "\n",
            "Loss over all Training data : 2.050853 \n",
            "\n",
            "Loss over all Test data : 2.020274 \n",
            "\n",
            "Loss over all Training data : 2.021715 \n",
            "\n",
            "Loss over all Test data : 1.971428 \n",
            "\n",
            "Loss over all Training data : 1.982098 \n",
            "\n",
            "Loss over all Test data : 1.937392 \n",
            "\n",
            "Loss over all Training data : 1.939259 \n",
            "\n",
            "Loss over all Test data : 1.909585 \n",
            "\n",
            "Loss over all Training data : 1.915972 \n",
            "\n",
            "Loss over all Test data : 1.786974 \n",
            "\n",
            "Loss over all Training data : 1.788787 \n",
            "\n",
            "Loss over all Test data : 1.772672 \n",
            "\n",
            "Loss over all Training data : 1.776116 \n",
            "\n",
            "Loss over all Test data : 1.794248 \n",
            "\n",
            "Loss over all Training data : 1.791341 \n",
            "\n",
            "Loss over all Test data : 1.752218 \n",
            "\n",
            "Loss over all Training data : 1.749926 \n",
            "\n",
            "Loss over all Test data : 1.743633 \n",
            "\n",
            "Loss over all Training data : 1.743902 \n",
            "\n",
            "Loss over all Test data : 1.762674 \n",
            "\n",
            "Loss over all Training data : 1.770883 \n",
            "\n",
            "Loss over all Test data : 1.685396 \n",
            "\n",
            "Loss over all Training data : 1.694972 \n",
            "\n",
            "End of Epoch 00001Train : \n",
            " Accuracy = 25.0, Loss =  0.069085\n",
            "End of Epoch 00001Test : \n",
            " Accuracy = 10.0, Loss =  2.306448\n",
            "Loss over all Training data : 1.680024 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Loss over all Test data : 1.641750 \n",
            "\n",
            "Loss over all Training data : 1.648023 \n",
            "\n",
            "Loss over all Test data : 1.725850 \n",
            "\n",
            "Loss over all Training data : 1.731224 \n",
            "\n",
            "Loss over all Test data : 1.676114 \n",
            "\n",
            "Loss over all Training data : 1.674813 \n",
            "\n",
            "Loss over all Test data : 1.624419 \n",
            "\n",
            "Loss over all Training data : 1.619024 \n",
            "\n",
            "Loss over all Test data : 1.611428 \n",
            "\n",
            "Loss over all Training data : 1.623152 \n",
            "\n",
            "Loss over all Test data : 1.623606 \n",
            "\n",
            "Loss over all Training data : 1.621786 \n",
            "\n",
            "Loss over all Test data : 1.744053 \n",
            "\n",
            "Loss over all Training data : 1.742412 \n",
            "\n",
            "Loss over all Test data : 1.662999 \n",
            "\n",
            "Loss over all Training data : 1.667858 \n",
            "\n",
            "Loss over all Test data : 1.660115 \n",
            "\n",
            "Loss over all Training data : 1.662386 \n",
            "\n",
            "Loss over all Test data : 1.602046 \n",
            "\n",
            "Loss over all Training data : 1.601608 \n",
            "\n",
            "Loss over all Test data : 1.590582 \n",
            "\n",
            "Loss over all Training data : 1.594902 \n",
            "\n",
            "Loss over all Test data : 1.634800 \n",
            "\n",
            "Loss over all Training data : 1.636810 \n",
            "\n",
            "Loss over all Test data : 1.580355 \n",
            "\n",
            "Loss over all Training data : 1.583606 \n",
            "\n",
            "Loss over all Test data : 1.590338 \n",
            "\n",
            "Loss over all Training data : 1.591345 \n",
            "\n",
            "Loss over all Test data : 1.592041 \n",
            "\n",
            "Loss over all Training data : 1.584060 \n",
            "\n",
            "End of Epoch 00002Train : \n",
            " Accuracy = 37.0, Loss =  0.065774\n",
            "End of Epoch 00002Test : \n",
            " Accuracy = 10.0, Loss =  2.306448\n",
            "Loss over all Training data : 1.589899 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "End of Epoch 3Train : \n",
            " Accuracy = 42.6, Loss =  1.542290\n",
            "Loss over all Test data : 1.515924 \n",
            "\n",
            "Loss over all Training data : 1.521406 \n",
            "\n",
            "End of Epoch 00003Test : \n",
            " Accuracy = 43.8, Loss =  1.515924\n",
            "Loss over all Training data : 1.521436 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "End of Epoch 4Train : \n",
            " Accuracy = 46.8, Loss =  1.454102\n",
            "Loss over all Test data : 1.387887 \n",
            "\n",
            "Loss over all Training data : 1.377913 \n",
            "\n",
            "End of Epoch 00004Test : \n",
            " Accuracy = 48.0, Loss =  1.387887\n",
            "Loss over all Training data : 1.377791 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "End of Epoch 5Train : \n",
            " Accuracy = 49.8, Loss =  1.380370\n",
            "Loss over all Test data : 1.406409 \n",
            "\n",
            "Loss over all Training data : 1.397302 \n",
            "\n",
            "End of Epoch 00005Test : \n",
            " Accuracy = 48.8, Loss =  1.406409\n",
            "Loss over all Training data : 1.397544 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "End of Epoch 6Train : \n",
            " Accuracy = 51.7, Loss =  1.327448\n",
            "Loss over all Test data : 1.331064 \n",
            "\n",
            "Loss over all Training data : 1.323742 \n",
            "\n",
            "End of Epoch 00006Test : \n",
            " Accuracy = 52.3, Loss =  1.331064\n",
            "Loss over all Training data : 1.323758 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "End of Epoch 7Train : \n",
            " Accuracy = 53.6, Loss =  1.282082\n",
            "Loss over all Test data : 1.241923 \n",
            "\n",
            "Loss over all Training data : 1.216631 \n",
            "\n",
            "End of Epoch 00007Test : \n",
            " Accuracy = 55.4, Loss =  1.241923\n",
            "Loss over all Training data : 1.216459 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "End of Epoch 8Train : \n",
            " Accuracy = 55.1, Loss =  1.247245\n",
            "Loss over all Test data : 1.240335 \n",
            "\n",
            "Loss over all Training data : 1.209150 \n",
            "\n",
            "End of Epoch 00008Test : \n",
            " Accuracy = 55.7, Loss =  1.240335\n",
            "Loss over all Training data : 1.208999 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "End of Epoch 9Train : \n",
            " Accuracy = 56.0, Loss =  1.221274\n",
            "Loss over all Test data : 1.215271 \n",
            "\n",
            "Loss over all Training data : 1.181762 \n",
            "\n",
            "End of Epoch 00009Test : \n",
            " Accuracy = 56.4, Loss =  1.215271\n",
            "Loss over all Training data : 1.181798 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "End of Epoch 10Train : \n",
            " Accuracy = 57.2, Loss =  1.185898\n",
            "Loss over all Test data : 1.193148 \n",
            "\n",
            "Loss over all Training data : 1.159882 \n",
            "\n",
            "End of Epoch 00010Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.159666 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "End of Epoch 11Train : \n",
            " Accuracy = 58.5, Loss =  1.161034\n",
            "End of Epoch 11Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.237925 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "End of Epoch 12Train : \n",
            " Accuracy = 58.5, Loss =  1.160070\n",
            "End of Epoch 12Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.627817 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "End of Epoch 13Train : \n",
            " Accuracy = 52.7, Loss =  1.317495\n",
            "End of Epoch 13Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.202271 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "End of Epoch 14Train : \n",
            " Accuracy = 58.0, Loss =  1.177457\n",
            "End of Epoch 14Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.134911 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "End of Epoch 15Train : \n",
            " Accuracy = 59.5, Loss =  1.132114\n",
            "End of Epoch 15Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.112132 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "End of Epoch 16Train : \n",
            " Accuracy = 60.4, Loss =  1.106615\n",
            "End of Epoch 16Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.075773 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "End of Epoch 17Train : \n",
            " Accuracy = 61.5, Loss =  1.085741\n",
            "End of Epoch 17Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.043610 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "End of Epoch 18Train : \n",
            " Accuracy = 60.6, Loss =  1.111530\n",
            "End of Epoch 18Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.026924 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "End of Epoch 19Train : \n",
            " Accuracy = 62.3, Loss =  1.063198\n",
            "End of Epoch 19Test : \n",
            " Accuracy = 56.7, Loss =  1.193148\n",
            "Loss over all Training data : 1.105205 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "End of Epoch 20Train : \n",
            " Accuracy = 62.9, Loss =  1.051094\n",
            "Loss over all Test data : 1.091483 \n",
            "\n",
            "Loss over all Training data : 1.030919 \n",
            "\n",
            "End of Epoch 00020Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.030803 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "End of Epoch 21Train : \n",
            " Accuracy = 63.2, Loss =  1.043154\n",
            "End of Epoch 21Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.053179 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "End of Epoch 22Train : \n",
            " Accuracy = 62.4, Loss =  1.055521\n",
            "End of Epoch 22Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.045611 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "End of Epoch 23Train : \n",
            " Accuracy = 54.6, Loss =  1.269993\n",
            "End of Epoch 23Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.145764 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "End of Epoch 24Train : \n",
            " Accuracy = 60.3, Loss =  1.115536\n",
            "End of Epoch 24Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.031864 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "End of Epoch 25Train : \n",
            " Accuracy = 62.8, Loss =  1.050246\n",
            "End of Epoch 25Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.013326 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "End of Epoch 26Train : \n",
            " Accuracy = 53.0, Loss =  1.584838\n",
            "End of Epoch 26Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.714299 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "End of Epoch 27Train : \n",
            " Accuracy = 39.6, Loss =  1.622378\n",
            "End of Epoch 27Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.497067 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "End of Epoch 28Train : \n",
            " Accuracy = 46.2, Loss =  1.470263\n",
            "End of Epoch 28Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.370441 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "End of Epoch 29Train : \n",
            " Accuracy = 52.4, Loss =  1.313364\n",
            "End of Epoch 29Test : \n",
            " Accuracy = 61.4, Loss =  1.091483\n",
            "Loss over all Training data : 1.307503 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "End of Epoch 30Train : \n",
            " Accuracy = 51.0, Loss =  1.348311\n",
            "Loss over all Test data : 1.254962 \n",
            "\n",
            "Loss over all Training data : 1.225938 \n",
            "\n",
            "End of Epoch 00030Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.225999 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "End of Epoch 31Train : \n",
            " Accuracy = 43.3, Loss =  1.559739\n",
            "End of Epoch 31Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.370849 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "End of Epoch 32Train : \n",
            " Accuracy = 52.1, Loss =  1.319805\n",
            "End of Epoch 32Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.261208 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "End of Epoch 33Train : \n",
            " Accuracy = 51.8, Loss =  1.338402\n",
            "End of Epoch 33Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.225862 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "End of Epoch 34Train : \n",
            " Accuracy = 53.7, Loss =  1.288871\n",
            "End of Epoch 34Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.392995 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "End of Epoch 35Train : \n",
            " Accuracy = 54.8, Loss =  1.253593\n",
            "End of Epoch 35Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.177336 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "End of Epoch 36Train : \n",
            " Accuracy = 56.1, Loss =  1.220749\n",
            "End of Epoch 36Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.154467 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "End of Epoch 37Train : \n",
            " Accuracy = 56.8, Loss =  1.198310\n",
            "End of Epoch 37Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.150320 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "End of Epoch 38Train : \n",
            " Accuracy = 49.9, Loss =  1.386690\n",
            "End of Epoch 38Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.413592 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "End of Epoch 39Train : \n",
            " Accuracy = 39.2, Loss =  1.658699\n",
            "End of Epoch 39Test : \n",
            " Accuracy = 54.9, Loss =  1.254962\n",
            "Loss over all Training data : 1.584552 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "End of Epoch 40Train : \n",
            " Accuracy = 45.7, Loss =  1.473683\n",
            "Loss over all Test data : 1.412076 \n",
            "\n",
            "Loss over all Training data : 1.407153 \n",
            "\n",
            "End of Epoch 00040Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.407035 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "End of Epoch 41Train : \n",
            " Accuracy = 47.3, Loss =  1.429664\n",
            "End of Epoch 41Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.487864 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "End of Epoch 42Train : \n",
            " Accuracy = 48.8, Loss =  1.408792\n",
            "End of Epoch 42Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.555339 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "End of Epoch 43Train : \n",
            " Accuracy = 51.5, Loss =  1.351940\n",
            "End of Epoch 43Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.275248 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "End of Epoch 44Train : \n",
            " Accuracy = 55.4, Loss =  1.249454\n",
            "End of Epoch 44Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.167331 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "End of Epoch 45Train : \n",
            " Accuracy = 54.9, Loss =  1.255187\n",
            "End of Epoch 45Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.181123 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "End of Epoch 46Train : \n",
            " Accuracy = 57.2, Loss =  1.196468\n",
            "End of Epoch 46Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.185450 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "End of Epoch 47Train : \n",
            " Accuracy = 57.0, Loss =  1.212087\n",
            "End of Epoch 47Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.155918 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "End of Epoch 48Train : \n",
            " Accuracy = 58.6, Loss =  1.164771\n",
            "End of Epoch 48Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.147942 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "End of Epoch 49Train : \n",
            " Accuracy = 57.9, Loss =  1.173729\n",
            "End of Epoch 49Test : \n",
            " Accuracy = 47.3, Loss =  1.412076\n",
            "Loss over all Training data : 1.104670 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "End of Epoch 50Train : \n",
            " Accuracy = 60.1, Loss =  1.124907\n",
            "Loss over all Test data : 1.113628 \n",
            "\n",
            "Loss over all Training data : 1.076027 \n",
            "\n",
            "End of Epoch 00050Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.076033 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "End of Epoch 51Train : \n",
            " Accuracy = 60.9, Loss =  1.102888\n",
            "End of Epoch 51Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.055246 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "End of Epoch 52Train : \n",
            " Accuracy = 61.8, Loss =  1.080426\n",
            "End of Epoch 52Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.058791 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "End of Epoch 53Train : \n",
            " Accuracy = 61.1, Loss =  1.107128\n",
            "End of Epoch 53Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.011669 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "End of Epoch 54Train : \n",
            " Accuracy = 62.3, Loss =  1.067608\n",
            "End of Epoch 54Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.081305 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "End of Epoch 55Train : \n",
            " Accuracy = 62.1, Loss =  1.073941\n",
            "End of Epoch 55Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.159685 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "End of Epoch 56Train : \n",
            " Accuracy = 62.9, Loss =  1.043458\n",
            "End of Epoch 56Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.006626 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "End of Epoch 57Train : \n",
            " Accuracy = 63.4, Loss =  1.036432\n",
            "End of Epoch 57Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 0.994523 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "End of Epoch 58Train : \n",
            " Accuracy = 63.6, Loss =  1.032444\n",
            "End of Epoch 58Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.010970 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "End of Epoch 59Train : \n",
            " Accuracy = 61.0, Loss =  1.098386\n",
            "End of Epoch 59Test : \n",
            " Accuracy = 60.1, Loss =  1.113628\n",
            "Loss over all Training data : 1.005713 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "End of Epoch 60Train : \n",
            " Accuracy = 63.7, Loss =  1.029790\n",
            "Loss over all Test data : 1.072168 \n",
            "\n",
            "Loss over all Training data : 1.007018 \n",
            "\n",
            "End of Epoch 00060Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.006957 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "End of Epoch 61Train : \n",
            " Accuracy = 64.1, Loss =  1.021520\n",
            "End of Epoch 61Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 0.989198 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "End of Epoch 62Train : \n",
            " Accuracy = 64.2, Loss =  1.011140\n",
            "End of Epoch 62Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 0.988756 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "End of Epoch 63Train : \n",
            " Accuracy = 61.9, Loss =  1.091771\n",
            "End of Epoch 63Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.048773 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "End of Epoch 64Train : \n",
            " Accuracy = 44.6, Loss =  1.625198\n",
            "End of Epoch 64Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.443599 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "End of Epoch 65Train : \n",
            " Accuracy = 48.5, Loss =  1.408842\n",
            "End of Epoch 65Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.364179 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "End of Epoch 66Train : \n",
            " Accuracy = 51.6, Loss =  1.333792\n",
            "End of Epoch 66Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.291312 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "End of Epoch 67Train : \n",
            " Accuracy = 53.5, Loss =  1.285756\n",
            "End of Epoch 67Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.279727 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "End of Epoch 68Train : \n",
            " Accuracy = 54.3, Loss =  1.269427\n",
            "End of Epoch 68Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.496731 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "End of Epoch 69Train : \n",
            " Accuracy = 53.8, Loss =  1.280936\n",
            "End of Epoch 69Test : \n",
            " Accuracy = 61.6, Loss =  1.072168\n",
            "Loss over all Training data : 1.184749 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "End of Epoch 70Train : \n",
            " Accuracy = 56.9, Loss =  1.199861\n",
            "Loss over all Test data : 1.204871 \n",
            "\n",
            "Loss over all Training data : 1.180605 \n",
            "\n",
            "End of Epoch 00070Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.180653 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "End of Epoch 71Train : \n",
            " Accuracy = 57.8, Loss =  1.179575\n",
            "End of Epoch 71Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.158041 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "End of Epoch 72Train : \n",
            " Accuracy = 57.9, Loss =  1.172328\n",
            "End of Epoch 72Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.231043 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "End of Epoch 73Train : \n",
            " Accuracy = 58.4, Loss =  1.161494\n",
            "End of Epoch 73Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.132876 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "End of Epoch 74Train : \n",
            " Accuracy = 57.7, Loss =  1.182562\n",
            "End of Epoch 74Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.328962 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "End of Epoch 75Train : \n",
            " Accuracy = 56.3, Loss =  1.220459\n",
            "End of Epoch 75Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.184129 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "End of Epoch 76Train : \n",
            " Accuracy = 59.5, Loss =  1.131276\n",
            "End of Epoch 76Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.071604 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "End of Epoch 77Train : \n",
            " Accuracy = 59.6, Loss =  1.130107\n",
            "End of Epoch 77Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.184108 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "End of Epoch 78Train : \n",
            " Accuracy = 58.8, Loss =  1.163848\n",
            "End of Epoch 78Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.081540 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "End of Epoch 79Train : \n",
            " Accuracy = 59.1, Loss =  1.145104\n",
            "End of Epoch 79Test : \n",
            " Accuracy = 56.0, Loss =  1.204871\n",
            "Loss over all Training data : 1.179540 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "End of Epoch 80Train : \n",
            " Accuracy = 60.1, Loss =  1.116061\n",
            "Loss over all Test data : 1.122203 \n",
            "\n",
            "Loss over all Training data : 1.067906 \n",
            "\n",
            "End of Epoch 00080Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.067866 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "End of Epoch 81Train : \n",
            " Accuracy = 61.2, Loss =  1.088164\n",
            "End of Epoch 81Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.076137 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "End of Epoch 82Train : \n",
            " Accuracy = 60.1, Loss =  1.115188\n",
            "End of Epoch 82Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.085806 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "End of Epoch 83Train : \n",
            " Accuracy = 61.2, Loss =  1.085696\n",
            "End of Epoch 83Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.017354 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "End of Epoch 84Train : \n",
            " Accuracy = 59.2, Loss =  1.135297\n",
            "End of Epoch 84Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.071492 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "End of Epoch 85Train : \n",
            " Accuracy = 60.7, Loss =  1.095929\n",
            "End of Epoch 85Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.080577 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "End of Epoch 86Train : \n",
            " Accuracy = 61.2, Loss =  1.079391\n",
            "End of Epoch 86Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.073680 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "End of Epoch 87Train : \n",
            " Accuracy = 61.1, Loss =  1.083567\n",
            "End of Epoch 87Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.058834 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "End of Epoch 88Train : \n",
            " Accuracy = 62.5, Loss =  1.049653\n",
            "End of Epoch 88Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.036111 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "End of Epoch 89Train : \n",
            " Accuracy = 61.6, Loss =  1.074192\n",
            "End of Epoch 89Test : \n",
            " Accuracy = 59.7, Loss =  1.122203\n",
            "Loss over all Training data : 1.074444 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "End of Epoch 90Train : \n",
            " Accuracy = 62.7, Loss =  1.055540\n",
            "Loss over all Test data : 1.164727 \n",
            "\n",
            "Loss over all Training data : 1.122715 \n",
            "\n",
            "End of Epoch 00090Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.122659 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "End of Epoch 91Train : \n",
            " Accuracy = 43.7, Loss =  1.533851\n",
            "End of Epoch 91Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.375221 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "End of Epoch 92Train : \n",
            " Accuracy = 51.5, Loss =  1.337494\n",
            "End of Epoch 92Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.253406 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "End of Epoch 93Train : \n",
            " Accuracy = 54.3, Loss =  1.267900\n",
            "End of Epoch 93Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.191289 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "End of Epoch 94Train : \n",
            " Accuracy = 56.6, Loss =  1.213096\n",
            "End of Epoch 94Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.174279 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "End of Epoch 95Train : \n",
            " Accuracy = 55.0, Loss =  1.253167\n",
            "End of Epoch 95Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.197344 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "End of Epoch 96Train : \n",
            " Accuracy = 57.7, Loss =  1.183964\n",
            "End of Epoch 96Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.129708 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "End of Epoch 97Train : \n",
            " Accuracy = 58.6, Loss =  1.163590\n",
            "End of Epoch 97Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.157475 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "End of Epoch 98Train : \n",
            " Accuracy = 57.9, Loss =  1.176487\n",
            "End of Epoch 98Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.158043 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "End of Epoch 99Train : \n",
            " Accuracy = 58.1, Loss =  1.165510\n",
            "End of Epoch 99Test : \n",
            " Accuracy = 58.5, Loss =  1.164727\n",
            "Loss over all Training data : 1.114894 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "End of Epoch 100Train : \n",
            " Accuracy = 59.7, Loss =  1.124523\n",
            "Loss over all Test data : 1.128299 \n",
            "\n",
            "Loss over all Training data : 1.079240 \n",
            "\n",
            "End of Epoch 00100Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.079361 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "End of Epoch 101Train : \n",
            " Accuracy = 60.4, Loss =  1.113079\n",
            "End of Epoch 101Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.065052 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "End of Epoch 102Train : \n",
            " Accuracy = 60.9, Loss =  1.101711\n",
            "End of Epoch 102Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.058909 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "End of Epoch 103Train : \n",
            " Accuracy = 58.5, Loss =  1.168576\n",
            "End of Epoch 103Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.127603 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "End of Epoch 104Train : \n",
            " Accuracy = 61.4, Loss =  1.081489\n",
            "End of Epoch 104Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.046161 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "End of Epoch 105Train : \n",
            " Accuracy = 57.9, Loss =  1.193938\n",
            "End of Epoch 105Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.270620 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "End of Epoch 106Train : \n",
            " Accuracy = 58.9, Loss =  1.152447\n",
            "End of Epoch 106Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.121577 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "End of Epoch 107Train : \n",
            " Accuracy = 58.6, Loss =  1.161033\n",
            "End of Epoch 107Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.080103 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "End of Epoch 108Train : \n",
            " Accuracy = 61.6, Loss =  1.077214\n",
            "End of Epoch 108Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.051603 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "End of Epoch 109Train : \n",
            " Accuracy = 62.5, Loss =  1.053050\n",
            "End of Epoch 109Test : \n",
            " Accuracy = 59.7, Loss =  1.128299\n",
            "Loss over all Training data : 1.050601 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "End of Epoch 110Train : \n",
            " Accuracy = 62.8, Loss =  1.048763\n",
            "Loss over all Test data : 1.203156 \n",
            "\n",
            "Loss over all Training data : 1.154555 \n",
            "\n",
            "End of Epoch 00110Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 1.154348 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "End of Epoch 111Train : \n",
            " Accuracy = 58.3, Loss =  1.169743\n",
            "End of Epoch 111Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 1.114183 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "End of Epoch 112Train : \n",
            " Accuracy = 62.3, Loss =  1.059323\n",
            "End of Epoch 112Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 1.011185 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "End of Epoch 113Train : \n",
            " Accuracy = 63.3, Loss =  1.035953\n",
            "End of Epoch 113Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 0.989096 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "End of Epoch 114Train : \n",
            " Accuracy = 63.8, Loss =  1.020210\n",
            "End of Epoch 114Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 1.023865 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "End of Epoch 115Train : \n",
            " Accuracy = 64.0, Loss =  1.021455\n",
            "End of Epoch 115Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 0.963013 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "End of Epoch 116Train : \n",
            " Accuracy = 61.9, Loss =  1.072523\n",
            "End of Epoch 116Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 1.114233 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "End of Epoch 117Train : \n",
            " Accuracy = 62.8, Loss =  1.049697\n",
            "End of Epoch 117Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 0.978516 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "End of Epoch 118Train : \n",
            " Accuracy = 64.5, Loss =  1.001583\n",
            "End of Epoch 118Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 0.998236 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "End of Epoch 119Train : \n",
            " Accuracy = 64.7, Loss =  0.999453\n",
            "End of Epoch 119Test : \n",
            " Accuracy = 58.7, Loss =  1.203156\n",
            "Loss over all Training data : 0.954359 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "End of Epoch 120Train : \n",
            " Accuracy = 64.5, Loss =  1.001196\n",
            "Loss over all Test data : 1.074921 \n",
            "\n",
            "Loss over all Training data : 1.012780 \n",
            "\n",
            "End of Epoch 00120Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 1.012688 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "End of Epoch 121Train : \n",
            " Accuracy = 61.7, Loss =  1.079878\n",
            "End of Epoch 121Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 1.071478 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "End of Epoch 122Train : \n",
            " Accuracy = 62.2, Loss =  1.065062\n",
            "End of Epoch 122Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.981709 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "End of Epoch 123Train : \n",
            " Accuracy = 64.8, Loss =  0.992403\n",
            "End of Epoch 123Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.937537 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "End of Epoch 124Train : \n",
            " Accuracy = 65.4, Loss =  0.977880\n",
            "End of Epoch 124Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.941930 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "End of Epoch 125Train : \n",
            " Accuracy = 63.7, Loss =  1.029424\n",
            "End of Epoch 125Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 1.051244 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "End of Epoch 126Train : \n",
            " Accuracy = 61.4, Loss =  1.095662\n",
            "End of Epoch 126Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.977217 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "End of Epoch 127Train : \n",
            " Accuracy = 65.3, Loss =  0.982087\n",
            "End of Epoch 127Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.926864 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "End of Epoch 128Train : \n",
            " Accuracy = 66.2, Loss =  0.955670\n",
            "End of Epoch 128Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.934974 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "End of Epoch 129Train : \n",
            " Accuracy = 66.4, Loss =  0.947309\n",
            "End of Epoch 129Test : \n",
            " Accuracy = 61.9, Loss =  1.074921\n",
            "Loss over all Training data : 0.914344 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "End of Epoch 130Train : \n",
            " Accuracy = 66.3, Loss =  0.953002\n",
            "Loss over all Test data : 1.026787 \n",
            "\n",
            "Loss over all Training data : 0.921128 \n",
            "\n",
            "End of Epoch 00130Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.921420 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "End of Epoch 131Train : \n",
            " Accuracy = 66.4, Loss =  0.957032\n",
            "End of Epoch 131Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.913496 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "End of Epoch 132Train : \n",
            " Accuracy = 66.8, Loss =  0.945285\n",
            "End of Epoch 132Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.944039 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "End of Epoch 133Train : \n",
            " Accuracy = 66.6, Loss =  0.947066\n",
            "End of Epoch 133Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.960036 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "End of Epoch 134Train : \n",
            " Accuracy = 65.7, Loss =  0.971163\n",
            "End of Epoch 134Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.928283 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "End of Epoch 135Train : \n",
            " Accuracy = 66.7, Loss =  0.937212\n",
            "End of Epoch 135Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.918486 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "End of Epoch 136Train : \n",
            " Accuracy = 67.1, Loss =  0.927521\n",
            "End of Epoch 136Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.911132 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "End of Epoch 137Train : \n",
            " Accuracy = 60.7, Loss =  1.117542\n",
            "End of Epoch 137Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 1.022865 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "End of Epoch 138Train : \n",
            " Accuracy = 64.4, Loss =  1.001602\n",
            "End of Epoch 138Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.927203 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "End of Epoch 139Train : \n",
            " Accuracy = 66.8, Loss =  0.933686\n",
            "End of Epoch 139Test : \n",
            " Accuracy = 63.9, Loss =  1.026787\n",
            "Loss over all Training data : 0.899142 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "End of Epoch 140Train : \n",
            " Accuracy = 66.4, Loss =  0.951804\n",
            "Loss over all Test data : 1.217947 \n",
            "\n",
            "Loss over all Training data : 1.162876 \n",
            "\n",
            "End of Epoch 00140Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 1.162982 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "End of Epoch 141Train : \n",
            " Accuracy = 66.5, Loss =  0.949857\n",
            "End of Epoch 141Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.913789 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "End of Epoch 142Train : \n",
            " Accuracy = 67.3, Loss =  0.923642\n",
            "End of Epoch 142Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.882210 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "End of Epoch 143Train : \n",
            " Accuracy = 67.7, Loss =  0.913261\n",
            "End of Epoch 143Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.887377 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "End of Epoch 144Train : \n",
            " Accuracy = 67.9, Loss =  0.915785\n",
            "End of Epoch 144Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.918646 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "End of Epoch 145Train : \n",
            " Accuracy = 67.6, Loss =  0.919280\n",
            "End of Epoch 145Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.909880 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "End of Epoch 146Train : \n",
            " Accuracy = 67.8, Loss =  0.909553\n",
            "End of Epoch 146Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.884960 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "End of Epoch 147Train : \n",
            " Accuracy = 67.5, Loss =  0.921760\n",
            "End of Epoch 147Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.934636 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "End of Epoch 148Train : \n",
            " Accuracy = 67.3, Loss =  0.923882\n",
            "End of Epoch 148Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.864932 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "End of Epoch 149Train : \n",
            " Accuracy = 68.2, Loss =  0.898942\n",
            "End of Epoch 149Test : \n",
            " Accuracy = 56.6, Loss =  1.217947\n",
            "Loss over all Training data : 0.853375 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "End of Epoch 150Train : \n",
            " Accuracy = 68.4, Loss =  0.893415\n",
            "Loss over all Test data : 0.990334 \n",
            "\n",
            "Loss over all Training data : 0.852313 \n",
            "\n",
            "End of Epoch 00150Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 0.852413 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "End of Epoch 151Train : \n",
            " Accuracy = 68.2, Loss =  0.904359\n",
            "End of Epoch 151Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 0.974989 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "End of Epoch 152Train : \n",
            " Accuracy = 68.4, Loss =  0.896463\n",
            "End of Epoch 152Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 0.857266 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "End of Epoch 153Train : \n",
            " Accuracy = 69.0, Loss =  0.871296\n",
            "End of Epoch 153Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 0.906980 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "End of Epoch 154Train : \n",
            " Accuracy = 68.6, Loss =  0.893114\n",
            "End of Epoch 154Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 1.087067 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "End of Epoch 155Train : \n",
            " Accuracy = 67.5, Loss =  0.916424\n",
            "End of Epoch 155Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 0.883304 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "End of Epoch 156Train : \n",
            " Accuracy = 69.0, Loss =  0.879049\n",
            "End of Epoch 156Test : \n",
            " Accuracy = 66.2, Loss =  0.990334\n",
            "Loss over all Training data : 0.861046 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n"
          ]
        }
      ],
      "source": [
        "x_axis_global = []\n",
        "\n",
        "train_loss_stopping_criteria = .00000001\n",
        "\n",
        "# n_neurons_list = [8*32*32, 8*32*32, 8*32*32, 64]\n",
        "n_neurons_list = [64, 64]\n",
        "# n_neurons_list = [256]\n",
        "# n_epochs = 100\n",
        "n_cnn_layers = 5\n",
        "n_hidden_layers,  n_neurons, n_runs =  6, 64, 1 #n_hidden_layers = n_cnn_layers + n_dense_layers; n_neurons is for dense layer\n",
        "inp_dim, out_dim = inp_dim, n_classes if is_classification else 1\n",
        "lr = 9e-4\n",
        "save_result, download_result = True, False\n",
        "'''\n",
        "Available Protocols : MLP, DGN, DLGN, DLGN-SF, DGN-DLGN-SF\n",
        "Available learning types : \"BOTH\",  'ONPV', 'ONPF'\n",
        "Available input types : 'PWL', 'PWC'\n",
        "'''\n",
        "# protocols = ['MLP', \"DGN\", 'DLGN', 'DLGN-SF','DGN-DLGN-SF']\n",
        "\n",
        "# protocols = ['DGN', 'DLGN', 'DLGN-SF']\n",
        "# learning_types = ['ONPV', 'ONPF']\n",
        "# input_types = ['PWL', 'PWC']\n",
        "\n",
        "# protocols = [ 'DLGN' ]\n",
        "# learning_types = ['ONPV', 'BOTH_ONPV'] \n",
        "# input_types = ['PWC']\n",
        "protocols = [ 'DLGN']\n",
        "learning_types = ['BOTH'] \n",
        "input_types = ['PWC']\n",
        "# epsilon = .075\n",
        "n_points_vicinity = 2000\n",
        "pos_neg_ratio_criteria = 4\n",
        "\n",
        "bias = True\n",
        "plot_hyperplanes_figures = False\n",
        "set_seed(seed = seed)\n",
        "\n",
        "for protocol in protocols:\n",
        "  for learning_type in learning_types:\n",
        "      if learning_type == 'ONPF' or learning_type == 'BOTH':\n",
        "        act_types = ['soft']\n",
        "      elif learning_type == 'BOTH_ONPV':\n",
        "        act_types = ['soft_soft']\n",
        "      elif learning_type == 'ONPV':\n",
        "        act_types = [ 'soft']\n",
        "      if learning_type == 'BOTH':\n",
        "        n_epochs = 200\n",
        "        step_stop_point = 2\n",
        "        step_stepsize = 100\n",
        "        epoch_stop_point = 10\n",
        "        epoch_stepsize = 10\n",
        "        start_point = get_start_point(epoch_stop_point, epoch_stepsize)\n",
        "      else:\n",
        "        n_epochs = 200\n",
        "        step_stop_point = 2\n",
        "        step_stepsize = 100\n",
        "        epoch_stop_point = 10\n",
        "        epoch_stepsize = 10\n",
        "        start_point = get_start_point(epoch_stop_point, epoch_stepsize)\n",
        "\n",
        "      for input_type in input_types:\n",
        "        for act_type in act_types:\n",
        "          set_seed(seed = seed)\n",
        "          \n",
        "          if protocol == 'MLP':\n",
        "            act_type = 'hard'\n",
        "            learning_type = 'BOTH'\n",
        "            hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs = run_mlp_model(train_dataloader= train_dataloader,\n",
        "                                                                                            test_dataloader = test_dataloader,\n",
        "                                                                                            mini_dl = mini_dl,\n",
        "                                                                                            inp_dim = inp_dim,\n",
        "                                                                                            out_dim = out_dim,\n",
        "                                                                                            n_h_l = n_hidden_layers,\n",
        "                                                                                            n_n = n_neurons,\n",
        "                                                                                            n_runs = n_runs, n_epochs = n_epochs, bias = bias)\n",
        "          else:\n",
        "            if input_type == 'PWL':\n",
        "              train_dl_npv = train_dataloader\n",
        "              test_dl_npv = test_dataloader\n",
        "            elif input_type == 'PWC':\n",
        "              train_dl_npv = train_dl_PWC\n",
        "              test_dl_npv = test_dl_PWC\n",
        "          \n",
        "          \n",
        "\n",
        "            hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs, weights_biases_all_runs = run_npf_npv_model(train_dl_npf = train_dataloader,\n",
        "                                                                                              train_dl_npv = train_dl_npv,\n",
        "                                                                                              test_dl_npf = test_dataloader,\n",
        "                                                                                              test_dl_npv = test_dl_npv,\n",
        "                                                                                              mini_dl = mini_dl,\n",
        "                                                                                              protocol = protocol,\n",
        "                                                                                              learning_type = learning_type,\n",
        "                                                                                              act_type = act_type,\n",
        "                                                                                              inp_dim = inp_dim,\n",
        "                                                                                              out_dim = out_dim,\n",
        "                                                                                              n_h_l = n_hidden_layers,\n",
        "                                                                                              n_n = n_neurons,\n",
        "                                                                                              n_runs = n_runs,\n",
        "                                                                                              n_epochs = n_epochs,\n",
        "                                                                                              lr = lr,\n",
        "                                                                                              bias = bias,\n",
        "                                                                                              epsilon = None)\n",
        "          # state_collections.append((state_info_5_runs,predictions_5_runs))\n",
        "          # state_collections[0] = (state_info_5_runs,predictions_5_runs)\n",
        "          # kernel_5_runs = build_kernels(protocol, hidden_layer_outs_5_runs)\n",
        "          # for kernel_name in ['K2', 'K3','K']:\n",
        "          #   plot_kernel_values_stepwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "          #   plot_kernel_values_epochwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "\n",
        "          # plot_stepwise_activations(hidden_layer_no =2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "          # plot_epochwise_activations(hidden_layer_no = 2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "          x_axis_global = get_x_axis_global(state_info_5_runs, x_axis_global)\n",
        "          # plot_stepwise_loss(state_info_5_runs)\n",
        "          plot_epochwise_loss(state_info_5_runs)\n",
        "          # plot_stepwise_acc(state_info_5_runs)\n",
        "          plot_epochwise_acc(state_info_5_runs)\n",
        "         \n",
        "          # plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)\n",
        "          # plot_hyperplanes(weights_biases_all_runs, state_info_5_runs, n_rows = 8 , n_cols = 2)\n",
        "          # plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs, n_rows = 8, n_cols = 2)\n",
        "          # for epsilon in [.6]:\n",
        "            \n",
        "          #   posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs, epsilon, train_data_curr, train_labels_curr)\n",
        "          # #   plot_hyp_posneg(posneg_pairs_all_runs,n_neurons_list, epsilon)\n",
        "          # #   plot_hyp_posneg_ratio(posneg_pairs_all_runs,n_neurons_list, epsilon)\n",
        "          #   plot_hyp_local_usefulness(local_usefulness_all_runs,n_neurons_list, epsilon)\n",
        "\n",
        "            # plot_hyperplanes_temp(weights_biases_all_runs, state_info_5_runs)\n",
        "            # plot_hyperplanes_all_temp(weights_biases_all_runs, state_info_5_runs)\n",
        "#-----------\n",
        "            #   plot_hyp_posneg(hyp_5_runs)\n",
        "            # plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)\n",
        "            # plot_predictions(21, predictions_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)\n",
        "            # #Saving results of each protocol\n",
        "#------------\n",
        "          if save_result:\n",
        "            # file_name = f'{protocol}_{learning_type}_{input_type}_{act_type}_{n_hidden_layers}_{n_neurons}_n={n_data}_modes={num_modes}_d={d}_dsphere'\n",
        "            file_name = f'{protocol}_{learning_type}_{input_type}_{n_hidden_layers}_{n_neurons}_{dataset_name}'\n",
        "\n",
        "            save_results(file_name)\n",
        "\n",
        "  # #Download all protocols result\n",
        "if download_result:\n",
        "  download_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len()"
      ],
      "metadata": {
        "id": "JRBVcW3Tj7t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)"
      ],
      "metadata": {
        "id": "4qNNTQI7-5VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "etTjvOENWuTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IG0cTRzVWtuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqahzG5Varyh"
      },
      "outputs": [],
      "source": [
        "plot_predictions(None, state_collections[0][1], state_collections[0][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "plot_predictions(None, state_collections[1][1], state_collections[1][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "plot_predictions(None, state_collections[2][1], state_collections[2][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "plot_predictions(None, state_collections[3][1], state_collections[3][0],for_all_inputs = True, save_fig = False, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTX16i5onSao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ufiHOUCCkOR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EQQZ38ynSX5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TqiKaZjnSUz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUBQoBSSnw5K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vyrGKmfnw2T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLwDGDOlY1aq"
      },
      "outputs": [],
      "source": [
        "# !rm -r *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg-hIvqHOdEo"
      },
      "outputs": [],
      "source": [
        "plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEpAJdblLniI"
      },
      "outputs": [],
      "source": [
        "state_info_5_runs[0][5][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A0Wn8wBrLwY"
      },
      "outputs": [],
      "source": [
        "plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w1qEhYoexl1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mAs03p_gm_x"
      },
      "outputs": [],
      "source": [
        "plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zncSiZNFK12l"
      },
      "outputs": [],
      "source": [
        "file_name = f'{protocol}_{learning_type}_{input_type}_{n_hidden_layers}_{n_neurons}_{dataset_name}'\n",
        "save_results(file_name)\n",
        "download_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NYTooMfn1fG"
      },
      "source": [
        "#Testing arena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKPTwirGuwFJ"
      },
      "outputs": [],
      "source": [
        "for r in range(len(hidden_layer_outs_5_runs)):\n",
        "  for e in range(len(hidden_layer_outs_5_runs[r])):\n",
        "    for s in range(len(hidden_layer_outs_5_runs[r][e])):\n",
        "      print(hidden_layer_outs_5_runs[r][e][s][1].shape, state_info_5_runs[r][e][s][\"epoch\"], state_info_5_runs[r][e][s][\"step\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQR7dYUfuwCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3cYcHIMnxMF"
      },
      "source": [
        "#Commented codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5PoOBQMbvIi"
      },
      "outputs": [],
      "source": [
        "# plot_hyp_posneg(posneg_pairs_all_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doEdNkMXL5cA"
      },
      "outputs": [],
      "source": [
        "# posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs, epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEd9d9MCBJvY"
      },
      "outputs": [],
      "source": [
        "# plot_hyp_posneg(posneg_pairs_all_runs)\n",
        "# plot_hyp_posneg_ratio(posneg_pairs_all_runs)\n",
        "# plot_hyp_local_usefulness(local_usefulness_all_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBpUB9COMTn8"
      },
      "outputs": [],
      "source": [
        "# posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs)\n",
        "# plot_hyp_posneg(posneg_pairs_all_runs)\n",
        "# plot_hyp_posneg_ratio(posneg_pairs_all_runs)\n",
        "# plot_hyp_local_usefulness(local_usefulness_all_runs)\n",
        "\n",
        "# plot_hyperplanes_temp(weights_biases_all_runs, state_info_5_runs)\n",
        "# plot_hyperplanes_all_temp(weights_biases_all_runs, state_info_5_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDylYxC7L0jj"
      },
      "outputs": [],
      "source": [
        "# !rm -r *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPPvW2UeaSQm"
      },
      "outputs": [],
      "source": [
        "# class NPFNeuralNetwork(nn.Module):\n",
        "#     def __init__(self, n_hidden_layers, n_neurons, protocol_type, bias):\n",
        "#         super(NPFNeuralNetwork, self).__init__()\n",
        "#         self.protocol_type = protocol_type\n",
        "#         self.n_hidden_layers = n_hidden_layers\n",
        "#         self.n_neurons = n_neurons\n",
        "#         self.bias = bias\n",
        "        \n",
        "#         self.layers = nn.ModuleList([])\n",
        "   \n",
        "#         #For DLGN-SF, DGN-DLGN-SF\n",
        "#         if self.protocol_type == \"DLGN-SF\":\n",
        "#           self.layers.append(nn.Linear(2,  self.n_neurons, bias = self.bias).to(device))\n",
        "#           for i in range(self.n_hidden_layers-1):\n",
        "#             self.layers.append(nn.Linear( 2,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "#         elif self.protocol_type == \"DGN-DLGN-SF\":\n",
        "#           self.layers.append(nn.Sequential(nn.Linear(2,  self.n_neurons, bias = self.bias).to(device),\n",
        "#                                             nn.ReLU(),\n",
        "#                                             nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "#                                             )\n",
        "#                             )\n",
        "          \n",
        "#           for i in range(self.n_hidden_layers-1):\n",
        "#             self.layers.append(nn.Sequential(\n",
        "#                                   nn.Linear( 2,  self.n_neurons, bias = self.bias).to(device),\n",
        "#                                   nn.ReLU(),\n",
        "#                                   nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "#                               )\n",
        "#                         )\n",
        "#         #For DGN, DLGN\n",
        "#         else: \n",
        "#           self.layers.append(nn.Linear(2,  self.n_neurons, bias = self.bias).to(device))\n",
        "#           for i in range(self.n_hidden_layers-1):\n",
        "#             self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "        \n",
        "        \n",
        "        \n",
        "#         # self.layers.append(nn.Linear( self.n_neurons, 1, bias = False).to(device))\n",
        "  \n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         hidden_layer_outputs = []\n",
        "      \n",
        "#         if self.protocol_type == \"DGN\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             x = self.layers[i](x)\n",
        "#             out = torch.relu(x)\n",
        "#             hidden_layer_outputs.append(out)\n",
        "#             x = out\n",
        "#           #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "#         elif  self.protocol_type == \"DLGN\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             x = self.layers[i](x)\n",
        "#             out = x\n",
        "#             hidden_layer_outputs.append(out)\n",
        "#             x = out\n",
        "#           #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "#         elif  self.protocol_type == \"DLGN-SF\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             out = self.layers[i](x)\n",
        "#             hidden_layer_outputs.append(out)\n",
        "#         elif  self.protocol_type == \"DGN-DLGN-SF\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             out = self.layers[i](x) \n",
        "#             hidden_layer_outputs.append(out)\n",
        "        \n",
        "\n",
        "#         return hidden_layer_outputs\n",
        "\n",
        "\n",
        "\n",
        "# class NPVNeuralNetwork(nn.Module):\n",
        "#     def __init__(self, n_hidden_layers, n_neurons, bias):\n",
        "#         super(NPVNeuralNetwork, self).__init__()\n",
        "  \n",
        "#         self.n_hidden_layers = n_hidden_layers\n",
        "#         self.n_neurons = n_neurons\n",
        "#         self.bias = bias\n",
        "#         self.layers = nn.ModuleList([nn.Linear(2,  self.n_neurons, bias = self.bias).to(device)])\n",
        "      \n",
        "#         for i in range(self.n_hidden_layers-1):\n",
        "#           self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "          \n",
        "#         self.layers.append(nn.Linear( self.n_neurons, 1, bias = self.bias).to(device))\n",
        "\n",
        "#         self.gate = Gate()\n",
        "      \n",
        "#     def forward(self, x,act_type, gating_mask):\n",
        "#         hidden_layer_outputs = []\n",
        "#         for i in range(len(self.layers)-1):\n",
        "#           x = self.layers[i](x)\n",
        "          \n",
        "#           out = self.gate(act_type, x, i, gating_mask)\n",
        "          \n",
        "#           hidden_layer_outputs.append(out)\n",
        "#           x = out\n",
        "#         x = self.layers[len(self.layers)-1](x)\n",
        "#         #print(\"debug 2\", x.shape, x)\n",
        "#         return x,  hidden_layer_outputs\n",
        "\n",
        "\n",
        "# def apply_gate(beta, idx, gating_mask):\n",
        "#   out = beta*(gating_mask[idx])\n",
        "  \n",
        "#   return out\n",
        "  \n",
        "# class Gate(nn.Module):\n",
        "#     def __init__(self, beta = 4):\n",
        "#         super(Gate,self).__init__()\n",
        "#         self.beta = beta\n",
        "\n",
        "#     def forward(self,act_type, x, idx, gating_mask):\n",
        "#       #Soft Relu\n",
        "#       if act_type == 'soft':\n",
        "#         return torch.mul(x,torch.sigmoid(apply_gate(self.beta, idx,gating_mask)))\n",
        "#       elif act_type == 'hard':\n",
        "#       #Hard Relu\n",
        "#         temp = torch.sign(gating_mask[idx])\n",
        "#         temp[temp <= 0] = 0\n",
        "#         return torch.mul(x,temp)\n",
        "\n",
        "\n",
        "\n",
        "# def train_decoupled(X1_dataloader,X2_dataloader, npf_model, npv_model, loss_fn, optimizer, dataloader_all = None, state_info = None):\n",
        "#     size = len(X1_dataloader.dataset)\n",
        "#     correct = 0\n",
        "#     state_info_over_batches = []\n",
        "#     hidden_layer_outs_over_batches = []\n",
        "#     predictions_over_batches = []\n",
        "\n",
        "#     if int(state_info['epoch']) <= 2:\n",
        "#       act_type = 'soft'\n",
        "#     elif int(state_info['epoch']) > 2:\n",
        "#       act_type = 'soft'\n",
        "#     else:\n",
        "#       print(\"Some error\")\n",
        "\n",
        "#     for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "#         X1, y1 = X1.to(device), y1.to(device)\n",
        "#         X2, y2 = X2.to(device), y2.to(device)\n",
        "        \n",
        "#         npf_model_hidden_layer_outs = npf_model(X1)\n",
        "#         pred, npv_model_hidden_layer_outs = npv_model(X2,act_type, npf_model_hidden_layer_outs)\n",
        "#         #print(\"Debug1 \", pred.shape)\n",
        "#         loss = loss_fn(pred, y1)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # if batch%10 == 0:\n",
        "#         #     #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#         #     loss, current = loss.item(), batch * len(X1)\n",
        "#         #     print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "#         if int(state_info['epoch']) <= 2:\n",
        "#           predictions, hidden_layer_outputs, loss = evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#           state_info.update({'step' : batch+1})\n",
        "#           state_info.update({'loss' : loss})\n",
        "#           state_info = format_state_info(state_info)\n",
        "#           routine(hidden_layer_outs_container = hidden_layer_outs_over_batches, state_info_container = state_info_over_batches, predictions_container = predictions_over_batches,\n",
        "#             hidden_layer_outputs = hidden_layer_outputs, state_info = state_info, predictions = predictions\n",
        "#             )\n",
        "        \n",
        "#     return predictions_over_batches, hidden_layer_outs_over_batches, state_info_over_batches\n",
        "\n",
        "# def evaluate_decoupled(dataloader, npf_model, npv_model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     npf_model.eval()\n",
        "#     npv_model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch, (X, y) in enumerate(dataloader):\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             npf_model_hidden_layer_outs = npf_model(X)\n",
        "#             pred, npv_model_hidden_layer_outs = npv_model(X,'soft', npf_model_hidden_layer_outs)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     npf_model_hidden_layer_outs = [x.clone().detach().to('cpu').numpy() for x in npf_model_hidden_layer_outs]\n",
        "#     print(f\"Loss over all data : {test_loss:>5f} \\n\")\n",
        "#     return pred, npf_model_hidden_layer_outs, test_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def run_npf_npv_model(train_dataloader, dataloader_all, protocol = 'DGN', n_h_l = 5,n_n = 32, n_runs = 5, n_epochs = 250, bias = True):\n",
        "\n",
        "#     model_protocol_type = protocol\n",
        "#     n_hidden_layers = n_h_l\n",
        "#     n_neurons = n_n\n",
        "\n",
        "#     state_info = {\"model_protocol_type\" : model_protocol_type,\"n_hidden_layers\":n_hidden_layers, \"n_neurons\" : n_neurons}\n",
        "#     hidden_layer_outs_5_runs = []\n",
        "#     state_info_5_runs = []\n",
        "#     predictions_5_runs = []\n",
        "    \n",
        "#     for run in range(n_runs):\n",
        "      \n",
        "#       #Model Init\n",
        "#       npf_model = NPFNeuralNetwork(n_hidden_layers, n_neurons, model_protocol_type, bias).to(device)\n",
        "#       npv_model = NPVNeuralNetwork(n_hidden_layers, n_neurons, bias).to(device)\n",
        "#       torch.save(npv_model.state_dict(), \"npv_init_weights\")\n",
        "\n",
        "#       loss_fn = nn.MSELoss()\n",
        "#       optimizer = torch.optim.Adam([\n",
        "#                       {'params': npf_model.parameters()},\n",
        "#                       {'params': npv_model.parameters()}],\n",
        "#                       lr = 3e-3) \n",
        "      \n",
        "#       #Make the non linear layer of NPF model untrainable\n",
        "#       #NOTE: Unchecked for different hidden layers(Currently working for n_h_l = 5)\n",
        "#       if model_protocol_type == 'DGN-DLGN-SF':\n",
        "#         for idx, param in enumerate(npf_model.parameters()):\n",
        "#           if idx % 4 == 0 or idx-1 % 4 == 0:\n",
        "#             param.requires_grad = False\n",
        "      \n",
        "#       #Local Variable Init\n",
        "#       hidden_layer_outs_run = []\n",
        "#       state_info_run = []\n",
        "#       predictions_run = []\n",
        "#       epochs = n_epochs\n",
        "\n",
        "#       #Evaluate before training starts\n",
        "#       predictions, hidden_layer_outputs, loss =  evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#       state_info.update({'run' : run+1, 'epoch' : 0,'step' : 0,'loss' : loss,'learning_status' : 'UnLearned'})\n",
        "#       state_info = format_state_info(state_info)\n",
        "#       routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#               hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions]\n",
        "#               )\n",
        "\n",
        "#       for epoch in range(epochs):\n",
        "#         print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "#         state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : None,'loss' : loss,'learning_status' : 'Learned'})\n",
        "#         predictions_batches, hidden_layer_outs_batches, state_info_batches = train_decoupled(train_dataloader,train_dataloader,npf_model, npv_model, loss_fn, optimizer,\n",
        "#                                                                   dataloader_all = dataloader_all, state_info = state_info)\n",
        "        \n",
        "\n",
        "#         if int(state_info['epoch']) <= 2: \n",
        "#             routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#                     hidden_layer_outputs = hidden_layer_outs_batches, state_info = state_info_batches, predictions = predictions_batches\n",
        "#                     )\n",
        "        \n",
        "#         # else: #int(state_info['epoch']) > 2:\n",
        "#         elif int(state_info['epoch'])  <= 20:\n",
        "#             predictions, hidden_layer_outputs, loss =  evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#             state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : 16,'loss' : loss,'learning_status' : 'Learned'})\n",
        "#             state_info = format_state_info(state_info)\n",
        "#             routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#                     hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions]\n",
        "#                     )\n",
        "#         elif int(state_info['epoch'])  % 10 == 0:\n",
        "#             predictions, hidden_layer_outputs, loss =  evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#             state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : 16,'loss' : loss,'learning_status' : 'Learned'})\n",
        "#             state_info = format_state_info(state_info)\n",
        "#             routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#                     hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions]\n",
        "#                     )\n",
        "#         if int(state_info['epoch']) == 2:\n",
        "#               for idx, param in enumerate(npf_model.parameters()):\n",
        "#                   param.requires_grad = False\n",
        "#               npv_model.load_state_dict(torch.load(\"npv_init_weights\"))\n",
        "#       state_info_5_runs.append(state_info_run)\n",
        "#       hidden_layer_outs_5_runs.append(hidden_layer_outs_run)\n",
        "#       predictions_5_runs.append(predictions_run)\n",
        "\n",
        "#     return hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3WoEdcgX7B7"
      },
      "outputs": [],
      "source": [
        "# seed = 10\n",
        "# set_seed(seed = seed)\n",
        "\n",
        "# train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr = get_multi_modes_data()\n",
        "# n_data = len(train_data_curr)\n",
        "# mini = get_mini_multi_modes_data(test_data_curr,num_modes = 21, n_examples_per_mode = 10)#get_mini_multi_modes_data(instance = 10)\n",
        "\n",
        "# train_dataloader, test_dataloader, train_dl_PWC, test_dl_PWC = get_dataloaders(train_data_curr, test_data_curr, train_labels_curr, test_labels_curr, batch_size = 64)\n",
        "# mini_dl = get_mini_multi_mode_dataloaders(mini, batch_size = len(mini))\n",
        "# x_idxs_pair = get_input_idxs(len(mini))\n",
        "# n_batches = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkPXWK6orsSa"
      },
      "outputs": [],
      "source": [
        "# def  get_hyperplanes_params_temp(model,protocol_type,  for_layers, for_mode):\n",
        "#   layer_weights = []\n",
        "#   layer_slopes = []\n",
        "#   layer_intercepts = []\n",
        "#   layer_biases = []\n",
        "\n",
        "#   # if for_mode == 1:\n",
        "#   #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,0:2]\n",
        "#   # elif for_mode == 21:\n",
        "#   #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,-2:]\n",
        "\n",
        "\n",
        "#   layer_weight = model.layers[0].weight.detach().to(\"cpu\")\n",
        "\n",
        "#   layer_bias = model.layers[0].bias.detach().to(\"cpu\")\n",
        "\n",
        "#   layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "#   layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "#   layer_slopes.append(layer_slope)\n",
        "#   layer_intercepts.append(layer_intercept)\n",
        "\n",
        "#   layer_weights.append(layer_weight)\n",
        "#   layer_biases.append(layer_bias)\n",
        "#   for i in range(1, for_layers):\n",
        "#     curr_layer_weight =  model.layers[i].weight.detach().to(\"cpu\")\n",
        "#     curr_layer_bias = model.layers[i].bias.detach().to(\"cpu\")\n",
        "#     if protocol_type == 'DLGN':\n",
        "#       layer_weight = torch.matmul(input= curr_layer_weight, other =  layer_weight)\n",
        "#       layer_bias = torch.matmul(input= curr_layer_weight, other =  layer_bias) + curr_layer_bias\n",
        "#     elif protocol_type == 'DLGN-SF':\n",
        "#       layer_weight = curr_layer_weight\n",
        "#       layer_bias = curr_layer_bias\n",
        "\n",
        "#     layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "#     layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "    \n",
        "#     layer_slopes.append(layer_slope)\n",
        "#     layer_intercepts.append(layer_intercept)\n",
        "\n",
        "#     layer_weights.append(layer_weight)\n",
        "#     layer_biases.append(layer_bias)\n",
        "#   return layer_weights, layer_biases, layer_slopes, layer_intercepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMRUt_d1QxQG"
      },
      "outputs": [],
      "source": [
        "#1010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2qL0ZIfebRT"
      },
      "outputs": [],
      "source": [
        "# seed = 1010 #10 for NN\n",
        "# set_seed(seed = seed)\n",
        "\n",
        "# train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels = get_cube_data()\n",
        "# n_data = len(train_data_curr)\n",
        "# # mini = get_mini_multi_modes_data(test_data_curr,num_modes = 6, n_examples_per_mode = 10)\n",
        "\n",
        "# train_dataloader, test_dataloader, train_dl_PWC, test_dl_PWC = get_dataloaders(train_data_curr, test_data_curr, train_labels_curr, test_labels_curr, batch_size = 64)\n",
        "# mini_dl = get_mini_multi_mode_dataloaders(mini_test_data, batch_size = len(mini_test_data))\n",
        "# x_idxs_pair = get_input_idxs(len(mini_test_data))\n",
        "# n_batches = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT-BugDaYWK4"
      },
      "outputs": [],
      "source": [
        "# def h(a, b, sigma):\n",
        "#   return np.amin([a/sigma, b/sigma, 1])\n",
        "# def Variable_K(X1, X2, sigma):\n",
        "#   print(X1.shape, X2.shape)\n",
        "#   K = np.zeros((X1.shape[0], X2.shape[0]))\n",
        "#   dist = get_dist(train_data_curr)\n",
        "#   # sigma = .1\n",
        "#   for i, x1 in enumerate(X1):\n",
        "#     for j, x2 in enumerate(X2):\n",
        "#       # K[i][j] = np.exp(- np.sum( np.power((x1 - x2),2) ) / float( 2*(sigma**2) ) )\n",
        "#       K[i][j] = np.exp(-np.square(np.linalg.norm(x1 - x2))/(2*sigma*sigma*h(dist[i], dist[j], sigma)))\n",
        "#   return K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN0tOHDz4e_D"
      },
      "outputs": [],
      "source": [
        "# def svm_run():\n",
        "\n",
        "#     gammas=[.001]\n",
        "#     C_vals = [ 5]\n",
        "# #     gammas=[10]\n",
        "# #     C_vals = [1.]\n",
        "#     if np.mean(train_labels_curr)==1 or np.mean(train_labels_curr)==0:\n",
        "#            return np.mean(test_labels_curr!=train_labels_curr[0]) , 0. , 0.01, 1.\n",
        "\n",
        "\n",
        "#     min_valierror=np.inf\n",
        "#     for g in gammas:\n",
        "#         for C in C_vals:\n",
        "#           for sigma in [.1]:\n",
        "#             classification_alg = SVC(kernel= \"precomputed\", gamma=g, C=C)\n",
        "#             classifier = classification_alg.fit(Variable_K(train_data_curr, train_data_curr, sigma), train_labels_curr)\n",
        "#             predictions = classifier.predict(Variable_K(train_data_curr, train_data_curr, sigma))\n",
        "#             train_err = np.sum(predictions!=train_labels_curr)\n",
        "#             # train_err = 0\n",
        "#             predictions = classifier.predict(Variable_K(vali_data_curr, train_data_curr, sigma))\n",
        "#             vali_err=np.sum(predictions!=vali_labels_curr)\n",
        "            \n",
        "#             predictions = classifier.predict(Variable_K(test_data_curr, train_data_curr, sigma))\n",
        "#             test_err=np.sum(predictions!=test_labels_curr)\n",
        "#             # test_err = 0\n",
        "#             if vali_err<min_valierror:\n",
        "#                 min_valierror = vali_err\n",
        "#                 min_testerror = test_err\n",
        "#                 best_C = C\n",
        "#                 best_gamma = g\n",
        "#                 best_train_err = train_err\n",
        "#                 best_pred = predictions\n",
        "#             print(vali_err*100/len(vali_labels_curr), g, C)\n",
        "#     print(min_testerror*100/len(test_labels_curr),min_valierror*100/len(vali_labels_curr), best_train_err, best_gamma, best_C, best_pred)\n",
        "#     return min_testerror*100/len(test_labels_curr),min_valierror*100/len(vali_labels_curr), best_train_err, best_gamma, best_C, best_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9TKGoSLPtN8"
      },
      "outputs": [],
      "source": [
        "# K = np.zeros((n_data, n_data))\n",
        "# dist = get_dist(train_data_curr)\n",
        "# sigma = 1\n",
        "# for i in range(n_data):\n",
        "#   for j in range(n_data):\n",
        "#     K[i][j] = np.exp(-np.square(np.linalg.norm(train_data_curr[i] - train_data_curr[j]))/(2*sigma*sigma*h(dist[i], dist[j], sigma)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw_Emr9AYpC9"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import classification_report\n",
        "# tuned_parameters = [\n",
        "#     {\"kernel\": [\"rbf\"], \"gamma\": [0.01,0.03,0.1,0.3,1,3,10,30,100,1000,10000], \"C\": [0.1, 1.,10.,100.,1000, 15000, 20000, 25000]},\n",
        "    \n",
        "# ]\n",
        "\n",
        "# scores = [\"accuracy\"]\n",
        "\n",
        "# for score in scores:\n",
        "#     print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "#     print()\n",
        "\n",
        "#     clf = GridSearchCV(SVC(), tuned_parameters, scoring='accuracy')\n",
        "#     clf.fit(train_data_curr, train_labels_curr)\n",
        "\n",
        "#     print(\"Best parameters set found on development set:\")\n",
        "#     print()\n",
        "#     print(clf.best_params_)\n",
        "#     print()\n",
        "#     print(\"Grid scores on development set:\")\n",
        "#     print()\n",
        "#     means = clf.cv_results_[\"mean_test_score\"]\n",
        "#     stds = clf.cv_results_[\"std_test_score\"]\n",
        "#     for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
        "#         print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "#     print()\n",
        "\n",
        "#     print(\"Detailed classification report:\")\n",
        "#     print()\n",
        "#     print(\"The model is trained on the full development set.\")\n",
        "#     print(\"The scores are computed on the full evaluation set.\")\n",
        "#     print()\n",
        "#     y_true, y_pred = vali_labels_curr, clf.predict(vali_data_curr)\n",
        "#     print(classification_report(y_true, y_pred))\n",
        "#     print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwBXMKanhEnw"
      },
      "outputs": [],
      "source": [
        "# plt.scatter(test_data_curr[1000:2000,-2],test_data_curr[1000:2000,-1], c = test_labels_curr[1000:2000] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kof2Nt9-4T5k"
      },
      "outputs": [],
      "source": [
        "# n_data = 10000\n",
        "# a, b, d = 10, 100, 4\n",
        "# both_mode, mode1, mode2, mini = get_dsphere_dls(n_data = n_data, a = a, b = b, d = d, batch_size = None, is_perturbed = True)\n",
        "# mode = 'both'\n",
        "# if mode == 'both':\n",
        "#   X_train, X_test, y_train, y_test = both_mode[0], both_mode[1], both_mode[2], both_mode[3]\n",
        "# elif mode == 'm1':\n",
        "#   X_train, X_test, y_train, y_test = mode1[0], mode1[1], mode1[2], mode1[3]\n",
        "# elif mode == 'm2':\n",
        "#   X_train, X_test, y_train, y_test = mode2[0], mode2[1], mode2[2], mode2[3]\n",
        "\n",
        "# train_dataloader, test_dataloader = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# mini_dl, Y_sorted = get_mini_dls(mini[0], mini[1], batch_size = 100)\n",
        "# Y_sorted = np.array(Y_sorted).reshape((len(Y_sorted), 1))\n",
        "# x_idxs_pair = get_input_idxs(Y_sorted)\n",
        "# n_batches = len(train_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjV696thp-kv"
      },
      "outputs": [],
      "source": [
        "# seed = 10\n",
        "# set_seed(seed = seed)\n",
        "# a, b = 2, 20\n",
        "# both_mode, mode1, mode2, mini = get_mnist_modified_data(a, b)\n",
        "# mode = 'both'\n",
        "# if mode == 'both':\n",
        "#   X_train, X_test, y_train, y_test = both_mode[0], both_mode[1], both_mode[2], both_mode[3]\n",
        "# elif mode == 'm1':\n",
        "#   X_train, X_test, y_train, y_test = mode1[0], mode1[1], mode1[2], mode1[3]\n",
        "# elif mode == 'm2':\n",
        "#   X_train, X_test, y_train, y_test = mode2[0], mode2[1], mode2[2], mode2[3]\n",
        "\n",
        "# train_dataloader, test_dataloader = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# # X_train, X_test, y_train, y_test = mode1[0], mode1[1], mode1[2], mode1[3]\n",
        "# # train_dataloader_m1, test_dataloader_m1 = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# # X_train, X_test, y_train, y_test = mode2[0], mode2[1], mode2[2], mode2[3]\n",
        "# # train_dataloader_m2, test_dataloader_m2 = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# mini_dl, Y_sorted = get_mini_dls(mini[0], mini[1], batch_size = 100)\n",
        "# Y_sorted = np.array(Y_sorted).reshape((len(Y_sorted), 1))\n",
        "# x_idxs_pair = get_input_idxs(Y_sorted)\n",
        "# n_batches = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ86z3ItJLwm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8EaJhAqKBPQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKlemLTz2ppN"
      },
      "outputs": [],
      "source": [
        "# for run in range(hyp_5_runs):\n",
        "#   for epoch in range(hyp_5_runs[run]):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nRCk8kKB9RH"
      },
      "outputs": [],
      "source": [
        "# n_parts = 4\n",
        "# n_neurons = 32\n",
        "# n_layers = 3\n",
        "# fig = plt.figure(figsize=(7*n_parts, n_layers*10))\n",
        "# outer = gridspec.GridSpec(3,1, wspace=0.2, hspace=0.2)\n",
        "\n",
        "# for layer_no in range(3):\n",
        "#     inner = gridspec.GridSpecFromSubplotSpec(2,4,\n",
        "#                     subplot_spec=outer[layer_no], wspace=0.1, hspace=0.2)\n",
        "\n",
        "#     # for j in range(8):\n",
        "#     #     ax = plt.Subplot(fig, inner[j])\n",
        "#         # t = ax.text(0.5,0.5, 'outer=%d, inner=%d' % (i, j))\n",
        "#     for i in range(n_parts):\n",
        "#       ax = plt.Subplot(fig, inner[i])\n",
        "#       ax.plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),0])\n",
        "#       ax.set_title(f\"# of positive pts,{int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "#       fig.add_subplot(ax)\n",
        "      \n",
        "#     for i in range(n_parts):\n",
        "#       ax = plt.Subplot(fig, inner[i+4])\n",
        "#       ax.plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),1])\n",
        "#       ax.set_title(f\"# of negative pts, {int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "#       fig.add_subplot(ax)\n",
        "\n",
        "# fig.show(N)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # for layer_no in range(1,4):  \n",
        " \n",
        "# #   n_parts = 4\n",
        "# #   n_neurons = 32\n",
        "# #   f, axes = plt.subplots(2, n_parts,figsize = (n_parts*10,10))\n",
        "\n",
        "# #   for i in range(n_parts):\n",
        "# #     axes[0][i].plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),0])\n",
        "# #     axes[0][i].set_title(f\"# of positive pts,{int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "# #   for i in range(n_parts):\n",
        "# #     axes[1][i].plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),1])\n",
        "# #     axes[1][i].set_title(f\"# of negative pts, {int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "# #   f.suptitle(f'Layer No = {layer_no}')\n",
        "# # plt.savefig('afdfdsfds')\n",
        "# # plt.plot(np.array(hyp_5_runs)[0][:,layer_no-1,:8,0], label = ['pos'])\n",
        "# # plt.figure()\n",
        "# # plt.plot(np.array(hyp_5_runs)[0][:,layer_no-1,8:16,0], label = ['pos'])\n",
        "\n",
        "# # plt.plot(np.array(hyp_5_runs)[0][:,layer_no-1,n_no-1,1], label = ['neg'])\n",
        "\n",
        "# # plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTf9aRebQ1Kw"
      },
      "outputs": [],
      "source": [
        "# B = pairwise_distances(mini_test_data,mini_test_data) \n",
        "# IP = np.dot(mini_test_data, mini_test_data.T)\n",
        "# A = overlap[0]*IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7Ps2AkNPcxE"
      },
      "outputs": [],
      "source": [
        "# effective_kernel_width = np.zeros(2400)\n",
        "# closeness_laplacian = np.zeros(2400)\n",
        "# for i in range(2400):\n",
        "#   d = np.inf\n",
        "#   for alpha in np.arange(.2, 1,.01):\n",
        "#     v1 = A[i,:]/A[i,i]\n",
        "#     v2 = np.exp(-B[i,:]/alpha)\n",
        "#     # plt.figure()\n",
        "#     # plt.plot(v1)\n",
        "#     # plt.plot(v2)\n",
        "#     if np.linalg.norm(v1-v2) < d:\n",
        "#       d = np.linalg.norm(v1-v2)\n",
        "#       effective_kernel_width[i] = alpha\n",
        "#       closeness_laplacian[i] = d/np.linalg.norm(v1)\n",
        "\n",
        "# plt.plot(effective_kernel_width)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG9UkwI-TRrV"
      },
      "outputs": [],
      "source": [
        "# plt.plot(closeness_laplacian)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyCxYYRwS_7m"
      },
      "outputs": [],
      "source": [
        "# plt.plot(effective_kernel_width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cnE_xxZJXs1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Epoch 10\n",
        "# -------------------------------\n",
        "# Train Loss: 0.253225 Batch:0 [    0/  640]\n",
        "# Train : \n",
        "#  Accuracy = 78.8, Loss =  0.346732\n",
        "# Test : \n",
        "#  Accuracy: 72.0%, Loss: 0.409975 \n",
        "\n",
        "# Epoch 11\n",
        "# -------------------------------\n",
        "# Train Loss: 0.371883 Batch:0 [    0/  640]\n",
        "# Train : \n",
        "#  Accuracy = 77.7, Loss =  0.345637\n",
        "# Epoch 12\n",
        "# -------------------------------\n",
        "# Train Loss: 0.362101 Batch:0 [    0/  640]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLhcSY7iKKrj"
      },
      "outputs": [],
      "source": [
        "# !zip -r resultall.zip /content/\n",
        "# files.download('resultall.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdri1UctAyjJ"
      },
      "outputs": [],
      "source": [
        "# def plot_kernels_all_modes(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False): \n",
        "#   instance = 100\n",
        "#   s_i = state_info_5_runs\n",
        "#   for run in range(1):\n",
        "#     for epoch in range(len(kernel_5_runs[run])):\n",
        "#       for step in range(len(kernel_5_runs[run][epoch])):\n",
        "#         for mode in range(num_modes):\n",
        "          \n",
        "#           curr_mode_kernels = kernel_5_runs[run][epoch][step][mode*instance : (mode+1)*instance][mode*instance : (mode+1)*instance]\n",
        "#           YKYs = get_YKYs(curr_mode_kernels)\n",
        "#           plot_heatmap(curr_mode_kernels,YKYs, s_i[run][epoch][step], all_input = for_all_inputs, save_fig = save_fig, show_fig = show_fig, mode = mode+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfQ3pG_xt5yy"
      },
      "outputs": [],
      "source": [
        "# acc_runs_test = []\n",
        "# acc_runs_train = []\n",
        "\n",
        "# for run in range(len(state_info_5_runs)):\n",
        "#   acc = []\n",
        "#   acc_tr = []\n",
        "#   for epoch in range(len(state_info_5_runs[run])):\n",
        "#     acc.append(state_info_5_runs[run][epoch][-1]['test_acc'])\n",
        "#     acc_tr.append(state_info_5_runs[run][epoch][-1]['train_acc'])\n",
        "#   acc_runs_test.append(max(acc))\n",
        "#   acc_runs_train.append(max(acc_tr))\n",
        "\n",
        "\n",
        "# acc_runs_train, acc_runs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuTeKnq_xWub"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mRC3dVwuXzi"
      },
      "outputs": [],
      "source": [
        "# max(acc), np.argmax(acc), len(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpoCD2aq23PO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# '''\n",
        "#  a := Number of half cycles in the top half of the circle\n",
        "#  b := number of half cycles in the bottom half of the circle\n",
        "#  For a, b = 1, 9:\n",
        "#   MLP takes 60 epochs\n",
        "#   DGN takes 130 epochs\n",
        "#   DLGN takes 80 epochs\n",
        "#   DLGN takes 70 epochs\n",
        "#   DLGN takes 70 epochs\n",
        "# '''\n",
        "# n_epochs = 200\n",
        "\n",
        "\n",
        "# step_stop_point = 2 \n",
        "# step_stepsize = 10\n",
        "# epoch_stop_point = 50\n",
        "# epoch_stepsize = 10\n",
        "# start_point = get_start_point(epoch_stop_point, epoch_stepsize)\n",
        "# # n_data_points = 500\n",
        "# # a, b = 1,9\n",
        "# # point1_degrees, point2_degrees, diff_in_degrees = 40, 300, 10\n",
        "# seed = 0\n",
        "# set_seed(seed = seed)\n",
        "\n",
        "# # train_dataloader, dataloader_all, Y_sorted = get_mnist_dls(batch_size = 64, is_perturbed = True)\n",
        "# train_dataloader, dataloader_all, Y_sorted = get_dsphere_dls(batch_size = 64, is_perturbed = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# n_hidden_layers,  n_neurons, n_runs =  5, 64, 5\n",
        "# save_result = True\n",
        "\n",
        "# #Available Protocols : MLP, DGN, DLGN, DLGN-SF, DGN-DLGN-SF\n",
        "# # protocols = ['MLP', \"DGN\", 'DLGN', 'DLGN-SF','DGN-DLGN-SF']\n",
        "# # protocols = ['DLGN', 'DLGN-SF','DGN-DLGN-SF']\n",
        "# protocols = ['MLP', 'DGN']\n",
        "\n",
        "# bias = True\n",
        "# for protocol in protocols:\n",
        "#   set_seed(seed = seed)\n",
        "#   if protocol == 'MLP':\n",
        "#     hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs = run_mlp_model(train_dataloader=train_dataloader,\n",
        "#                                                                                     dataloader_all = dataloader_all,\n",
        "#                                                                                     n_h_l = n_hidden_layers,\n",
        "#                                                                                     n_n = n_neurons,\n",
        "#                                                                                     n_runs = n_runs, n_epochs = n_epochs, bias = bias, is_mnist_data = True)\n",
        "#   else:\n",
        "#     hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs = run_npf_npv_model(train_dataloader=train_dataloader,\n",
        "#                                                                                         dataloader_all = dataloader_all,\n",
        "#                                                                                         protocol = protocol,\n",
        "#                                                                                         n_h_l = n_hidden_layers,\n",
        "#                                                                                         n_n = n_neurons,\n",
        "#                                                                                         n_runs = n_runs,\n",
        "#                                                                                         n_epochs = n_epochs, bias = bias)\n",
        "\n",
        "# kernel_5_runs = build_kernels(hidden_layer_outs_5_runs)\n",
        "# for kernel_name in ['K2', 'K3','K']:\n",
        "#   plot_kernel_values_stepwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "#   plot_kernel_values_epochwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "\n",
        "# # plot_stepwise_activations(hidden_layer_no =2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "# # plot_epochwise_activations(hidden_layer_no = 2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "# plot_stepwise_loss(state_info_5_runs)\n",
        "# plot_epochwise_loss(state_info_5_runs)\n",
        "# plot_kernels(hidden_layer_outs_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)\n",
        "\n",
        "# #Saving results of each protocol\n",
        "# if save_result:\n",
        "#   file_name = f'{protocol}_5_64_dsphere_all_epochs_figs'\n",
        "#   save_results(file_name)\n",
        "\n",
        "# #Download all protocols result\n",
        "# if save_result:\n",
        "#   download_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlvwtDngPoEM"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# pred = torch.argmax(nn.Softmax(dim = 1)(predictions_5_runs[0][-1][-1]), dim = 1).detach().to('cpu')\n",
        "# def get_predictions(predictions_5_runs):\n",
        "#   predictions = []\n",
        "#   for run in range(1):\n",
        "#     for epoch in range(len(predictions_5_runs[run])):\n",
        "#       for step in range(len(predictions_5_runs[run][epoch])):\n",
        "#         pred = torch.argmax(nn.Softmax(dim = 1)(predictions_5_runs[run][epoch][step]), dim = 1).detach().to('cpu')\n",
        "#         print(pred)\n",
        "#         predictions.append(pred)\n",
        "#   return predictions\n",
        "# preds = get_predictions(predictions_5_runs)\n",
        "# ConfusionMatrixDisplay.from_predictions(Y_sorted, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDuROlYUvNII"
      },
      "outputs": [],
      "source": [
        "#Slider for visulalizing Y_true v/s Y_predicted\n",
        "# preds = [v for x in predictions_5_runs[0] for v in x]\n",
        "# plot_y_prediction(Y_sorted, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuyw3HYsWA4C"
      },
      "outputs": [],
      "source": [
        "# #Store in pickle file\n",
        "# temp_dict = {'hidden_layer_outs':hidden_layer_outs_5_runs, 'state_info':state_info_5_runs}\n",
        "# info = state_info_5_runs[0][0][0]\n",
        "# fname = f'{info['model_protocol_type']}_{info['n_hidden_layers']}_{info['n_neurons']}_pkl.pkl'\n",
        "# with open(fname, \"wb\") as fp:   \n",
        "#   pickle.dump(temp_dict, fp)\n",
        "\n",
        "\n",
        "# os.system(f\"zip -R {fname}.zip '*.pkl'\")\n",
        "# # !zip -r /content/DGN_DLGN_SF_5_32_all_epochs_pkl.zip /content/DGN_DLGN_SF_5_32_all_epochs_pkl\n",
        "# files.download(f\"{fname}.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZISI7e5Q2MP"
      },
      "outputs": [],
      "source": [
        "# # !unzip /content/MLP_5_32_all_epochs_pkl.zip \n",
        "# with open(\"/content/content/MLP_5_32_all_epochs_pkl\", \"rb\") as fp:  \n",
        "#   b = pickle.load(fp)\n",
        "# kernel_5_runs = b['kernels']\n",
        "# state_info_5_runs = b['state_info']\n",
        "# intermediate_outs_5_runs = b['hidden_layer_outs']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQBqtlhzJ_o"
      },
      "source": [
        "----------------------------------------------------------- Commented Old Codes -----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FjPk7HKBtH4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "[ 4629  1771 50630 50628 43602 51228  5368 30384  4962 15825 30044 48554\n",
        " 32609 16326 45905 23372  2539  1386 39029 40320  1000 32574 48780 38177\n",
        " 39155 15288 36412   283 10254 54404  7409  2084   119 55785 18278 10323\n",
        " 57827 37220 24069 51629 33302 25325 43727 38055   489 17871 31476 40241\n",
        " 33640 28904]\n",
        "[46518 11637 53341 18546 51882 52868  5477  2907 21554 39444 45197 19742\n",
        " 23554  6356 50838 56291 33577 46936 53531 11901 33440 17662 14293 23885\n",
        " 59530 30117 36687 27967  3532  8873 57192 37155  4772 44705 26429 41524\n",
        " 25940 24288 39622 13725 49035 37635 56491 45182 53001 34033 55338  3120\n",
        " 46254 11793]\n",
        "[20028 57933  8960 13922 41489 37734 53237   917 16309 10988 55589 37084\n",
        "  8993 23171 19028 28832 39228 13774 47839 30975 30346 26794 39467 53837\n",
        " 48624 43153 34155  8598  8460 57966 55528 48946 55381  1715  2418  3965\n",
        " 43851  5624 52456 45505  3989 14063 21219 26164 50478 55001 16142 32485\n",
        " 25700 29911]\n",
        "[ 6118   495 51837 42120 49529 22234 56297 46525 25377 24356 37729 21325\n",
        " 15872  6539  2286 58878 24042 38894 18588 31249 20356 25291 18380  4001\n",
        " 46971 41629 44385 37913 51260 51776  6719   629  7625  4864 18044 58412\n",
        "  9730 29023 38086 34427 28174 52939  3670 29225 59652 30417 16074 30489\n",
        " 56721 15226]\n",
        "[11008 58650 10560 28253 52522 19033 19032 57443 55552 43481 55004  4781\n",
        " 39239 35624 13775 21590  3350  4768 39894  3903  7863 52988 23538 23103\n",
        " 55024 54819 31458 16802 33165 29941 16338 44523 35904 44213 18846 57537\n",
        " 11471 34389 40519  7327  3990 58225 29814 46052 55654 42346 35926 34465\n",
        " 35141 47680]\n",
        "[49048 19839 30390 36480 18416 24630 41533 42889 32501 18234 59822  6997\n",
        " 52364 57345 49820 52540 51254 59216 22101 47854 19706 47940  5456 13863\n",
        " 32570 42447 12484  7232  5292 41521   376 25295 34841   836 44819 55455\n",
        " 47320 26290 46774 25508    11 41693  4004 10904 54400 25407 29029 24887\n",
        " 57837 23183]\n",
        "[17626 46003 35099 47006 30950 19293 11132 47415 28883 26347 11871 55456\n",
        " 31606 48823 23421 49043 40491  7008 28934 19118 21025 16746 15051  9929\n",
        "  9120  6789 16957 33751 58724  7504 34698 36637 33692 48380  2320 59591\n",
        " 42027 51481 47864 43225 53081 16736 37966 34621 44742 37724 21003 19451\n",
        " 59866 43141]\n",
        "[52879 17703 24705  6221 59124  9992 54426 28723 54494  4217 34981 20370\n",
        " 28117  3899 34472 38243 30952 55431 54207 45533  7961 13255 14455 43277\n",
        " 27292 23621 10427 14076   968 11235  8828  6956 58977 42057 17443 58524\n",
        " 16771 42016 45530 36691 35539 35321 37971 49553 18498 15098 50742 21437\n",
        " 26560  9768]\n",
        "[35428 43372 22849 27508  7026 33018 59542 56613 16937 50413  6006 37030\n",
        " 53420   386 32573 20690 51412 49963 10738 58849 34820  8732 55761 59097\n",
        "  9008 38267 32166 51513 19557 58643 21694 25796 50346 20725  5394 57244\n",
        " 46793  6129 56284 44944 47030 16158 30067 12455 24110 54846 40453 58679\n",
        " 55929 32175]\n",
        "[46655 30085 11900 33199 22782 43596 51654 37115 50029  8899  2997 18731\n",
        "  3904 50558 28168 15222 49734 37400 45944 58796 33891 10799 11366  3598\n",
        " 11499 11388 21724 48580 52184 43888 55849 23732 42773 26877 16006 39932\n",
        " 55454  3316 37306 35741 53280  7053 34616  9394 28750 33701  9969 45787\n",
        " 21592 41953]\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dFQZtmAZs3"
      },
      "outputs": [],
      "source": [
        "# #activation for epoch wise for MLP\n",
        "# def plot_epochwise_activations():\n",
        "#   if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "#     temp = []\n",
        "#     y_label = 'ReLU(x)' if state_info_5_runs[0][0][0]['model_protocol_type'] == 'MLP' else 'ReLU(x)*G(x)'\n",
        "#     for i in range(5):\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
        "#           121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
        "#           134, 135, 136, 137, 138, 139, 140, 141, 142]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "    \n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()\n",
        "#   #activation for epoch wise for DGN\n",
        "#   else:\n",
        "#     temp = []\n",
        "#     x = np.concatenate((np.arange(31), np.arange(31,250,5)))\n",
        "#     for i in range(5):\n",
        "#     #   if i ==0:\n",
        "#     #     temp = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     #   else:\n",
        "#     #     temp += np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     # temp /= 5\n",
        "#       # intermediate_outs = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "#       y_label = 'ReLU(x)*G(x)'\n",
        "\n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(x,plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(x, plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(x, plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(x, plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPiXewrmpPjU"
      },
      "outputs": [],
      "source": [
        "# #activation for epoch wise for MLP\n",
        "# def plot_epochwise_activations():\n",
        "#   if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "#     temp = []\n",
        "#     y_label = 'ReLU(x)' if state_info_5_runs[0][0][0]['model_protocol_type'] == 'MLP' else 'ReLU(x)*G(x)'\n",
        "#     for i in range(5):\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
        "#           121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
        "#           134, 135, 136, 137, 138, 139, 140, 141, 142]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "    \n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()\n",
        "#   #activation for epoch wise for DGN\n",
        "#   else:\n",
        "#     temp = []\n",
        "#     x = np.concatenate((np.arange(31), np.arange(31,250,5)))\n",
        "#     for i in range(5):\n",
        "#     #   if i ==0:\n",
        "#     #     temp = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     #   else:\n",
        "#     #     temp += np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     # temp /= 5\n",
        "#       # intermediate_outs = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "#       y_label = 'ReLU(x)*G(x)'\n",
        "\n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(x,plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(x, plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(x, plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(x, plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9NK-t0fizc_"
      },
      "outputs": [],
      "source": [
        "# # For epoch wise loss analysis\n",
        "# def plot_epochwise_loss():\n",
        "#   values = []\n",
        "#   for run in range(len(kernel_5_runs)):\n",
        "#     for epoch in range(len(kernel_5_runs[run])):\n",
        "#         loss = state_info_5_runs[run][epoch][-1]['loss']\n",
        "#         values.append(loss)\n",
        "        \n",
        "#         # print(run, epoch)\n",
        "#         # plot_heatmap(kernel_5_runs[run][epoch][step], state_info_5_runs[run][epoch][step])\n",
        "      \n",
        "#   values = np.array(values)\n",
        "\n",
        "#   #For loss v/s epoch for MLP\n",
        "#   if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "#     f, axes = plt.subplots(1, 5,figsize = (5*10,10))\n",
        "#     for run in range(5):\n",
        "#       plt.suptitle('Loss v/s epoch',ha = 'center', fontsize = 30)\n",
        "#       for i in range(len(values)):\n",
        "#         one_run_len = int(values.shape[0]/5)\n",
        "#         start = run*one_run_len\n",
        "#         axes[run].plot(values[start:start+one_run_len])\n",
        "#         axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "#         axes[run].set_xticks(np.arange(0,one_run_len, 5.0))\n",
        "\n",
        "#     plt.savefig('loss_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#     plt.show()\n",
        "\n",
        "#   # For loss v/s epoch for DGN \n",
        "#   else:\n",
        "#     f, axes = plt.subplots(1, 5,figsize = (5*10,10))\n",
        "#     x = np.concatenate((np.arange(31), np.arange(31,250,5)))  \n",
        "#     for run in range(5):\n",
        "#       plt.suptitle('Loss v/s epoch',ha = 'center', fontsize = 30)\n",
        "#       for i in range(len(values)):\n",
        "#         one_run_len = int(values.shape[0]/5)\n",
        "#         start = run*one_run_len\n",
        "#         axes[run].plot(x, values[start:start+one_run_len])\n",
        "#         axes[run].set_xticks(np.arange(min(x), max(x)+1, 10.0))\n",
        "#         axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "#     plt.savefig('loss_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wffd27fzQbnu"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "  # Kernel value graphs epoch wise for MLP\n",
        "  if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "    labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "    f, axes = plt.subplots(5, 1,figsize = (20,7*5))\n",
        "    for run in range(5):\n",
        "      # plt.figure(figsize = (10,10))\n",
        "      plt.suptitle(f'{kernel_name}_epoch',ha = 'center', fontsize = 30)\n",
        "      for i in range(len(values)):\n",
        "        one_run_len = int(values.shape[1]/5)\n",
        "        start = run*one_run_len\n",
        "        axes[run].plot(values[i][start:start+one_run_len], label=labels[i])\n",
        "        axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "        axes[run].legend(loc='best')\n",
        "\n",
        "    plt.savefig(f'{kernel_name}_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "\n",
        "  #Kernel value graphs epoch wise for DGN since it is plotted in interval of 5 after 30 epochs\n",
        "  else:\n",
        "    labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "    f, axes = plt.subplots(5, 1,figsize = (20,7*5))\n",
        "    x = np.concatenate((np.arange(31), np.arange(31,250,5)))  \n",
        "    for run in range(5):\n",
        "      plt.suptitle(f'{kernel_name}_epoch',ha = 'center', fontsize = 30)\n",
        "      for i in range(len(values)):\n",
        "        one_run_len = int(values.shape[1]/5)\n",
        "        start = run*one_run_len\n",
        "        axes[run].plot(x, values[i][start:start+one_run_len], label=labels[i])\n",
        "        axes[run].set_xticks(np.arange(min(x), max(x)+1, 10.0))\n",
        "        axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "        axes[run].legend(loc='best')\n",
        "\n",
        "    plt.savefig(f'{kernel_name}_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oFhwInbQy01"
      },
      "outputs": [],
      "source": [
        "# import numpy as np \n",
        "# import plotly.graph_objs as go\n",
        "# from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
        "\n",
        "# #from  https://stackoverflow.com/questions/47230817/plotly-notebook-mode-with-google-colaboratory\n",
        "# def configure_plotly_browser_state():\n",
        "#   import IPython\n",
        "#   display(IPython.core.display.HTML('''\n",
        "#         <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "#         <script>\n",
        "#           requirejs.config({\n",
        "#             paths: {\n",
        "#               base: '/static/base',\n",
        "#               plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', \n",
        "#             },\n",
        "#           });\n",
        "#         </script>\n",
        "#         '''))\n",
        "\n",
        "\n",
        "# init_notebook_mode(connected=True)\n",
        "\n",
        "# #from plotly sliders\n",
        "# data = [dict(\n",
        "#         visible = False,\n",
        "#         line=dict(color='#00CED1', width=2),\n",
        "#         name = ' = '+str(step),\n",
        "#         x = np.arange(0,500,1),\n",
        "#         y = d.to('cpu').flatten(),\n",
        "#         #  y =Y_sorted.flatten()\n",
        "#        ) for d in predictions_run]\n",
        "# data[10]['visible'] = True\n",
        "\n",
        "# data2 = [dict(\n",
        "#         visible = False,\n",
        "#         line=dict(color='#00CED1', width=2),\n",
        "#         name = ' = '+str(step),\n",
        "#         x = np.arange(0,500,1),\n",
        "#         y = Y_sorted.flatten(),\n",
        "#        ) for d in predictions_run]\n",
        "# data[10]['visible'] = True\n",
        "\n",
        "# #configure added to visualize in colab\n",
        "# configure_plotly_browser_state()\n",
        "\n",
        "# steps = []\n",
        "# for i in range(len(data)):\n",
        "#     step = dict(\n",
        "#         method = 'restyle',  \n",
        "#         args = ['visible', [False] * len(data)],\n",
        "#     )\n",
        "#     step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
        "#     steps.append(step)\n",
        "\n",
        "# sliders = [dict(\n",
        "#     active = 10,\n",
        "#     currentvalue = {\"prefix\": \"Frequency: \"},\n",
        "#     pad = {\"t\": 50},\n",
        "#     steps = steps\n",
        "# )]\n",
        "\n",
        "# layout = dict(sliders=sliders)\n",
        "\n",
        "# fig = dict(data=data, layout=layout)\n",
        "\n",
        "# fig2 = dict(data=data2, layout=layout)\n",
        "# configure_plotly_browser_state()\n",
        "\n",
        "\n",
        "# #removed py. from original plotly code \n",
        "# iplot(fig, filename='Sine Wave Slider')\n",
        "# iplot(fig2, filename='Sine Wave Slider')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxeoMCVWUe0Y"
      },
      "outputs": [],
      "source": [
        "# output = pd.DataFrame(state_info)\n",
        "# sub_out = pd.DataFrame(list(output[4]))\n",
        "\n",
        "# sub_out = sub_out.fillna(value='Null')\n",
        "# cols = sub_out.ne('Null').cumsum(axis=1).idxmax(axis=1)\n",
        "# K_temp = sub_out.lookup(sub_out.index, cols)\n",
        "\n",
        "# cols = list(cols)\n",
        "\n",
        "# for i, row in sub_out.iterrows():\n",
        "#   sub_out.at[i,cols[i]] = 'Null'\n",
        "# sub_out.insert(0, 'K', K_temp)\n",
        "\n",
        "# final_sub_out = sub_out\n",
        "# final_output = pd.concat([output, final_sub_out], axis = 1, ignore_index=True)\n",
        "\n",
        "\n",
        "# final_output = final_output.drop([4, final_output.shape[1]-1], 1)\n",
        "# final_output\n",
        "\n",
        "#final_output = final_output.drop([4, -1], 1)\n",
        "# final_output = final_output.set_axis(['protocol', 'n_h_l', 'n_n','Run', 'Status', 'K', 'K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8'], axis=1, inplace=False)\n",
        "# final_output = final_output.set_axis(['protocol', 'n_h_l', 'n_n','Run', 'Status', 'K', 'K1', 'K2', 'K3', 'K4', 'K5'], axis=1, inplace=False)\n",
        "\n",
        "# final_output.to_csv('final_output.csv')\n",
        "# files.download(\"final_output.csv\")\n",
        "# dF1 = pd.read_csv(\"final_output1.csv\")\n",
        "# dF2 = pd.read_csv(\"final_output2.csv\")\n",
        "# dF3 = pd.read_csv(\"final_output3.csv\")\n",
        "# dF4 = pd.read_csv(\"final_output4.csv\")\n",
        "# dF5 = pd.read_csv(\"final_output5.csv\")\n",
        "# dF6 = pd.read_csv(\"final_output6.csv\")\n",
        "# sin10_all_out = pd.concat([dF6, dF1,dF3, dF4,dF5, dF2], axis=0).drop(['Unnamed: 0'], axis = 1)\n",
        "# dF7 = pd.read_csv(\"final_output7.csv\")\n",
        "# dF8 = pd.read_csv(\"final_output8.csv\")\n",
        "# sin4_all_out = pd.concat([dF7, dF8], axis=0).drop(['Unnamed: 0'], axis = 1)\n",
        "# sin10_all_out.to_csv('sin10_all_output.csv')\n",
        "# sin4_all_out.to_csv('sin4_all_output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMnnenOPBely"
      },
      "outputs": [],
      "source": [
        "# def  get_hyperplanes_params(model, for_layers):\n",
        "#   layer_weights = []\n",
        "#   layer_slopes = []\n",
        "#   layer_intercepts = []\n",
        "#   for i in range(for_layers):\n",
        "#     layer_weight = model.layers[i].weight.detach().to(\"cpu\")\n",
        "#     layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "#     layer_intercept = [0]*32#model.layers[i].bias.detach().to(\"cpu\")\n",
        "\n",
        "#     layer_weights.append(layer_weight)\n",
        "#     layer_slopes.append(layer_slope)\n",
        "#     layer_intercepts.append(layer_intercept)\n",
        "#   return layer_slopes, layer_intercepts\n",
        "\n",
        "\n",
        "# def plot_hyperplanes(model, train_status):\n",
        "  \n",
        "#   for_layers = len(model.layers)-1\n",
        "#   layer_slopes, layer_intercepts = get_hyperplanes_params(model, for_layers)\n",
        "  \n",
        "  \n",
        "#   x_axis = np.linspace(start = -2, stop = 2)\n",
        "#   for i in range(for_layers):\n",
        "#     plt.figure(figsize = (10,10))\n",
        "#     #fig, ax = plt.subplots(figsize = (10,10))\n",
        "#     ax = sns.scatterplot(X_sorted[:,0], X_sorted[:,1], c = prediction.to(\"cpu\"))\n",
        "#     ax.set_xlim((-2,2))\n",
        "#     ax.set_ylim((-2,2))\n",
        "#     for j in range(32):\n",
        "#       y_axis = layer_slopes[i][j]*x_axis + layer_intercepts[i][j]\n",
        "#       #plt.plot(x_axis, y_axis)\n",
        "#       sns.lineplot(x = x_axis, y = y_axis)\n",
        "#     plt.title(train_status+\" Layer_\"+str(i+1))\n",
        "#     plt.savefig(train_status+\" Layer_\"+str(i+1))\n",
        "#     plt.show()\n",
        "\n",
        "#     plot_hyperplanes(npf_model, model_protocol_type+\" \"+model_learning_status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSE8YdjH1yo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_Tcijr0Lq35P",
        "zC85OWcNoA8F",
        "pHzf9mTzrLYm",
        "-ITAB5txo6F8",
        "4M72vt8AmiUl",
        "XbqVgGWLnG1X",
        "3NYTooMfn1fG",
        "L3cYcHIMnxMF"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPT1/ORschbucV/om6SOhez",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "686a44e2edd148e596f3d2836bd5b88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95c8d1ea770c4a8a9578cc3a15a2ec84",
              "IPY_MODEL_197e23508eef4c37a67229d0e81edb3b",
              "IPY_MODEL_2357a0131edb45a2aa4a05564695f737"
            ],
            "layout": "IPY_MODEL_78cc0b5462674ec9aabab915143155f9"
          }
        },
        "95c8d1ea770c4a8a9578cc3a15a2ec84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efaea7d7c227497a903f3d7463816c99",
            "placeholder": "",
            "style": "IPY_MODEL_9ba16719fd0744818bab3f00697e23ef",
            "value": "100%"
          }
        },
        "197e23508eef4c37a67229d0e81edb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecba25841d9a4671805468c875ce058c",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eb5c4d2627a4385b3da33dad19e0ae5",
            "value": 170498071
          }
        },
        "2357a0131edb45a2aa4a05564695f737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9859e14274d64910a150cc5599f1572d",
            "placeholder": "",
            "style": "IPY_MODEL_1e186330d1414d789532c5f834a0beca",
            "value": " 170498071/170498071 [00:02&lt;00:00, 99324910.82it/s]"
          }
        },
        "78cc0b5462674ec9aabab915143155f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efaea7d7c227497a903f3d7463816c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba16719fd0744818bab3f00697e23ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecba25841d9a4671805468c875ce058c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eb5c4d2627a4385b3da33dad19e0ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9859e14274d64910a150cc5599f1572d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e186330d1414d789532c5f834a0beca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}