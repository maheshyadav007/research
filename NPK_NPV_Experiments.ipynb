{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NPK_NPV_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7Yz1IWBImFC3U5hGN+PSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshyadav007/research/blob/main/NPK_NPV_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCTVkM0Ndkgh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras.backend as K\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.linalg import qr\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0glYLmcd0yb"
      },
      "source": [
        "#X, Y = shuffle(X,Y)\n",
        "def split_data(X,Y):\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "\n",
        "  x_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.9)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "  x_val = np.array(x_val)\n",
        "  y_val = np.array(y_val)\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "KUrnJQpod7XM",
        "outputId": "01e81de5-6ead-4202-cc6a-dc58ec592715"
      },
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "# plot first few images\n",
        "for i in range(9):\n",
        "\t# define subplot\n",
        "\tpyplot.subplot(330 + 1 + i)\n",
        "\t# plot raw pixel data\n",
        "\tpyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "11501568/11490434 [==============================] - 1s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLH8jOesgXwJ"
      },
      "source": [
        "x_train, y_train, x_val, y_val = split_data(trainX,trainy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8cjn9VAfDdv",
        "outputId": "db5f4b80-c7a4-445b-ee0d-b66631d36249"
      },
      "source": [
        "epoch = 10\n",
        "model = Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=(28,28)))\n",
        "model.add(keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(128, activation=\"relu\", use_bias= False))\n",
        "model.add(keras.layers.Dense(128, activation=\"relu\", use_bias= False))\n",
        "model.add(keras.layers.Dense(128, activation=\"relu\", use_bias= False))\n",
        "model.add(keras.layers.Dense(128, activation=\"relu\", use_bias= False))\n",
        "model.add(keras.layers.Dense(128, activation=\"relu\", use_bias= False))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\", use_bias= False))\n",
        "model.compile(optimizer=keras.optimizers.SGD(learning_rate=.1),loss=keras.losses.sparse_categorical_crossentropy, metrics = [\"accuracy\"])\n",
        "model.fit(x_train, y_train,batch_size=32, epochs=epoch, validation_data=(x_val,y_val))#,callbacks=[CustomCallback()])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 7s 3ms/step - loss: 0.3367 - accuracy: 0.8960 - val_loss: 0.1695 - val_accuracy: 0.9505\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1265 - accuracy: 0.9622 - val_loss: 0.1449 - val_accuracy: 0.9560\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0910 - accuracy: 0.9726 - val_loss: 0.1223 - val_accuracy: 0.9618\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0705 - accuracy: 0.9778 - val_loss: 0.1696 - val_accuracy: 0.9527\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0566 - accuracy: 0.9824 - val_loss: 0.0994 - val_accuracy: 0.9730\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0461 - accuracy: 0.9856 - val_loss: 0.1602 - val_accuracy: 0.9577\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0387 - accuracy: 0.9879 - val_loss: 0.0927 - val_accuracy: 0.9770\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0327 - accuracy: 0.9895 - val_loss: 0.0921 - val_accuracy: 0.9757\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.1353 - val_accuracy: 0.9695\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0267 - accuracy: 0.9915 - val_loss: 0.1025 - val_accuracy: 0.9747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb356691ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tob3Znc2sq2c"
      },
      "source": [
        "npk_extractor = keras.Model(inputs=model.inputs,outputs=[layer.output for layer in model.layers])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMG7TcZltCoR"
      },
      "source": [
        "npk_features = npk_extractor(x_train)\n",
        "npk_features = npk_features[2:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evuF0KaPfP-0",
        "outputId": "0c63298a-bf66-406f-8567-15f2387ab377"
      },
      "source": [
        "npk_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(54000, 128), dtype=float32, numpy=\n",
              " array([[0.0000000e+00, 0.0000000e+00, 2.8022462e-01, ..., 0.0000000e+00,\n",
              "         0.0000000e+00, 5.9240955e-01],\n",
              "        [0.0000000e+00, 0.0000000e+00, 2.5003896e+00, ..., 3.2521172e+00,\n",
              "         0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 3.0209930e+00, 0.0000000e+00, ..., 2.4982542e-03,\n",
              "         0.0000000e+00, 0.0000000e+00],\n",
              "        ...,\n",
              "        [2.8580117e-01, 0.0000000e+00, 2.9952648e+00, ..., 2.8832569e+00,\n",
              "         0.0000000e+00, 0.0000000e+00],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.6258442e+00,\n",
              "         0.0000000e+00, 9.4742596e-01],\n",
              "        [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "         0.0000000e+00, 0.0000000e+00]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(54000, 128), dtype=float32, numpy=\n",
              " array([[0.        , 0.        , 0.        , ..., 1.6029418 , 2.0050745 ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.88593477, 0.        , 0.        , ..., 0.        , 0.12404643,\n",
              "         1.5161446 ],\n",
              "        ...,\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.29300308, ..., 1.1367875 , 1.1626778 ,\n",
              "         0.19025198],\n",
              "        [0.        , 0.        , 1.822887  , ..., 0.        , 0.        ,\n",
              "         0.        ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(54000, 128), dtype=float32, numpy=\n",
              " array([[0.        , 0.94934464, 0.        , ..., 0.        , 0.49948695,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 1.4447603 , ..., 4.1691103 , 0.        ,\n",
              "         0.7337773 ],\n",
              "        ...,\n",
              "        [0.        , 0.41817534, 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.14881623,\n",
              "         0.        ],\n",
              "        [0.        , 1.7473117 , 2.033675  , ..., 0.        , 1.1980301 ,\n",
              "         2.151366  ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(54000, 128), dtype=float32, numpy=\n",
              " array([[0.        , 0.25065416, 0.        , ..., 0.17179143, 0.49589914,\n",
              "         0.        ],\n",
              "        [0.        , 2.398297  , 0.3495168 , ..., 0.49262542, 1.1101896 ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.6742685 , ..., 0.5861953 , 0.        ,\n",
              "         0.        ],\n",
              "        ...,\n",
              "        [0.        , 2.5463872 , 0.21345237, ..., 0.24265763, 1.2047575 ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.44371527, 0.7913042 ,\n",
              "         0.        ],\n",
              "        [0.12913533, 0.        , 0.        , ..., 0.        , 1.458895  ,\n",
              "         0.        ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(54000, 128), dtype=float32, numpy=\n",
              " array([[0.47721982, 0.14154601, 0.1012218 , ..., 0.        , 0.        ,\n",
              "         2.7351968 ],\n",
              "        [1.2095158 , 0.        , 0.        , ..., 0.6510773 , 0.        ,\n",
              "         1.2828274 ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.9721122 ],\n",
              "        ...,\n",
              "        [1.1736895 , 0.        , 0.        , ..., 0.5591916 , 0.        ,\n",
              "         0.47177   ],\n",
              "        [0.22103328, 0.        , 0.0961628 , ..., 0.404485  , 0.        ,\n",
              "         1.4160597 ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 4.6346817 ,\n",
              "         1.690561  ]], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpggfKYWs1XK"
      },
      "source": [
        "keras.utils.plot_model(, show_shapes = True, expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mshSmdGp4ZQO"
      },
      "source": [
        "x_dummy_train = np.ones(shape = (x_train.shape[0], 28,28))\n",
        "x_dummy_val = np.ones(shape = (x_val.shape[0], 28, 28))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdv_8RW5LElg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdTTOAR0oSYo"
      },
      "source": [
        "\n",
        "class GaLU(keras.layers.Layer):\n",
        "    def __init__(self, units=128, layer_n = 0, batch_size= 32):\n",
        "        super(GaLU, self).__init__()\n",
        "        self.units = units\n",
        "        self.start = -54\n",
        "        self.batch_size = batch_size\n",
        "        self.end = self.start + self.batch_size\n",
        "        self.layer_n = layer_n\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      pass\n",
        "      \n",
        "    def call(self, inputs):\n",
        "      print( self.start,self.end, inputs.shape)\n",
        "      out = tf.math.multiply(inputs,tf.math.sign(npk_features[self.layer_n][self.start:self.end]))\n",
        "     \n",
        "      self.start = self.end\n",
        "      if self.start==54000:\n",
        "        self.start = 0\n",
        "      self.end = self.start + self.batch_size\n",
        "      if self.end >= 54000:\n",
        "        self.end = 54000\n",
        "      \n",
        "     \n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr0REPP9Dnsx"
      },
      "source": [
        "start = 0\n",
        "batch_size = 32\n",
        "end = start + batch_size\n",
        "counter = 0\n",
        "def custom_activation(i,x):\n",
        "  global start\n",
        "  global end\n",
        "  global batch_size\n",
        "  global counter\n",
        "  print(i, start,end, x.shape)\n",
        "  \n",
        "  out = tf.math.multiply(x,tf.math.sign(npk_features[i][start:end]))\n",
        "    \n",
        "  if counter >= 5:\n",
        "    start = end % 54000\n",
        "    end = start + batch_size\n",
        "    if end > 54000:\n",
        "      end = 54000\n",
        "\n",
        "  counter += 1\n",
        "  #i += 1\n",
        "  return out\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPXro9poAOjm"
      },
      "source": [
        "def custom_relu(x):\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQYxv5UHs0pK"
      },
      "source": [
        "\n",
        "epoch = 10\n",
        "#model.add(keras.layers.InputLayer(input_shape=(784,)))\n",
        "\n",
        "inputs = keras.layers.Input(shape=(28,28))\n",
        "x = keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255)(inputs)\n",
        "x = keras.layers.Flatten()(x)\n",
        "\n",
        "x = keras.layers.Dense(128, use_bias= False)(x)\n",
        "x = custom_activation(0,x)\n",
        "x = keras.layers.Lambda(custom_relu)(x)\n",
        "x = keras.layers.Dense(128, use_bias= False)(x)\n",
        "x = custom_activation(1,x)\n",
        "x = keras.layers.Lambda(custom_relu)(x)\n",
        "x = keras.layers.Dense(128, use_bias= False)(x)\n",
        "x = custom_activation(2,x)\n",
        "x = keras.layers.Lambda(custom_relu)(x)\n",
        "x = keras.layers.Dense(128, use_bias= False)(x)\n",
        "x = custom_activation(3,x)\n",
        "x = keras.layers.Lambda(custom_relu)(x)\n",
        "x = keras.layers.Dense(128, use_bias= False)(x)\n",
        "x = custom_activation(4,x)\n",
        "x = keras.layers.Lambda(custom_relu)(x)\n",
        "\n",
        "outputs = keras.layers.Dense(10, activation = \"softmax\", use_bias = False)(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "'''\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(GaLU(128, 0, batch_size))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(GaLU(128, 1, batch_size))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(GaLU(128, 2, batch_size))f\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(GaLU(128, 3, batch_size))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(GaLU(128, 4, batch_size))\n",
        "'''\n",
        "'''\n",
        "\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(keras.layers.Activation(custom_activation))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(keras.layers.Activation(custom_activation))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(keras.layers.Activation(custom_activation))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(keras.layers.Activation(custom_activation))\n",
        "model.add(keras.layers.Dense(128, use_bias= False))\n",
        "model.add(keras.layers.Activation(custom_activation))\n",
        "'''\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.SGD(learning_rate=.01),loss=keras.losses.sparse_categorical_crossentropy, metrics = [\"accuracy\"])\n",
        "model.fit(x_train, y_train,batch_size=batch_size, epochs=epoch)#,callbacks=[CustomCallback()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj62eOixN_29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6aabd2d-3fa1-4996-f635-ba121720e681"
      },
      "source": [
        "npv_extractor = keras.Model(inputs=model.inputs,outputs=[layer.output for layer in model.layers])\n",
        "npv_features = npv_extractor(x_train[:54])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 162 216 (54, 128)\n",
            "1 162 216 (54, 128)\n",
            "2 162 216 (54, 128)\n",
            "3 162 216 (54, 128)\n",
            "4 162 216 (54, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlQXWVnWYVjf"
      },
      "source": [
        "npv_features  = npv_features[2:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLOvaOwWg4sJ"
      },
      "source": [
        "keras.utils.plot_model(npv_extractor, show_shapes = True, expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u72fC11DUK5I"
      },
      "source": [
        "npv_features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py0s_6DyDS1l"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOzdnzqZHK9w",
        "outputId": "e548683c-240e-4c50-db24-f34054bc8c9e"
      },
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "RYfYr72nKFGX",
        "outputId": "b43c5797-f430-4c50-a106-2ec5f394578c"
      },
      "source": [
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "counter = 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[counter]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    counter += 1\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNGbufnZIk6v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7C9fpa-IyDY"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        #print(X.shape,y.shape)\n",
        "        # Compute prediction error\n",
        "        pred, _ = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch%100 == 0:\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "codFdUHmI0Y5"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred,_ = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXjFoZO-3HAg"
      },
      "source": [
        "def get_activation_output(dataloader, model):\n",
        "    gate = []\n",
        "    count = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            _,gate  = model(X)\n",
        "            \n",
        "            '''\n",
        "            if count >= 1:\n",
        "              gate = np.vstack((gate,g))\n",
        "              \n",
        "            else:\n",
        "              gate = np.reshape(g, shape= (1,5))\n",
        "              count += 1\n",
        "           '''\n",
        "    return gate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NagMLyIsfI7u",
        "outputId": "2d5809d2-17f7-4e72-b6a6-dafbb035451a"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "class NPKNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NPKNeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, 128)\n",
        "        self.fc5 = nn.Linear(128, 128)\n",
        "        self.fc6 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        o1 = torch.relu(x)\n",
        "        x = self.fc2(o1)\n",
        "        o2 = torch.relu(x)\n",
        "        x = self.fc3(o2)\n",
        "        o3 = torch.relu(x)\n",
        "        x = self.fc4(o3)\n",
        "        o4 = torch.relu(x)\n",
        "        x = self.fc5(o4)\n",
        "        o5 = torch.relu(x)\n",
        "        x = self.fc6(o5)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x, [o1,o2,o3,o4,o5]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUzRJzVQH93h",
        "outputId": "908cb78a-fbdf-400d-e1f1-0624a964ce3a"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, 128)\n",
        "        self.fc5 = nn.Linear(128, 128)\n",
        "        self.fc6 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        o1 = custom_relu(0,x, )\n",
        "        x = self.fc2(o1)\n",
        "        o2 = custom_relu(1,x)\n",
        "        x = self.fc3(o2)\n",
        "        o3 = custom_relu(2,x)\n",
        "        x = self.fc4(o3)\n",
        "        o4 = custom_relu(3,x)\n",
        "        x = self.fc5(o4)\n",
        "        o5 = custom_relu(4,x)\n",
        "        x = self.fc6(o5)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x,  [o1,o2,o3,o4,o5]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2vt5vwqCMhw"
      },
      "source": [
        "batch_size = 32\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "train_dataloader_all = DataLoader(training_data, batch_size=60000, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpA_IBHPI2dQ",
        "outputId": "2ccf20af-088b-44d5-a782-e70e7a796902"
      },
      "source": [
        "npk_model = NPKNeuralNetwork().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(npk_model.parameters(), lr=1e-1)\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, npk_model, loss_fn, optimizer)\n",
        "    test(test_dataloader, npk_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303520 Batch:0 [    0/60000]\n",
            "loss: 2.300847 Batch:100 [ 3200/60000]\n",
            "loss: 2.301060 Batch:200 [ 6400/60000]\n",
            "loss: 2.300098 Batch:300 [ 9600/60000]\n",
            "loss: 2.302687 Batch:400 [12800/60000]\n",
            "loss: 2.304715 Batch:500 [16000/60000]\n",
            "loss: 2.302285 Batch:600 [19200/60000]\n",
            "loss: 2.297561 Batch:700 [22400/60000]\n",
            "loss: 2.300750 Batch:800 [25600/60000]\n",
            "loss: 2.280433 Batch:900 [28800/60000]\n",
            "loss: 2.298139 Batch:1000 [32000/60000]\n",
            "loss: 2.290489 Batch:1100 [35200/60000]\n",
            "loss: 2.192960 Batch:1200 [38400/60000]\n",
            "loss: 2.062230 Batch:1300 [41600/60000]\n",
            "loss: 1.931111 Batch:1400 [44800/60000]\n",
            "loss: 1.917096 Batch:1500 [48000/60000]\n",
            "loss: 1.890377 Batch:1600 [51200/60000]\n",
            "loss: 1.852658 Batch:1700 [54400/60000]\n",
            "loss: 1.832281 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 25.3%, Avg loss: 1.812321 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.858753 Batch:0 [    0/60000]\n",
            "loss: 1.857159 Batch:100 [ 3200/60000]\n",
            "loss: 1.706246 Batch:200 [ 6400/60000]\n",
            "loss: 1.758515 Batch:300 [ 9600/60000]\n",
            "loss: 1.766557 Batch:400 [12800/60000]\n",
            "loss: 1.816014 Batch:500 [16000/60000]\n",
            "loss: 1.694897 Batch:600 [19200/60000]\n",
            "loss: 1.696967 Batch:700 [22400/60000]\n",
            "loss: 1.737818 Batch:800 [25600/60000]\n",
            "loss: 1.662894 Batch:900 [28800/60000]\n",
            "loss: 1.665176 Batch:1000 [32000/60000]\n",
            "loss: 1.672918 Batch:1100 [35200/60000]\n",
            "loss: 1.687704 Batch:1200 [38400/60000]\n",
            "loss: 1.748898 Batch:1300 [41600/60000]\n",
            "loss: 1.622940 Batch:1400 [44800/60000]\n",
            "loss: 1.674379 Batch:1500 [48000/60000]\n",
            "loss: 1.687874 Batch:1600 [51200/60000]\n",
            "loss: 1.693509 Batch:1700 [54400/60000]\n",
            "loss: 1.613171 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.2%, Avg loss: 1.657096 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.709256 Batch:0 [    0/60000]\n",
            "loss: 1.735196 Batch:100 [ 3200/60000]\n",
            "loss: 1.620341 Batch:200 [ 6400/60000]\n",
            "loss: 1.631935 Batch:300 [ 9600/60000]\n",
            "loss: 1.604480 Batch:400 [12800/60000]\n",
            "loss: 1.691307 Batch:500 [16000/60000]\n",
            "loss: 1.608697 Batch:600 [19200/60000]\n",
            "loss: 1.585707 Batch:700 [22400/60000]\n",
            "loss: 1.649505 Batch:800 [25600/60000]\n",
            "loss: 1.599246 Batch:900 [28800/60000]\n",
            "loss: 1.675290 Batch:1000 [32000/60000]\n",
            "loss: 1.601581 Batch:1100 [35200/60000]\n",
            "loss: 1.586176 Batch:1200 [38400/60000]\n",
            "loss: 1.679714 Batch:1300 [41600/60000]\n",
            "loss: 1.621685 Batch:1400 [44800/60000]\n",
            "loss: 1.650323 Batch:1500 [48000/60000]\n",
            "loss: 1.631338 Batch:1600 [51200/60000]\n",
            "loss: 1.629105 Batch:1700 [54400/60000]\n",
            "loss: 1.604870 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.610343 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.663772 Batch:0 [    0/60000]\n",
            "loss: 1.663058 Batch:100 [ 3200/60000]\n",
            "loss: 1.587523 Batch:200 [ 6400/60000]\n",
            "loss: 1.567643 Batch:300 [ 9600/60000]\n",
            "loss: 1.563638 Batch:400 [12800/60000]\n",
            "loss: 1.650610 Batch:500 [16000/60000]\n",
            "loss: 1.604476 Batch:600 [19200/60000]\n",
            "loss: 1.553756 Batch:700 [22400/60000]\n",
            "loss: 1.607911 Batch:800 [25600/60000]\n",
            "loss: 1.521307 Batch:900 [28800/60000]\n",
            "loss: 1.642465 Batch:1000 [32000/60000]\n",
            "loss: 1.580717 Batch:1100 [35200/60000]\n",
            "loss: 1.608600 Batch:1200 [38400/60000]\n",
            "loss: 1.617178 Batch:1300 [41600/60000]\n",
            "loss: 1.568826 Batch:1400 [44800/60000]\n",
            "loss: 1.634231 Batch:1500 [48000/60000]\n",
            "loss: 1.558323 Batch:1600 [51200/60000]\n",
            "loss: 1.577014 Batch:1700 [54400/60000]\n",
            "loss: 1.576909 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 1.578289 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.639191 Batch:0 [    0/60000]\n",
            "loss: 1.528087 Batch:100 [ 3200/60000]\n",
            "loss: 1.592797 Batch:200 [ 6400/60000]\n",
            "loss: 1.597700 Batch:300 [ 9600/60000]\n",
            "loss: 1.527139 Batch:400 [12800/60000]\n",
            "loss: 1.596465 Batch:500 [16000/60000]\n",
            "loss: 1.577978 Batch:600 [19200/60000]\n",
            "loss: 1.543375 Batch:700 [22400/60000]\n",
            "loss: 1.567479 Batch:800 [25600/60000]\n",
            "loss: 1.512439 Batch:900 [28800/60000]\n",
            "loss: 1.590212 Batch:1000 [32000/60000]\n",
            "loss: 1.541474 Batch:1100 [35200/60000]\n",
            "loss: 1.568581 Batch:1200 [38400/60000]\n",
            "loss: 1.559936 Batch:1300 [41600/60000]\n",
            "loss: 1.541172 Batch:1400 [44800/60000]\n",
            "loss: 1.573273 Batch:1500 [48000/60000]\n",
            "loss: 1.527861 Batch:1600 [51200/60000]\n",
            "loss: 1.532375 Batch:1700 [54400/60000]\n",
            "loss: 1.538744 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.6%, Avg loss: 1.530514 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.595501 Batch:0 [    0/60000]\n",
            "loss: 1.493092 Batch:100 [ 3200/60000]\n",
            "loss: 1.535618 Batch:200 [ 6400/60000]\n",
            "loss: 1.502521 Batch:300 [ 9600/60000]\n",
            "loss: 1.478079 Batch:400 [12800/60000]\n",
            "loss: 1.571513 Batch:500 [16000/60000]\n",
            "loss: 1.494755 Batch:600 [19200/60000]\n",
            "loss: 1.482828 Batch:700 [22400/60000]\n",
            "loss: 1.558692 Batch:800 [25600/60000]\n",
            "loss: 1.469444 Batch:900 [28800/60000]\n",
            "loss: 1.580909 Batch:1000 [32000/60000]\n",
            "loss: 1.496927 Batch:1100 [35200/60000]\n",
            "loss: 1.557172 Batch:1200 [38400/60000]\n",
            "loss: 1.564459 Batch:1300 [41600/60000]\n",
            "loss: 1.511874 Batch:1400 [44800/60000]\n",
            "loss: 1.527841 Batch:1500 [48000/60000]\n",
            "loss: 1.497983 Batch:1600 [51200/60000]\n",
            "loss: 1.477883 Batch:1700 [54400/60000]\n",
            "loss: 1.513358 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.8%, Avg loss: 1.512425 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.536561 Batch:0 [    0/60000]\n",
            "loss: 1.466284 Batch:100 [ 3200/60000]\n",
            "loss: 1.555287 Batch:200 [ 6400/60000]\n",
            "loss: 1.484021 Batch:300 [ 9600/60000]\n",
            "loss: 1.477209 Batch:400 [12800/60000]\n",
            "loss: 1.500567 Batch:500 [16000/60000]\n",
            "loss: 1.478788 Batch:600 [19200/60000]\n",
            "loss: 1.489296 Batch:700 [22400/60000]\n",
            "loss: 1.502764 Batch:800 [25600/60000]\n",
            "loss: 1.465894 Batch:900 [28800/60000]\n",
            "loss: 1.502887 Batch:1000 [32000/60000]\n",
            "loss: 1.493585 Batch:1100 [35200/60000]\n",
            "loss: 1.551315 Batch:1200 [38400/60000]\n",
            "loss: 1.535501 Batch:1300 [41600/60000]\n",
            "loss: 1.501676 Batch:1400 [44800/60000]\n",
            "loss: 1.524114 Batch:1500 [48000/60000]\n",
            "loss: 1.505692 Batch:1600 [51200/60000]\n",
            "loss: 1.477160 Batch:1700 [54400/60000]\n",
            "loss: 1.509487 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.7%, Avg loss: 1.506181 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.508471 Batch:0 [    0/60000]\n",
            "loss: 1.473927 Batch:100 [ 3200/60000]\n",
            "loss: 1.527171 Batch:200 [ 6400/60000]\n",
            "loss: 1.463278 Batch:300 [ 9600/60000]\n",
            "loss: 1.469229 Batch:400 [12800/60000]\n",
            "loss: 1.507594 Batch:500 [16000/60000]\n",
            "loss: 1.488563 Batch:600 [19200/60000]\n",
            "loss: 1.467720 Batch:700 [22400/60000]\n",
            "loss: 1.473621 Batch:800 [25600/60000]\n",
            "loss: 1.462926 Batch:900 [28800/60000]\n",
            "loss: 1.482777 Batch:1000 [32000/60000]\n",
            "loss: 1.492775 Batch:1100 [35200/60000]\n",
            "loss: 1.499499 Batch:1200 [38400/60000]\n",
            "loss: 1.540606 Batch:1300 [41600/60000]\n",
            "loss: 1.512413 Batch:1400 [44800/60000]\n",
            "loss: 1.488817 Batch:1500 [48000/60000]\n",
            "loss: 1.483573 Batch:1600 [51200/60000]\n",
            "loss: 1.475778 Batch:1700 [54400/60000]\n",
            "loss: 1.505039 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.8%, Avg loss: 1.497913 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.495622 Batch:0 [    0/60000]\n",
            "loss: 1.467343 Batch:100 [ 3200/60000]\n",
            "loss: 1.528793 Batch:200 [ 6400/60000]\n",
            "loss: 1.470167 Batch:300 [ 9600/60000]\n",
            "loss: 1.470716 Batch:400 [12800/60000]\n",
            "loss: 1.498702 Batch:500 [16000/60000]\n",
            "loss: 1.467506 Batch:600 [19200/60000]\n",
            "loss: 1.464022 Batch:700 [22400/60000]\n",
            "loss: 1.473177 Batch:800 [25600/60000]\n",
            "loss: 1.465965 Batch:900 [28800/60000]\n",
            "loss: 1.478311 Batch:1000 [32000/60000]\n",
            "loss: 1.494129 Batch:1100 [35200/60000]\n",
            "loss: 1.544302 Batch:1200 [38400/60000]\n",
            "loss: 1.517006 Batch:1300 [41600/60000]\n",
            "loss: 1.500339 Batch:1400 [44800/60000]\n",
            "loss: 1.475248 Batch:1500 [48000/60000]\n",
            "loss: 1.464146 Batch:1600 [51200/60000]\n",
            "loss: 1.474285 Batch:1700 [54400/60000]\n",
            "loss: 1.498766 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 1.498941 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.496880 Batch:0 [    0/60000]\n",
            "loss: 1.498603 Batch:100 [ 3200/60000]\n",
            "loss: 1.491257 Batch:200 [ 6400/60000]\n",
            "loss: 1.468875 Batch:300 [ 9600/60000]\n",
            "loss: 1.473713 Batch:400 [12800/60000]\n",
            "loss: 1.521993 Batch:500 [16000/60000]\n",
            "loss: 1.468999 Batch:600 [19200/60000]\n",
            "loss: 1.461482 Batch:700 [22400/60000]\n",
            "loss: 1.467587 Batch:800 [25600/60000]\n",
            "loss: 1.462376 Batch:900 [28800/60000]\n",
            "loss: 1.484621 Batch:1000 [32000/60000]\n",
            "loss: 1.496253 Batch:1100 [35200/60000]\n",
            "loss: 1.497074 Batch:1200 [38400/60000]\n",
            "loss: 1.503472 Batch:1300 [41600/60000]\n",
            "loss: 1.502350 Batch:1400 [44800/60000]\n",
            "loss: 1.473478 Batch:1500 [48000/60000]\n",
            "loss: 1.465648 Batch:1600 [51200/60000]\n",
            "loss: 1.468007 Batch:1700 [54400/60000]\n",
            "loss: 1.498761 Batch:1800 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 1.493699 \n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG1eSLPy5eqV"
      },
      "source": [
        "npk_features = get_activation_output(train_dataloader_all, npk_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rlh2bwsStCk"
      },
      "source": [
        "npk_features[0] = npk_features[0].detach()\n",
        "npk_features[1] = npk_features[1].detach()\n",
        "npk_features[2] = npk_features[2].detach()\n",
        "npk_features[3] = npk_features[3].detach()\n",
        "npk_features[4] = npk_features[4].detach()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kAjeNUTUQu6"
      },
      "source": [
        "start = 0\n",
        "count = 0\n",
        "def custom_relu(i,x):\n",
        "  global start\n",
        "  global batch_size\n",
        "  global count\n",
        "  \n",
        "  #npk_features_tensor = t(npk_features[i][start:end].detach()))\n",
        "  #npk_features_tensor = npk_features_tensor.to(device)\n",
        "  end = start + x.shape[0]\n",
        "  #print(count, start,end, x.shape,npk_features[i][start:end].shape)\n",
        "\n",
        "  out = torch.mul(x,torch.sign(npk_features[i][start:end]))\n",
        "  \n",
        "  if i == 4:\n",
        "    count += 1\n",
        "    start = end \n",
        "    if x.shape[0] < batch_size:\n",
        "      print(count,start, end,  x.shape[0])\n",
        "      start = 0\n",
        "      count = 0\n",
        "    if start == 60000:\n",
        "      print(count,start, end,  x.shape[0])\n",
        "      start = 0\n",
        "      count = 0\n",
        "  \n",
        "  return out\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC2UC3tGcQJS"
      },
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, size = 60000):\n",
        "        self.size = size\n",
        "        self.x = torch.ones(60000,784)\n",
        "        self.y = torch.tensor([x for _,x in training_data])\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(self.x[idx].shape,self.y[idx].shape )\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM_4UVkPnHog"
      },
      "source": [
        "x_dummy_dataloader_all = DataLoader(CustomImageDataset(), batch_size=60000, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG12YwH9Xfb_"
      },
      "source": [
        "x_dummy_dataloader = DataLoader(CustomImageDataset(), batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhbrbxQ0B6E5",
        "outputId": "80942244-1734-4abf-a198-8eaacf3df052"
      },
      "source": [
        "npv_model = NeuralNetwork().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(npv_model.parameters(), lr=1e-1)\n",
        "epochs = 10\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(x_dummy_dataloader, npv_model, loss_fn, optimizer)\n",
        "    test(x_dummy_dataloader, npv_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300483 Batch:0 [    0/60000]\n",
            "loss: 2.281097 Batch:100 [ 3200/60000]\n",
            "loss: 1.726893 Batch:200 [ 6400/60000]\n",
            "loss: 1.518400 Batch:300 [ 9600/60000]\n",
            "loss: 1.477747 Batch:400 [12800/60000]\n",
            "loss: 1.518617 Batch:500 [16000/60000]\n",
            "loss: 1.477794 Batch:600 [19200/60000]\n",
            "loss: 1.464655 Batch:700 [22400/60000]\n",
            "loss: 1.478743 Batch:800 [25600/60000]\n",
            "loss: 1.461625 Batch:900 [28800/60000]\n",
            "loss: 1.483799 Batch:1000 [32000/60000]\n",
            "loss: 1.495897 Batch:1100 [35200/60000]\n",
            "loss: 1.496146 Batch:1200 [38400/60000]\n",
            "loss: 1.513319 Batch:1300 [41600/60000]\n",
            "loss: 1.494576 Batch:1400 [44800/60000]\n",
            "loss: 1.467272 Batch:1500 [48000/60000]\n",
            "loss: 1.465674 Batch:1600 [51200/60000]\n",
            "loss: 1.470124 Batch:1700 [54400/60000]\n",
            "loss: 1.526213 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 1.487714 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.492480 Batch:0 [    0/60000]\n",
            "loss: 1.470164 Batch:100 [ 3200/60000]\n",
            "loss: 1.521769 Batch:200 [ 6400/60000]\n",
            "loss: 1.496223 Batch:300 [ 9600/60000]\n",
            "loss: 1.475521 Batch:400 [12800/60000]\n",
            "loss: 1.515028 Batch:500 [16000/60000]\n",
            "loss: 1.478666 Batch:600 [19200/60000]\n",
            "loss: 1.461214 Batch:700 [22400/60000]\n",
            "loss: 1.465455 Batch:800 [25600/60000]\n",
            "loss: 1.461733 Batch:900 [28800/60000]\n",
            "loss: 1.501121 Batch:1000 [32000/60000]\n",
            "loss: 1.492330 Batch:1100 [35200/60000]\n",
            "loss: 1.496193 Batch:1200 [38400/60000]\n",
            "loss: 1.493792 Batch:1300 [41600/60000]\n",
            "loss: 1.494729 Batch:1400 [44800/60000]\n",
            "loss: 1.467106 Batch:1500 [48000/60000]\n",
            "loss: 1.465483 Batch:1600 [51200/60000]\n",
            "loss: 1.470506 Batch:1700 [54400/60000]\n",
            "loss: 1.512380 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 1.485516 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.491895 Batch:0 [    0/60000]\n",
            "loss: 1.466009 Batch:100 [ 3200/60000]\n",
            "loss: 1.500788 Batch:200 [ 6400/60000]\n",
            "loss: 1.496705 Batch:300 [ 9600/60000]\n",
            "loss: 1.473981 Batch:400 [12800/60000]\n",
            "loss: 1.515106 Batch:500 [16000/60000]\n",
            "loss: 1.475314 Batch:600 [19200/60000]\n",
            "loss: 1.465427 Batch:700 [22400/60000]\n",
            "loss: 1.466576 Batch:800 [25600/60000]\n",
            "loss: 1.465446 Batch:900 [28800/60000]\n",
            "loss: 1.500981 Batch:1000 [32000/60000]\n",
            "loss: 1.492043 Batch:1100 [35200/60000]\n",
            "loss: 1.492543 Batch:1200 [38400/60000]\n",
            "loss: 1.473350 Batch:1300 [41600/60000]\n",
            "loss: 1.492456 Batch:1400 [44800/60000]\n",
            "loss: 1.471678 Batch:1500 [48000/60000]\n",
            "loss: 1.465434 Batch:1600 [51200/60000]\n",
            "loss: 1.465650 Batch:1700 [54400/60000]\n",
            "loss: 1.507607 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 1.485430 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.496194 Batch:0 [    0/60000]\n",
            "loss: 1.474012 Batch:100 [ 3200/60000]\n",
            "loss: 1.525046 Batch:200 [ 6400/60000]\n",
            "loss: 1.492442 Batch:300 [ 9600/60000]\n",
            "loss: 1.465343 Batch:400 [12800/60000]\n",
            "loss: 1.509571 Batch:500 [16000/60000]\n",
            "loss: 1.472921 Batch:600 [19200/60000]\n",
            "loss: 1.461154 Batch:700 [22400/60000]\n",
            "loss: 1.465470 Batch:800 [25600/60000]\n",
            "loss: 1.461175 Batch:900 [28800/60000]\n",
            "loss: 1.485883 Batch:1000 [32000/60000]\n",
            "loss: 1.496682 Batch:1100 [35200/60000]\n",
            "loss: 1.492399 Batch:1200 [38400/60000]\n",
            "loss: 1.499322 Batch:1300 [41600/60000]\n",
            "loss: 1.497546 Batch:1400 [44800/60000]\n",
            "loss: 1.469116 Batch:1500 [48000/60000]\n",
            "loss: 1.465459 Batch:1600 [51200/60000]\n",
            "loss: 1.465738 Batch:1700 [54400/60000]\n",
            "loss: 1.510650 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 1.484778 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.494158 Batch:0 [    0/60000]\n",
            "loss: 1.465406 Batch:100 [ 3200/60000]\n",
            "loss: 1.497855 Batch:200 [ 6400/60000]\n",
            "loss: 1.495214 Batch:300 [ 9600/60000]\n",
            "loss: 1.465860 Batch:400 [12800/60000]\n",
            "loss: 1.501003 Batch:500 [16000/60000]\n",
            "loss: 1.474061 Batch:600 [19200/60000]\n",
            "loss: 1.461843 Batch:700 [22400/60000]\n",
            "loss: 1.461174 Batch:800 [25600/60000]\n",
            "loss: 1.461166 Batch:900 [28800/60000]\n",
            "loss: 1.474189 Batch:1000 [32000/60000]\n",
            "loss: 1.492643 Batch:1100 [35200/60000]\n",
            "loss: 1.492372 Batch:1200 [38400/60000]\n",
            "loss: 1.474494 Batch:1300 [41600/60000]\n",
            "loss: 1.500352 Batch:1400 [44800/60000]\n",
            "loss: 1.466307 Batch:1500 [48000/60000]\n",
            "loss: 1.465434 Batch:1600 [51200/60000]\n",
            "loss: 1.465491 Batch:1700 [54400/60000]\n",
            "loss: 1.511427 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 1.483999 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.492406 Batch:0 [    0/60000]\n",
            "loss: 1.465426 Batch:100 [ 3200/60000]\n",
            "loss: 1.497271 Batch:200 [ 6400/60000]\n",
            "loss: 1.493584 Batch:300 [ 9600/60000]\n",
            "loss: 1.465889 Batch:400 [12800/60000]\n",
            "loss: 1.500986 Batch:500 [16000/60000]\n",
            "loss: 1.468624 Batch:600 [19200/60000]\n",
            "loss: 1.461801 Batch:700 [22400/60000]\n",
            "loss: 1.465454 Batch:800 [25600/60000]\n",
            "loss: 1.461152 Batch:900 [28800/60000]\n",
            "loss: 1.485967 Batch:1000 [32000/60000]\n",
            "loss: 1.496670 Batch:1100 [35200/60000]\n",
            "loss: 1.492720 Batch:1200 [38400/60000]\n",
            "loss: 1.466734 Batch:1300 [41600/60000]\n",
            "loss: 1.497378 Batch:1400 [44800/60000]\n",
            "loss: 1.465862 Batch:1500 [48000/60000]\n",
            "loss: 1.478727 Batch:1600 [51200/60000]\n",
            "loss: 1.469692 Batch:1700 [54400/60000]\n",
            "loss: 1.527092 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 1.484976 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.492402 Batch:0 [    0/60000]\n",
            "loss: 1.466650 Batch:100 [ 3200/60000]\n",
            "loss: 1.496697 Batch:200 [ 6400/60000]\n",
            "loss: 1.497149 Batch:300 [ 9600/60000]\n",
            "loss: 1.473866 Batch:400 [12800/60000]\n",
            "loss: 1.525553 Batch:500 [16000/60000]\n",
            "loss: 1.473959 Batch:600 [19200/60000]\n",
            "loss: 1.461151 Batch:700 [22400/60000]\n",
            "loss: 1.464028 Batch:800 [25600/60000]\n",
            "loss: 1.461494 Batch:900 [28800/60000]\n",
            "loss: 1.472004 Batch:1000 [32000/60000]\n",
            "loss: 1.496683 Batch:1100 [35200/60000]\n",
            "loss: 1.492391 Batch:1200 [38400/60000]\n",
            "loss: 1.477726 Batch:1300 [41600/60000]\n",
            "loss: 1.494823 Batch:1400 [44800/60000]\n",
            "loss: 1.469408 Batch:1500 [48000/60000]\n",
            "loss: 1.492056 Batch:1600 [51200/60000]\n",
            "loss: 1.461290 Batch:1700 [54400/60000]\n",
            "loss: 1.497901 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 1.483983 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.492402 Batch:0 [    0/60000]\n",
            "loss: 1.469740 Batch:100 [ 3200/60000]\n",
            "loss: 1.498958 Batch:200 [ 6400/60000]\n",
            "loss: 1.496305 Batch:300 [ 9600/60000]\n",
            "loss: 1.468586 Batch:400 [12800/60000]\n",
            "loss: 1.517419 Batch:500 [16000/60000]\n",
            "loss: 1.476069 Batch:600 [19200/60000]\n",
            "loss: 1.461151 Batch:700 [22400/60000]\n",
            "loss: 1.464147 Batch:800 [25600/60000]\n",
            "loss: 1.461152 Batch:900 [28800/60000]\n",
            "loss: 1.475502 Batch:1000 [32000/60000]\n",
            "loss: 1.496678 Batch:1100 [35200/60000]\n",
            "loss: 1.491897 Batch:1200 [38400/60000]\n",
            "loss: 1.473549 Batch:1300 [41600/60000]\n",
            "loss: 1.472871 Batch:1400 [44800/60000]\n",
            "loss: 1.467765 Batch:1500 [48000/60000]\n",
            "loss: 1.465286 Batch:1600 [51200/60000]\n",
            "loss: 1.465495 Batch:1700 [54400/60000]\n",
            "loss: 1.507702 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 1.483068 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.492402 Batch:0 [    0/60000]\n",
            "loss: 1.465428 Batch:100 [ 3200/60000]\n",
            "loss: 1.500916 Batch:200 [ 6400/60000]\n",
            "loss: 1.496680 Batch:300 [ 9600/60000]\n",
            "loss: 1.469287 Batch:400 [12800/60000]\n",
            "loss: 1.514142 Batch:500 [16000/60000]\n",
            "loss: 1.478108 Batch:600 [19200/60000]\n",
            "loss: 1.461152 Batch:700 [22400/60000]\n",
            "loss: 1.462729 Batch:800 [25600/60000]\n",
            "loss: 1.461151 Batch:900 [28800/60000]\n",
            "loss: 1.473287 Batch:1000 [32000/60000]\n",
            "loss: 1.496663 Batch:1100 [35200/60000]\n",
            "loss: 1.492565 Batch:1200 [38400/60000]\n",
            "loss: 1.492302 Batch:1300 [41600/60000]\n",
            "loss: 1.470216 Batch:1400 [44800/60000]\n",
            "loss: 1.469428 Batch:1500 [48000/60000]\n",
            "loss: 1.473622 Batch:1600 [51200/60000]\n",
            "loss: 1.469772 Batch:1700 [54400/60000]\n",
            "loss: 1.500952 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 1.483975 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.492402 Batch:0 [    0/60000]\n",
            "loss: 1.465564 Batch:100 [ 3200/60000]\n",
            "loss: 1.496684 Batch:200 [ 6400/60000]\n",
            "loss: 1.497348 Batch:300 [ 9600/60000]\n",
            "loss: 1.469386 Batch:400 [12800/60000]\n",
            "loss: 1.523587 Batch:500 [16000/60000]\n",
            "loss: 1.473985 Batch:600 [19200/60000]\n",
            "loss: 1.461427 Batch:700 [22400/60000]\n",
            "loss: 1.461151 Batch:800 [25600/60000]\n",
            "loss: 1.461169 Batch:900 [28800/60000]\n",
            "loss: 1.478434 Batch:1000 [32000/60000]\n",
            "loss: 1.496677 Batch:1100 [35200/60000]\n",
            "loss: 1.495601 Batch:1200 [38400/60000]\n",
            "loss: 1.466990 Batch:1300 [41600/60000]\n",
            "loss: 1.498778 Batch:1400 [44800/60000]\n",
            "loss: 1.466713 Batch:1500 [48000/60000]\n",
            "loss: 1.465427 Batch:1600 [51200/60000]\n",
            "loss: 1.465492 Batch:1700 [54400/60000]\n",
            "loss: 1.512935 Batch:1800 [57600/60000]\n",
            "1875 60000 60000 32\n",
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 1.482656 \n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V-Dq0lKH3Q4",
        "outputId": "12b6f6c9-6e5c-4466-c428-1d3e15dd0c3b"
      },
      "source": [
        "test(x_dummy_dataloader, npv_model, loss_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875 60000 60000 32\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 1.482656 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFLJ5wDuOjl2",
        "outputId": "34abe13b-76fa-4f44-f236-4c7ebfcd5245"
      },
      "source": [
        "npv_features = get_activation_output(train_dataloader_all, npv_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 60000 60000 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W802gAI_nkwa",
        "outputId": "00540b71-c682-4fdb-b04e-c117e4006f29"
      },
      "source": [
        "npk_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1445, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.9706, 1.1460, 0.0000,  ..., 0.0000, 0.0000, 0.3695],\n",
              "         ...,\n",
              "         [0.0000, 0.0000, 0.0478,  ..., 0.0000, 0.0000, 0.5184],\n",
              "         [0.0733, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.2388,  ..., 0.0000, 0.6036, 0.7320]],\n",
              "        device='cuda:0'),\n",
              " tensor([[1.3773, 1.5552, 0.3290,  ..., 0.9093, 0.6695, 1.4113],\n",
              "         [2.3106, 2.3575, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.0000, 0.0841],\n",
              "         ...,\n",
              "         [1.3608, 1.7373, 0.0000,  ..., 1.1201, 0.8647, 1.7885],\n",
              "         [0.8850, 0.8192, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "         [0.3168, 0.0000, 0.0000,  ..., 0.0664, 0.3493, 0.5734]],\n",
              "        device='cuda:0'),\n",
              " tensor([[0.0000e+00, 7.5501e-01, 8.6100e-01,  ..., 0.0000e+00, 2.3590e+00,\n",
              "          1.1204e+00],\n",
              "         [0.0000e+00, 2.5445e+00, 3.3580e-01,  ..., 0.0000e+00, 4.6407e+00,\n",
              "          6.8284e-01],\n",
              "         [6.4876e-01, 3.0748e-01, 9.5799e-02,  ..., 0.0000e+00, 7.8412e-01,\n",
              "          1.8218e-03],\n",
              "         ...,\n",
              "         [0.0000e+00, 1.2955e+00, 1.7595e+00,  ..., 0.0000e+00, 4.5256e+00,\n",
              "          1.6071e+00],\n",
              "         [0.0000e+00, 6.3365e-01, 1.3228e+00,  ..., 0.0000e+00, 1.1953e+00,\n",
              "          2.2661e-02],\n",
              "         [6.9403e-01, 6.2054e-02, 1.7521e+00,  ..., 0.0000e+00, 1.5038e+00,\n",
              "          1.0222e-01]], device='cuda:0'),\n",
              " tensor([[0.0000, 1.9284, 2.3774,  ..., 1.4951, 1.7771, 1.3612],\n",
              "         [0.1273, 1.2358, 3.9626,  ..., 0.0355, 1.4661, 0.0000],\n",
              "         [0.0000, 0.0000, 0.4461,  ..., 0.0000, 0.0000, 0.0400],\n",
              "         ...,\n",
              "         [0.0000, 2.7449, 4.2979,  ..., 2.3556, 2.2874, 1.8229],\n",
              "         [0.0000, 0.0000, 1.1001,  ..., 0.0000, 0.0000, 0.0576],\n",
              "         [0.0000, 0.0960, 1.1410,  ..., 1.7168, 0.0000, 1.4597]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 5.4468,  2.2319, 14.1734,  ...,  7.9347, 12.7444,  0.0000],\n",
              "         [13.9904,  4.1120, 12.7955,  ..., 12.8702, 17.7166,  0.0000],\n",
              "         [ 1.7386,  0.0000,  0.0000,  ...,  0.6539,  0.0000,  0.0000],\n",
              "         ...,\n",
              "         [ 9.2291,  3.1803, 19.0199,  ..., 12.2994, 17.3618,  0.0000],\n",
              "         [ 0.7138,  0.0000,  2.3026,  ...,  2.4850,  5.3944,  2.6533],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  1.8303,  0.0000,  0.0000]],\n",
              "        device='cuda:0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgXlmJH-nir_",
        "outputId": "f8d1581f-9543-4723-b986-f4399f3160ba"
      },
      "source": [
        "npv_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.1235, -0.0000],\n",
              "         [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
              "         [ 0.3345, -0.9954, -0.0000,  ..., -0.0000,  0.0000, -0.2897],\n",
              "         ...,\n",
              "         [ 0.0000, -0.0000, -0.1436,  ...,  0.0000,  0.0000, -0.1096],\n",
              "         [ 0.4943, -0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
              "         [ 0.0000, -0.0000, -0.1588,  ...,  0.0000,  0.2184, -0.1047]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 0.4391,  0.3837, -0.0869,  ..., -0.2399, -0.3492,  0.0164],\n",
              "         [ 0.9204,  0.3013, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.4214,  ...,  0.0000,  0.0000, -0.2148],\n",
              "         ...,\n",
              "         [ 0.8514,  0.4592,  0.0000,  ...,  0.1396, -0.5176,  0.0384],\n",
              "         [ 0.2497,  0.2367,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
              "         [ 0.6724,  0.0000,  0.0000,  ...,  0.3744, -0.2209,  0.0961]],\n",
              "        device='cuda:0'),\n",
              " tensor([[-0.0000, -1.1215,  0.2886,  ...,  0.0000, -0.7472,  0.2822],\n",
              "         [-0.0000, -1.2413,  0.3869,  ...,  0.0000, -0.8827, -0.1824],\n",
              "         [-0.1657, -1.3983,  0.2498,  ..., -0.0000, -0.1985, -0.1577],\n",
              "         ...,\n",
              "         [-0.0000, -2.5446,  0.5203,  ...,  0.0000, -1.3394,  0.3541],\n",
              "         [-0.0000, -0.5644, -0.1096,  ...,  0.0000, -0.5925,  0.0281],\n",
              "         [-0.7592, -2.5972,  0.5107,  ..., -0.0000, -0.9349, -0.0263]],\n",
              "        device='cuda:0'),\n",
              " tensor([[-0.0000, -0.2291,  0.1966,  ...,  0.1863, -1.6033, -0.2982],\n",
              "         [-0.3591,  0.2409,  1.0782,  ...,  0.4189, -1.3463,  0.0000],\n",
              "         [-0.0000,  0.0000,  0.1048,  ...,  0.0000, -0.0000,  0.0607],\n",
              "         ...,\n",
              "         [-0.0000,  0.0541,  0.3443,  ...,  0.4554, -2.6895, -0.0844],\n",
              "         [ 0.0000,  0.0000,  0.3607,  ...,  0.0000, -0.0000, -0.2295],\n",
              "         [-0.0000,  0.0539,  0.4417,  ...,  0.7651, -0.0000, -0.3599]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 1.8296, -3.2896,  1.8336,  ..., -1.5107, -0.6931, -0.0000],\n",
              "         [ 2.1899, -1.9190,  1.4650,  ..., -0.6251, -1.0876, -0.0000],\n",
              "         [ 1.2645, -0.0000,  0.0000,  ..., -0.7444, -0.0000,  0.0000],\n",
              "         ...,\n",
              "         [ 2.9499, -5.7069,  3.1546,  ..., -2.6335, -1.1285, -0.0000],\n",
              "         [ 1.1052, -0.0000,  0.7795,  ..., -1.1838, -0.3840, -1.4668],\n",
              "         [ 0.0000, -0.0000,  0.0000,  ..., -4.2166, -0.0000, -0.0000]],\n",
              "        device='cuda:0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM2Q4T2Sdblc",
        "outputId": "8c97519d-ed57-4783-e25d-c8162a2fba82"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=89ab6c631e77f18a46b62c619dbf8f69d75d89be47d9869d6fb4eafe6fd2dabf\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PSqb9yy8d04Y",
        "outputId": "8dfa1d06-4675-4c9d-c1d9-0d3bd2293960"
      },
      "source": [
        "from torchviz import make_dot\n",
        "x, y = training_data[0]\n",
        "pred, _ = npv_model(x.to(device))\n",
        "make_dot(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fb164d9c450>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"495pt\" height=\"918pt\"\n viewBox=\"0.00 0.00 495.43 918.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.9107 .9107) rotate(0) translate(4 1004)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1004 540,-1004 540,4 -4,4\"/>\n<!-- 140399877953008 -->\n<g id=\"node1\" class=\"node\">\n<title>140399877953008</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"382,-31 317,-31 317,0 382,0 382,-31\"/>\n<text text-anchor=\"middle\" x=\"349.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 10)</text>\n</g>\n<!-- 140399877931536 -->\n<g id=\"node2\" class=\"node\">\n<title>140399877931536</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"403,-86 296,-86 296,-67 403,-67 403,-86\"/>\n<text text-anchor=\"middle\" x=\"349.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SigmoidBackward</text>\n</g>\n<!-- 140399877931536&#45;&gt;140399877953008 -->\n<g id=\"edge42\" class=\"edge\">\n<title>140399877931536&#45;&gt;140399877953008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M349.5,-66.9688C349.5,-60.1289 349.5,-50.5621 349.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"353.0001,-41.3678 349.5,-31.3678 346.0001,-41.3678 353.0001,-41.3678\"/>\n</g>\n<!-- 140399877932240 -->\n<g id=\"node3\" class=\"node\">\n<title>140399877932240</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"397,-141 302,-141 302,-122 397,-122 397,-141\"/>\n<text text-anchor=\"middle\" x=\"349.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140399877932240&#45;&gt;140399877931536 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140399877932240&#45;&gt;140399877931536</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M349.5,-121.9197C349.5,-114.9083 349.5,-105.1442 349.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"353.0001,-96.3408 349.5,-86.3408 346.0001,-96.3409 353.0001,-96.3408\"/>\n</g>\n<!-- 140399874830736 -->\n<g id=\"node4\" class=\"node\">\n<title>140399874830736</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"287,-196 186,-196 186,-177 287,-177 287,-196\"/>\n<text text-anchor=\"middle\" x=\"236.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399874830736&#45;&gt;140399877932240 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140399874830736&#45;&gt;140399877932240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M256.1831,-176.9197C273.9787,-168.2581 300.4052,-155.3957 320.5977,-145.5675\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"322.2036,-148.6785 329.6633,-141.155 319.14,-142.3844 322.2036,-148.6785\"/>\n</g>\n<!-- 140399874562720 -->\n<g id=\"node5\" class=\"node\">\n<title>140399874562720</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"263.5,-263 209.5,-263 209.5,-232 263.5,-232 263.5,-263\"/>\n<text text-anchor=\"middle\" x=\"236.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 140399874562720&#45;&gt;140399874830736 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140399874562720&#45;&gt;140399874830736</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M236.5,-231.791C236.5,-224.0249 236.5,-214.5706 236.5,-206.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"240.0001,-206.0647 236.5,-196.0648 233.0001,-206.0648 240.0001,-206.0647\"/>\n</g>\n<!-- 140399877959888 -->\n<g id=\"node6\" class=\"node\">\n<title>140399877959888</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"394,-196 305,-196 305,-177 394,-177 394,-196\"/>\n<text text-anchor=\"middle\" x=\"349.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140399877959888&#45;&gt;140399877932240 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140399877959888&#45;&gt;140399877932240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M349.5,-176.9197C349.5,-169.9083 349.5,-160.1442 349.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"353.0001,-151.3408 349.5,-141.3408 346.0001,-151.3409 353.0001,-151.3408\"/>\n</g>\n<!-- 140399877959760 -->\n<g id=\"node7\" class=\"node\">\n<title>140399877959760</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"392,-257 297,-257 297,-238 392,-238 392,-257\"/>\n<text text-anchor=\"middle\" x=\"344.5\" y=\"-245\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140399877959760&#45;&gt;140399877959888 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140399877959760&#45;&gt;140399877959888</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M345.2813,-237.9688C345.9743,-229.5131 347.009,-216.8901 347.8802,-206.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"351.379,-206.4182 348.7077,-196.1656 344.4024,-205.8462 351.379,-206.4182\"/>\n</g>\n<!-- 140399874831440 -->\n<g id=\"node8\" class=\"node\">\n<title>140399874831440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"245,-324 144,-324 144,-305 245,-305 245,-324\"/>\n<text text-anchor=\"middle\" x=\"194.5\" y=\"-312\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399874831440&#45;&gt;140399877959760 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140399874831440&#45;&gt;140399877959760</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M215.9913,-304.9005C241.6103,-293.4574 284.6099,-274.2509 313.7447,-261.2374\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"315.3454,-264.3557 323.0485,-257.0817 312.4905,-257.9643 315.3454,-264.3557\"/>\n</g>\n<!-- 140399874562240 -->\n<g id=\"node9\" class=\"node\">\n<title>140399874562240</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"221.5,-397 167.5,-397 167.5,-366 221.5,-366 221.5,-397\"/>\n<text text-anchor=\"middle\" x=\"194.5\" y=\"-373\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140399874562240&#45;&gt;140399874831440 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140399874562240&#45;&gt;140399874831440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M194.5,-365.9604C194.5,-356.6356 194.5,-344.6748 194.5,-334.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"198.0001,-334.35 194.5,-324.3501 191.0001,-334.3501 198.0001,-334.35\"/>\n</g>\n<!-- 140399877959824 -->\n<g id=\"node10\" class=\"node\">\n<title>140399877959824</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"352,-324 263,-324 263,-305 352,-305 352,-324\"/>\n<text text-anchor=\"middle\" x=\"307.5\" y=\"-312\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140399877959824&#45;&gt;140399877959760 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140399877959824&#45;&gt;140399877959760</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M312.8012,-304.9005C318.4062,-294.751 327.3839,-278.4941 334.3565,-265.8679\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"337.4382,-267.5275 339.2086,-257.0817 331.3105,-264.1435 337.4382,-267.5275\"/>\n</g>\n<!-- 140399877960272 -->\n<g id=\"node11\" class=\"node\">\n<title>140399877960272</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"345,-391 250,-391 250,-372 345,-372 345,-391\"/>\n<text text-anchor=\"middle\" x=\"297.5\" y=\"-379\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140399877960272&#45;&gt;140399877959824 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140399877960272&#45;&gt;140399877959824</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M298.9328,-371.9005C300.4031,-362.0495 302.7321,-346.4451 304.5909,-333.9912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"308.0553,-334.4888 306.0699,-324.0817 301.132,-333.4554 308.0553,-334.4888\"/>\n</g>\n<!-- 140399877857744 -->\n<g id=\"node12\" class=\"node\">\n<title>140399877857744</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"197,-458 96,-458 96,-439 197,-439 197,-458\"/>\n<text text-anchor=\"middle\" x=\"146.5\" y=\"-446\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877857744&#45;&gt;140399877960272 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140399877857744&#45;&gt;140399877960272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M168.1346,-438.9005C193.9243,-427.4574 237.2106,-408.2509 266.5397,-395.2374\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"268.1844,-398.3367 275.9055,-391.0817 265.3454,-391.9383 268.1844,-398.3367\"/>\n</g>\n<!-- 140399874563840 -->\n<g id=\"node13\" class=\"node\">\n<title>140399874563840</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"173.5,-531 119.5,-531 119.5,-500 173.5,-500 173.5,-531\"/>\n<text text-anchor=\"middle\" x=\"146.5\" y=\"-507\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140399874563840&#45;&gt;140399877857744 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140399874563840&#45;&gt;140399877857744</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M146.5,-499.9604C146.5,-490.6356 146.5,-478.6748 146.5,-468.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"150.0001,-468.35 146.5,-458.3501 143.0001,-468.3501 150.0001,-468.35\"/>\n</g>\n<!-- 140399877960464 -->\n<g id=\"node14\" class=\"node\">\n<title>140399877960464</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"304,-458 215,-458 215,-439 304,-439 304,-458\"/>\n<text text-anchor=\"middle\" x=\"259.5\" y=\"-446\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140399877960464&#45;&gt;140399877960272 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140399877960464&#45;&gt;140399877960272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M264.9445,-438.9005C270.7009,-428.751 279.9213,-412.4941 287.0824,-399.8679\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"290.1766,-401.5067 292.0656,-391.0817 284.0877,-398.0533 290.1766,-401.5067\"/>\n</g>\n<!-- 140399877960528 -->\n<g id=\"node15\" class=\"node\">\n<title>140399877960528</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"297,-525 202,-525 202,-506 297,-506 297,-525\"/>\n<text text-anchor=\"middle\" x=\"249.5\" y=\"-513\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140399877960528&#45;&gt;140399877960464 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140399877960528&#45;&gt;140399877960464</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M250.9328,-505.9005C252.4031,-496.0495 254.7321,-480.4451 256.5909,-467.9912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.0553,-468.4888 258.0699,-458.0817 253.132,-467.4554 260.0553,-468.4888\"/>\n</g>\n<!-- 140399877858128 -->\n<g id=\"node16\" class=\"node\">\n<title>140399877858128</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"149,-592 48,-592 48,-573 149,-573 149,-592\"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858128&#45;&gt;140399877960528 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140399877858128&#45;&gt;140399877960528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M120.1346,-572.9005C145.9243,-561.4574 189.2106,-542.2509 218.5397,-529.2374\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.1844,-532.3367 227.9055,-525.0817 217.3454,-525.9383 220.1844,-532.3367\"/>\n</g>\n<!-- 140399874563680 -->\n<g id=\"node17\" class=\"node\">\n<title>140399874563680</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"125.5,-665 71.5,-665 71.5,-634 125.5,-634 125.5,-665\"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-641\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140399874563680&#45;&gt;140399877858128 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140399874563680&#45;&gt;140399877858128</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98.5,-633.9604C98.5,-624.6356 98.5,-612.6748 98.5,-602.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.0001,-602.35 98.5,-592.3501 95.0001,-602.3501 102.0001,-602.35\"/>\n</g>\n<!-- 140399877960784 -->\n<g id=\"node18\" class=\"node\">\n<title>140399877960784</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"256,-592 167,-592 167,-573 256,-573 256,-592\"/>\n<text text-anchor=\"middle\" x=\"211.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140399877960784&#45;&gt;140399877960528 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140399877960784&#45;&gt;140399877960528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M216.9445,-572.9005C222.7009,-562.751 231.9213,-546.4941 239.0824,-533.8679\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"242.1766,-535.5067 244.0656,-525.0817 236.0877,-532.0533 242.1766,-535.5067\"/>\n</g>\n<!-- 140399877960848 -->\n<g id=\"node19\" class=\"node\">\n<title>140399877960848</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"249,-659 154,-659 154,-640 249,-640 249,-659\"/>\n<text text-anchor=\"middle\" x=\"201.5\" y=\"-647\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140399877960848&#45;&gt;140399877960784 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140399877960848&#45;&gt;140399877960784</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M202.9328,-639.9005C204.4031,-630.0495 206.7321,-614.4451 208.5909,-601.9912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"212.0553,-602.4888 210.0699,-592.0817 205.132,-601.4554 212.0553,-602.4888\"/>\n</g>\n<!-- 140399877858512 -->\n<g id=\"node20\" class=\"node\">\n<title>140399877858512</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-726 0,-726 0,-707 101,-707 101,-726\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-714\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858512&#45;&gt;140399877960848 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140399877858512&#45;&gt;140399877960848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M72.1346,-706.9005C97.9243,-695.4574 141.2106,-676.2509 170.5397,-663.2374\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.1844,-666.3367 179.9055,-659.0817 169.3454,-659.9383 172.1844,-666.3367\"/>\n</g>\n<!-- 140399874560960 -->\n<g id=\"node21\" class=\"node\">\n<title>140399874560960</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-799 23.5,-799 23.5,-768 77.5,-768 77.5,-799\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-775\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140399874560960&#45;&gt;140399877858512 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140399874560960&#45;&gt;140399877858512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-767.9604C50.5,-758.6356 50.5,-746.6748 50.5,-736.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-736.35 50.5,-726.3501 47.0001,-736.3501 54.0001,-736.35\"/>\n</g>\n<!-- 140399877961104 -->\n<g id=\"node22\" class=\"node\">\n<title>140399877961104</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-726 119,-726 119,-707 208,-707 208,-726\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-714\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140399877961104&#45;&gt;140399877960848 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140399877961104&#45;&gt;140399877960848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M168.9445,-706.9005C174.7009,-696.751 183.9213,-680.4941 191.0824,-667.8679\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"194.1766,-669.5067 196.0656,-659.0817 188.0877,-666.0533 194.1766,-669.5067\"/>\n</g>\n<!-- 140399877961168 -->\n<g id=\"node23\" class=\"node\">\n<title>140399877961168</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"202,-793 107,-793 107,-774 202,-774 202,-793\"/>\n<text text-anchor=\"middle\" x=\"154.5\" y=\"-781\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140399877961168&#45;&gt;140399877961104 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140399877961168&#45;&gt;140399877961104</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M155.7895,-773.9005C157.0994,-764.149 159.1666,-748.7597 160.831,-736.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.3503,-736.4586 162.2129,-726.0817 157.4127,-735.5266 164.3503,-736.4586\"/>\n</g>\n<!-- 140399877858896 -->\n<g id=\"node24\" class=\"node\">\n<title>140399877858896</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"112,-860 11,-860 11,-841 112,-841 112,-860\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-848\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858896&#45;&gt;140399877961168 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140399877858896&#45;&gt;140399877961168</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M74.8246,-840.9005C90.0178,-829.9549 115.0703,-811.9064 133.0284,-798.9688\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"135.1323,-801.7668 141.2001,-793.0817 131.0405,-796.0872 135.1323,-801.7668\"/>\n</g>\n<!-- 140399874562560 -->\n<g id=\"node25\" class=\"node\">\n<title>140399874562560</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"88.5,-933 34.5,-933 34.5,-902 88.5,-902 88.5,-933\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-909\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128)</text>\n</g>\n<!-- 140399874562560&#45;&gt;140399877858896 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140399874562560&#45;&gt;140399877858896</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M61.5,-901.9604C61.5,-892.6356 61.5,-880.6748 61.5,-870.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"65.0001,-870.35 61.5,-860.3501 58.0001,-870.3501 65.0001,-870.35\"/>\n</g>\n<!-- 140399877961424 -->\n<g id=\"node26\" class=\"node\">\n<title>140399877961424</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"201,-860 130,-860 130,-841 201,-841 201,-860\"/>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-848\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140399877961424&#45;&gt;140399877961168 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140399877961424&#45;&gt;140399877961168</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M163.924,-840.9005C162.3066,-831.0495 159.7447,-815.4451 157.7,-802.9912\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.1471,-802.3825 156.0731,-793.0817 154.2395,-803.5166 161.1471,-802.3825\"/>\n</g>\n<!-- 140399877858960 -->\n<g id=\"node27\" class=\"node\">\n<title>140399877858960</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"216,-927 115,-927 115,-908 216,-908 216,-927\"/>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-915\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858960&#45;&gt;140399877961424 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140399877858960&#45;&gt;140399877961424</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M165.5,-907.9005C165.5,-898.149 165.5,-882.7597 165.5,-870.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"169.0001,-870.0816 165.5,-860.0817 162.0001,-870.0817 169.0001,-870.0816\"/>\n</g>\n<!-- 140399874560320 -->\n<g id=\"node28\" class=\"node\">\n<title>140399874560320</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"207,-1000 124,-1000 124,-969 207,-969 207,-1000\"/>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-976\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128, 784)</text>\n</g>\n<!-- 140399874560320&#45;&gt;140399877858960 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140399874560320&#45;&gt;140399877858960</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M165.5,-968.9604C165.5,-959.6356 165.5,-947.6748 165.5,-937.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"169.0001,-937.35 165.5,-927.3501 162.0001,-937.3501 169.0001,-937.35\"/>\n</g>\n<!-- 140399877961296 -->\n<g id=\"node29\" class=\"node\">\n<title>140399877961296</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"297,-726 226,-726 226,-707 297,-707 297,-726\"/>\n<text text-anchor=\"middle\" x=\"261.5\" y=\"-714\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140399877961296&#45;&gt;140399877960848 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140399877961296&#45;&gt;140399877960848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M252.9035,-706.9005C243.547,-696.4525 228.3948,-679.5325 216.9599,-666.7635\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"219.3592,-664.1963 210.0806,-659.0817 214.1445,-668.8661 219.3592,-664.1963\"/>\n</g>\n<!-- 140399877858832 -->\n<g id=\"node30\" class=\"node\">\n<title>140399877858832</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"321,-793 220,-793 220,-774 321,-774 321,-793\"/>\n<text text-anchor=\"middle\" x=\"270.5\" y=\"-781\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858832&#45;&gt;140399877961296 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140399877858832&#45;&gt;140399877961296</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M269.2105,-773.9005C267.9006,-764.149 265.8334,-748.7597 264.169,-736.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"267.5873,-735.5266 262.7871,-726.0817 260.6497,-736.4586 267.5873,-735.5266\"/>\n</g>\n<!-- 140401608734832 -->\n<g id=\"node31\" class=\"node\">\n<title>140401608734832</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"312,-866 229,-866 229,-835 312,-835 312,-866\"/>\n<text text-anchor=\"middle\" x=\"270.5\" y=\"-842\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128, 128)</text>\n</g>\n<!-- 140401608734832&#45;&gt;140399877858832 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140401608734832&#45;&gt;140399877858832</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M270.5,-834.9604C270.5,-825.6356 270.5,-813.6748 270.5,-803.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"274.0001,-803.35 270.5,-793.3501 267.0001,-803.3501 274.0001,-803.35\"/>\n</g>\n<!-- 140399877960976 -->\n<g id=\"node32\" class=\"node\">\n<title>140399877960976</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"345,-592 274,-592 274,-573 345,-573 345,-592\"/>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140399877960976&#45;&gt;140399877960528 -->\n<g id=\"edge30\" class=\"edge\">\n<title>140399877960976&#45;&gt;140399877960528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M300.9035,-572.9005C291.547,-562.4525 276.3948,-545.5325 264.9599,-532.7635\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"267.3592,-530.1963 258.0806,-525.0817 262.1445,-534.8661 267.3592,-530.1963\"/>\n</g>\n<!-- 140399877858448 -->\n<g id=\"node33\" class=\"node\">\n<title>140399877858448</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"369,-659 268,-659 268,-640 369,-640 369,-659\"/>\n<text text-anchor=\"middle\" x=\"318.5\" y=\"-647\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858448&#45;&gt;140399877960976 -->\n<g id=\"edge31\" class=\"edge\">\n<title>140399877858448&#45;&gt;140399877960976</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M317.2105,-639.9005C315.9006,-630.149 313.8334,-614.7597 312.169,-602.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"315.5873,-601.5266 310.7871,-592.0817 308.6497,-602.4586 315.5873,-601.5266\"/>\n</g>\n<!-- 140399874560160 -->\n<g id=\"node34\" class=\"node\">\n<title>140399874560160</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"398,-732 315,-732 315,-701 398,-701 398,-732\"/>\n<text text-anchor=\"middle\" x=\"356.5\" y=\"-708\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128, 128)</text>\n</g>\n<!-- 140399874560160&#45;&gt;140399877858448 -->\n<g id=\"edge32\" class=\"edge\">\n<title>140399874560160&#45;&gt;140399877858448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M347.6865,-700.9604C342.1703,-691.2345 335.0278,-678.6411 329.1892,-668.3468\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"332.0645,-666.3218 324.0866,-659.3501 325.9756,-669.7752 332.0645,-666.3218\"/>\n</g>\n<!-- 140399877960656 -->\n<g id=\"node35\" class=\"node\">\n<title>140399877960656</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"393,-458 322,-458 322,-439 393,-439 393,-458\"/>\n<text text-anchor=\"middle\" x=\"357.5\" y=\"-446\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140399877960656&#45;&gt;140399877960272 -->\n<g id=\"edge33\" class=\"edge\">\n<title>140399877960656&#45;&gt;140399877960272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M348.9035,-438.9005C339.547,-428.4525 324.3948,-411.5325 312.9599,-398.7635\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"315.3592,-396.1963 306.0806,-391.0817 310.1445,-400.8661 315.3592,-396.1963\"/>\n</g>\n<!-- 140399877858064 -->\n<g id=\"node36\" class=\"node\">\n<title>140399877858064</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"417,-525 316,-525 316,-506 417,-506 417,-525\"/>\n<text text-anchor=\"middle\" x=\"366.5\" y=\"-513\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877858064&#45;&gt;140399877960656 -->\n<g id=\"edge34\" class=\"edge\">\n<title>140399877858064&#45;&gt;140399877960656</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M365.2105,-505.9005C363.9006,-496.149 361.8334,-480.7597 360.169,-468.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"363.5873,-467.5266 358.7871,-458.0817 356.6497,-468.4586 363.5873,-467.5266\"/>\n</g>\n<!-- 140399874564000 -->\n<g id=\"node37\" class=\"node\">\n<title>140399874564000</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"446,-598 363,-598 363,-567 446,-567 446,-598\"/>\n<text text-anchor=\"middle\" x=\"404.5\" y=\"-574\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128, 128)</text>\n</g>\n<!-- 140399874564000&#45;&gt;140399877858064 -->\n<g id=\"edge35\" class=\"edge\">\n<title>140399874564000&#45;&gt;140399877858064</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M395.6865,-566.9604C390.1703,-557.2345 383.0278,-544.6411 377.1892,-534.3468\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.0645,-532.3218 372.0866,-525.3501 373.9756,-535.7752 380.0645,-532.3218\"/>\n</g>\n<!-- 140399877960336 -->\n<g id=\"node38\" class=\"node\">\n<title>140399877960336</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"441,-324 370,-324 370,-305 441,-305 441,-324\"/>\n<text text-anchor=\"middle\" x=\"405.5\" y=\"-312\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140399877960336&#45;&gt;140399877959760 -->\n<g id=\"edge36\" class=\"edge\">\n<title>140399877960336&#45;&gt;140399877959760</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M396.7602,-304.9005C387.2478,-294.4525 371.843,-277.5325 360.2175,-264.7635\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"362.5439,-262.1198 353.2236,-257.0817 357.3678,-266.8324 362.5439,-262.1198\"/>\n</g>\n<!-- 140399877857680 -->\n<g id=\"node39\" class=\"node\">\n<title>140399877857680</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"465,-391 364,-391 364,-372 465,-372 465,-391\"/>\n<text text-anchor=\"middle\" x=\"414.5\" y=\"-379\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399877857680&#45;&gt;140399877960336 -->\n<g id=\"edge37\" class=\"edge\">\n<title>140399877857680&#45;&gt;140399877960336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M413.2105,-371.9005C411.9006,-362.149 409.8334,-346.7597 408.169,-334.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"411.5873,-333.5266 406.7871,-324.0817 404.6497,-334.4586 411.5873,-333.5266\"/>\n</g>\n<!-- 140399874562080 -->\n<g id=\"node40\" class=\"node\">\n<title>140399874562080</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"494,-464 411,-464 411,-433 494,-433 494,-464\"/>\n<text text-anchor=\"middle\" x=\"452.5\" y=\"-440\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (128, 128)</text>\n</g>\n<!-- 140399874562080&#45;&gt;140399877857680 -->\n<g id=\"edge38\" class=\"edge\">\n<title>140399874562080&#45;&gt;140399877857680</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M443.6865,-432.9604C438.1703,-423.2345 431.0278,-410.6411 425.1892,-400.3468\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"428.0645,-398.3218 420.0866,-391.3501 421.9756,-401.7752 428.0645,-398.3218\"/>\n</g>\n<!-- 140399877960016 -->\n<g id=\"node41\" class=\"node\">\n<title>140399877960016</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"492,-196 421,-196 421,-177 492,-177 492,-196\"/>\n<text text-anchor=\"middle\" x=\"456.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140399877960016&#45;&gt;140399877932240 -->\n<g id=\"edge39\" class=\"edge\">\n<title>140399877960016&#45;&gt;140399877932240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M437.862,-176.9197C421.1638,-168.3365 396.4402,-155.6281 377.3882,-145.8351\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"378.7773,-142.6138 368.2834,-141.155 375.5772,-148.8395 378.7773,-142.6138\"/>\n</g>\n<!-- 140399874831696 -->\n<g id=\"node42\" class=\"node\">\n<title>140399874831696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"512,-257 411,-257 411,-238 512,-238 512,-257\"/>\n<text text-anchor=\"middle\" x=\"461.5\" y=\"-245\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140399874831696&#45;&gt;140399877960016 -->\n<g id=\"edge40\" class=\"edge\">\n<title>140399874831696&#45;&gt;140399877960016</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M460.7188,-237.9688C460.0257,-229.5131 458.991,-216.8901 458.1198,-206.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"461.5976,-205.8462 457.2923,-196.1656 454.621,-206.4182 461.5976,-205.8462\"/>\n</g>\n<!-- 140399874561760 -->\n<g id=\"node43\" class=\"node\">\n<title>140399874561760</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"536,-330 459,-330 459,-299 536,-299 536,-330\"/>\n<text text-anchor=\"middle\" x=\"497.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 128)</text>\n</g>\n<!-- 140399874561760&#45;&gt;140399874831696 -->\n<g id=\"edge41\" class=\"edge\">\n<title>140399874561760&#45;&gt;140399874831696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M489.1504,-298.9604C483.9245,-289.2345 477.1579,-276.6411 471.6266,-266.3468\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"474.6089,-264.5024 466.7926,-257.3501 468.4427,-267.8156 474.6089,-264.5024\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE5i_9z_fSKp",
        "outputId": "607305a4-baf2-4e97-f2c1-3de6bd00f9d2"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}