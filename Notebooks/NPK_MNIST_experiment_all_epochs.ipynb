{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshyadav007/research/blob/main/NPK_MNIST_experiment_all_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT0drpoPq8Zj"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlvMamTR1KuX",
        "outputId": "e62fa121-7d35-40ad-8f68-0a5aad97fe4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.37.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-D1NaAQiLtE"
      },
      "outputs": [],
      "source": [
        "# !pip install matplotlib --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLLVL4t-zk8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import torch \n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split \n",
        "import seaborn as sns\n",
        "#from torchviz import make_dot\n",
        "# from google.colab import files\n",
        "from scipy.linalg import eigh\n",
        "import gc\n",
        "# %matplotlib inline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "%matplotlib inline\n",
        "# matplotlib.use('agg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s82wzBl2uimR"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import pairwise_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t9ztNdKpP8w"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, svm, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3o1H1_Gu4iI"
      },
      "outputs": [],
      "source": [
        "# import numpy as np \n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tcijr0Lq35P"
      },
      "source": [
        "#Seed Setter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwD9vbnrraWL"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  \n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  # torch.use_deterministic_algorithms(True)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  # Python RNG\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXWpj9Z5qVke"
      },
      "source": [
        "#Making Dataset\n",
        "\n",
        "*   CIFAR 10\n",
        "*   MNIST\n",
        "*   L1 \n",
        "*   Boxed\n",
        "*   Cube\n",
        "*   Circle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgLT-lFguTTX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def circle_dataset_routine(n_data, a, b, seed, plot):\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data)\n",
        "\n",
        "  x=np.zeros((len(angles),2))\n",
        "  y=np.zeros(len(angles))\n",
        "\n",
        "  #a := Number of half cycles in the top half of the circle\n",
        "  #b := number of half cycles in the bottom half of the circle\n",
        "\n",
        "  for idx, angle in enumerate(angles):\n",
        "      x[idx,0]=np.cos(angle)\n",
        "      x[idx,1]=np.sin(angle)\n",
        "      if angle<np.pi:\n",
        "          y[idx] = np.sin(a*angle)\n",
        "      else:\n",
        "          y[idx] = np.sin(a*np.pi+b*(angle-np.pi))   \n",
        "\n",
        "  y = np.float32(y)\n",
        "  \n",
        "  if plot:\n",
        "    plt.plot(angles, y)\n",
        "    plt.xlabel(\"angle in radians\")\n",
        "    plt.ylabel(\"label function (y)\")\n",
        "    plt.figure()\n",
        "    plt.axis('equal')\n",
        "    plt.scatter(x[:,0],x[:,1],c=y)\n",
        "    plt.colorbar()\n",
        "    \n",
        "  return x, np.squeeze(y)\n",
        "def circle_dataset(dataset_args):\n",
        "  n_data, a, b, plot = dataset_args['n_data'], dataset_args['a'], dataset_args['b'], dataset_args['plot'] \n",
        "  x_train, y_train = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = plot)\n",
        "  x_val, y_val = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "  x_test, y_test = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ihN5vlCgrD0"
      },
      "outputs": [],
      "source": [
        "def circle_dataset_routine(n_data, a, b, seed, plot):\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data)\n",
        "\n",
        "  x=np.zeros((len(angles),2))\n",
        "  y=np.zeros(len(angles))\n",
        "\n",
        "  #a := Number of half cycles in the top half of the circle\n",
        "  #b := number of half cycles in the bottom half of the circle\n",
        "\n",
        "  for idx, angle in enumerate(angles):\n",
        "      x[idx,0]=np.cos(angle)\n",
        "      x[idx,1]=np.sin(angle)\n",
        "      if angle<np.pi:\n",
        "          y[idx] = np.sin(a*angle)\n",
        "      else:\n",
        "          y[idx] = np.sin(a*np.pi+b*(angle-np.pi))   \n",
        "\n",
        "  y = np.float32(y)\n",
        "  \n",
        "  if plot:\n",
        "    plt.plot(angles, y)\n",
        "    plt.xlabel(\"angle in radians\")\n",
        "    plt.ylabel(\"label function (y)\")\n",
        "    # plt.savefig(\"circle.pdf\", format = 'pdf')\n",
        "    plt.figure()\n",
        "    plt.axis('equal')\n",
        "    plt.scatter(x[:,0],x[:,1],c=y)\n",
        "    plt.colorbar()\n",
        "    # plt.savefig(\"circle_label_func.pdf\", format = 'pdf')\n",
        "    \n",
        "  return x, np.squeeze(y)\n",
        "def circle_dataset(dataset_args):\n",
        "  n_data, a, b, plot = dataset_args['n_data'], dataset_args['a'], dataset_args['b'], dataset_args['plot'] \n",
        "  x_train, y_train = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = plot)\n",
        "  x_val, y_val = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "  x_test, y_test = circle_dataset_routine(n_data = n_data,a = a,b = b, seed = None, plot = False)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bB2f--FokVV"
      },
      "outputs": [],
      "source": [
        "# _ = circle_dataset(dataset_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ojRKBSxCczl"
      },
      "outputs": [],
      "source": [
        "def binary_cifar_dataset_routine(size, is_train,is_binary, flattened):\n",
        "  data = torchvision.datasets.CIFAR10(root = '/CIFAR10', train = is_train, download = True,  transform=ToTensor())\n",
        "  X, Y = [], []\n",
        "  for idx, d in enumerate(data):\n",
        "    X.append(d[0].numpy())\n",
        "    Y.append(d[1])\n",
        "  Y = np.expand_dims(Y, axis = 1)\n",
        "  X = np.array(X)\n",
        "  # X = np.squeeze(X, axis = 1)\n",
        "  if is_binary :\n",
        "    X = X[np.logical_or(Y==4, Y == 8).squeeze()]\n",
        "    Y = Y[np.logical_or(Y==4, Y == 8)]\n",
        "    Y[Y == 4], Y[Y == 8] = 0, 1\n",
        "  if flattened : \n",
        "    X = X.reshape((X.shape[0], -1))\n",
        "  return X, np.squeeze(Y)\n",
        "\n",
        "def binary_cifar_dataset(dataset_args):\n",
        "  flattened, is_binary = dataset_args['flattened'], dataset_args['is_binary']\n",
        "  n_data = 0\n",
        "  x_train, y_train = binary_cifar_dataset_routine(n_data, is_train = True,is_binary = is_binary, flattened = flattened)\n",
        "  x_val, y_val = None, None\n",
        "  x_test, y_test = binary_cifar_dataset_routine(n_data, is_train = False,is_binary = is_binary, flattened = flattened)\n",
        "  print(x_train.shape, y_train.shape)\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKig7wTFfZ9p"
      },
      "outputs": [],
      "source": [
        "def binary_mnist_dataset_routine(size, is_train):\n",
        "  mnist_train_data = torchvision.datasets.MNIST(root = '/MNIST', train = is_train, download = True,  transform=ToTensor())\n",
        "  X, Y = [], []\n",
        "  for idx, data in enumerate(mnist_train_data):\n",
        "    X.append(torch.flatten(data[0], start_dim=1, end_dim=- 1).numpy())\n",
        "    Y.append(data[1])\n",
        "  Y = np.expand_dims(Y, axis = 1)\n",
        "  X = np.array(X)\n",
        "  # X = np.squeeze(X, axis = 1)\n",
        "  X = X[np.logical_or(Y==0, Y == 1)]\n",
        "  Y = Y[np.logical_or(Y==0, Y == 1)]\n",
        "  # Y[Y==1] = 0\n",
        "  # Y[Y == 7] = 1\n",
        "  return X, np.squeeze(Y)\n",
        "\n",
        "def binary_mnist_dataset(*args):\n",
        "  n_data = None\n",
        "  x_train, y_train = binary_mnist_dataset_routine(n_data, is_train = True)\n",
        "  x_val, y_val = None, None\n",
        "  x_test, y_test = binary_mnist_dataset_routine(n_data, is_train = False)\n",
        "  print(x_train.shape, y_train.shape)\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUPXqUU68xuj"
      },
      "outputs": [],
      "source": [
        "def boxed_complex_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(10)\n",
        "  x1 = rng.random((size,1))*(2*2*np.pi) - 2*np.pi\n",
        "  x2 = rng.random((size,1))*(2*np.pi) - np.pi\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y1 = np.sign(np.sin(2*x[:,0]))\n",
        "  y2 = np.sign(np.sin(x[:,1]))\n",
        "  y = y1*y2\n",
        "  y1[y1 == -1] = 0\n",
        "  y2[y2 == -1] = 0\n",
        "  y[y == -1] = 0\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def boxed_complex_dataset():\n",
        "  n_data = 2000\n",
        "  x_train, y_train = boxed_complex_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = boxed_complex_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = boxed_complex_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1yQzgmwmaV0"
      },
      "outputs": [],
      "source": [
        "def boxed_complex_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(10)\n",
        "  x1 = rng.random((size,1))*(2*2*np.pi) - 2*np.pi\n",
        "  x2 = rng.random((size,1))*(2*np.pi) - np.pi\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y1 = np.sign(np.sin(2*x[:,0]))\n",
        "  y2 = np.sign(np.sin(x[:,1]))\n",
        "  y = y1*y2\n",
        "  y1[y1 == -1] = 0\n",
        "  y2[y2 == -1] = 0\n",
        "  y[y == -1] = 0\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def boxed_complex_dataset():\n",
        "  n_data = 2000\n",
        "  x_train, y_train = boxed_complex_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = boxed_complex_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = boxed_complex_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini\n",
        "def boxed_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  x1 = rng.random((size,1))*(2*2*np.pi) - 2*np.pi\n",
        "  x2 = rng.random((size,1))*(2*np.pi) - np.pi\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y = np.sign(np.sin(2*x[:,0]))\n",
        "  # y2 = np.sign(np.sin(x[:,1]))\n",
        "  # y = y1*y2\n",
        "  # y1[y1 == -1] = 0\n",
        "  # y2[y2 == -1] = 0\n",
        "  y[y == -1] = 0\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def boxed_dataset(*args):\n",
        "  n_data = 1000\n",
        "  x_train, y_train = boxed_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = boxed_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = boxed_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpKkE-vQfro3"
      },
      "outputs": [],
      "source": [
        "def union_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  x1 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x2 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  \n",
        "  w1, w2 = np.array([[-1 ],[1]]), np.array([[1 ],[-1]])\n",
        "  w1, w2 = w1/np.linalg.norm(w1), w2/np.linalg.norm(w2)\n",
        "  y1 = np.sign(np.matmul(x, w1) - np.sqrt(2)/4.5)\n",
        "  y2 = np.sign(np.matmul(x, w2) +  2*np.sqrt(2)/3)\n",
        "  y3 = np.sign(np.matmul(x, w1) + 2* np.sqrt(2)/3)\n",
        "  y4 = np.sign(np.matmul(x, w2) -  np.sqrt(2)/4.5)\n",
        "  # y1[y1 == -1],y2[y2 == -1]  = 0, 0\n",
        "  y = y1 + y2 + y3 + y4\n",
        "  x[:,1] = x[:,1] + 3\n",
        "  X, Y = x, y\n",
        "  \n",
        "  x1 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x2 = rng.uniform(low = -1, high = 1, size = (size,1))\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  w1, w2 = np.array([[1 ],[1]]), np.array([[-1 ],[-1]])\n",
        "  w1, w2 = w1/np.linalg.norm(w1), w2/np.linalg.norm(w2)\n",
        "  y1 = np.sign(np.matmul(x, w1) - np.sqrt(2)/4.5)\n",
        "  y2 = np.sign(np.matmul(x, w2) +  2*np.sqrt(2)/3)\n",
        "  y3 = np.sign(np.matmul(x, w1) + 2* np.sqrt(2)/3)\n",
        "  y4 = np.sign(np.matmul(x, w2) -  np.sqrt(2)/4.5)\n",
        "  y = y1 + y2 + y3 + y4\n",
        "  \n",
        "  X, Y = np.concatenate((X,x), axis = 0), np.concatenate((Y,y), axis = 0)\n",
        "  Y[Y==2] = 1\n",
        "  plt.scatter(X[:,0], X[:,1], c = Y)\n",
        "  plt.axis('equal')\n",
        "  return X, np.squeeze(Y)\n",
        "def union_dataset():\n",
        "  n_data = 2000\n",
        "  x_train, y_train = union_dataset_routine(n_data, 1)\n",
        "  x_val, y_val = union_dataset_routine(n_data, 2)\n",
        "  x_test, y_test = union_dataset_routine(n_data, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU1lLoOpF-Hw"
      },
      "outputs": [],
      "source": [
        "def L1_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(10)\n",
        "  x1 = rng.random((1000,1))*(2) - 1\n",
        "  x2 = rng.random((1000,1))*(2) - 1\n",
        "  # scaler = np.linalg.norm(np.concatenate((x1, x2), axis = 1), ord = 1, axis = 1, keepdims = True) * 1/.5\n",
        "  x = np.concatenate((x1, x2), axis = 1)\n",
        "  y = np.linalg.norm(np.concatenate((x1, x2), axis = 1), ord = 1, axis = 1, keepdims = True) >= 1\n",
        "  y = y*1\n",
        "  plt.scatter(x[:,0], x[:,1], c = y)\n",
        "  plt.axis('equal')\n",
        "  return x, np.squeeze(y)\n",
        "def L1_dataset():\n",
        "  n_data = 1000\n",
        "  x_train, y_train = L1_dataset_routine(1000, 1)\n",
        "  x_val, y_val = L1_dataset_routine(1000, 2)\n",
        "  x_test, y_test = L1_dataset_routine(1000, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSRjT4qo3Srv"
      },
      "outputs": [],
      "source": [
        "def parabola_dataset_routine(size, seed):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  x1 = rng.random((size,1))*(2) - 1\n",
        "  x2 = rng.random((size,1))*(2) - .75\n",
        "  y =  x2 >= x1**2/(1)\n",
        "  y = y*1\n",
        "  return np.concatenate((x1, x2), axis = 1), np.squeeze(y)\n",
        "def parabola_dataset():\n",
        "  n_data = 1000\n",
        "  x_train, y_train = parabola_dataset_routine(1000, 1)\n",
        "  x_val, y_val = parabola_dataset_routine(1000, 2)\n",
        "  x_test, y_test = parabola_dataset_routine(1000, 3)\n",
        "\n",
        "  x_mini, y_mini = x_test[:500], y_test[:500]\n",
        "  \n",
        "  return x_train, y_train, x_val, y_val, x_test, y_test,  x_mini, y_mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQc42HwrNKoO"
      },
      "outputs": [],
      "source": [
        "def get_dist(test_data_curr):\n",
        "  data = test_data_curr\n",
        "  dist = []\n",
        "  start = 0\n",
        "  end = 400\n",
        "  for n in range(6):\n",
        "    \n",
        "    if n == 0 or n == 1:\n",
        "      dist.append(np.minimum(np.minimum(np.abs(data[start:end,2] - .5), np.abs(data[start:end,2] + .5)), np.minimum(np.abs(data[start:end,1] - .5), np.abs(data[start:end,1] + .5))))\n",
        "    if n == 2 or n == 3:\n",
        "      dist.append(np.minimum(np.minimum(np.abs(data[start:end,0] - .5), np.abs(data[start:end,0] + .5)), np.minimum(np.abs(data[start:end,2] - .5), np.abs(data[start:end,2] + .5))))\n",
        "    if n == 4 or n == 5:\n",
        "      dist.append(np.minimum(np.minimum(np.abs(data[start:end,0] - .5), np.abs(data[start:end,0] + .5)), np.minimum(np.abs(data[start:end,1] - .5), np.abs(data[start:end,1] + .5))))\n",
        "    start = end\n",
        "    end = end + 400\n",
        "  dist = np.concatenate(dist)\n",
        "  return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V61tbiSjeM3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPuUqfIWc9wi"
      },
      "outputs": [],
      "source": [
        "num_modes=6\n",
        "d=num_modes//2\n",
        "def get_cube_data():\n",
        "  num_modes=6 # Each mode is a d-1 dimensional hyperplane perpendicular to one of the exes, and intercept at +b or -b\n",
        "  d=num_modes//2 # Choose d to be a multiple of num_modes\n",
        "  d=3\n",
        "  num_modes=6\n",
        "  a=[2]*6\n",
        "  mode_frac = np.array([1./num_modes]*num_modes)\n",
        "  b=1.\n",
        "\n",
        "  num_train_data = 2400 \n",
        "  num_vali_data=2400\n",
        "  num_test_data=2400\n",
        "  num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "  num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "  num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "  landmarks=[-1]*num_modes\n",
        "  labels=[-1]*num_modes\n",
        "  for i in range(num_modes):\n",
        "      landmarks[i] = np.random.randn(a[i],d)\n",
        "      \n",
        "      landmarks[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(a[i])\n",
        "      print(\"Colum \", str(i//2),\"of mode\", str(i), \"with\", str(2*b*((i%2)-0.5)*np.ones(a[i])))\n",
        "      landmarks[i]/=2.\n",
        "      labels[i] = (np.arange(len(landmarks[i])))%2\n",
        "\n",
        "  data=[-1]*num_modes\n",
        "  data_labels=[-1]*num_modes\n",
        "\n",
        "  train_data=[-1]*num_modes\n",
        "  train_data_labels=[-1]*num_modes\n",
        "\n",
        "  test_data=[-1]*num_modes\n",
        "  test_data_labels=[-1]*num_modes\n",
        "\n",
        "  vali_data=[-1]*num_modes\n",
        "  vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "  modes_data=[]\n",
        "  for i in range(num_modes):\n",
        "      data[i] = np.random.randn(num_data_per_mode[i],d)\n",
        "      data[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(num_data_per_mode[i])\n",
        "      data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "      for j in range(len(data_labels[i])):\n",
        "          dists = pairwise_distances(data[i][j:j+1,:],landmarks[i])                                   \n",
        "          j_star = np.argmin(dists[0])\n",
        "          data_labels[i][j]=labels[i][j_star]\n",
        "      \n",
        "      data[i]/=2.\n",
        "      train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "      train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "      vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "      vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "      test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "      test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "  train_data_curr = np.concatenate(train_data)\n",
        "  train_labels_curr = np.concatenate(train_data_labels)\n",
        "  vali_data_curr = np.concatenate(vali_data)\n",
        "  vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "  test_data_curr = np.concatenate(test_data)\n",
        "  test_labels_curr = np.concatenate(test_data_labels)\n",
        "  dist = get_dist(test_data_curr)\n",
        "  sorted_idx = np.argsort(dist)\n",
        "  sorted_idx = np.concatenate((sorted_idx[:250], sorted_idx[-250:]))\n",
        "\n",
        "  mini_test_data = np.squeeze(test_data_curr[sorted_idx])\n",
        "  mini_test_labels = np.squeeze(test_labels_curr[sorted_idx])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  # num_modes=6\n",
        "  # d=num_modes//2 # Choose d to be a multiple of num_modes\n",
        "  # a=[2]*(num_modes-1) + [40]\n",
        "  # mode_frac = np.array([0.25/(num_modes-1)]*(num_modes-1)+ [0.75])\n",
        "\n",
        "  # label_noise_frac=[0.0]*(num_modes-1)+[0.0]\n",
        "  # instance_noise_stdev = [0.0]*num_modes\n",
        "\n",
        "  # # These two numbers make sense only if d=2*num_modes\n",
        "  # corrs = [0.0]*num_modes\n",
        "  # nn_transform = [np.diag([1.,1.]*num_modes)]*num_modes  \n",
        "\n",
        "  # num_train_data = 640 \n",
        "  # num_vali_data=2000\n",
        "  # num_test_data=2000\n",
        "  # num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "  # num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "  # num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "  # landmarks=[-1]*num_modes\n",
        "  # labels=[-1]*num_modes\n",
        "  # for i in range(num_modes):\n",
        "  #     landmarks[i] = np.zeros((a[i],d))\n",
        "  #     thetas = np.arange(0,2*np.pi-0.01,2*np.pi/a[i])\n",
        "  #     temp = np.concatenate((np.cos(thetas)[:,None], np.sin(thetas)[:,None]),axis=1)\n",
        "      \n",
        "\n",
        "  #     landmarks[i][:, i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "  #     if i<num_modes-1:\n",
        "  #         labels[i] = np.ones((len(landmarks[i])))*(i)%2        \n",
        "  #     else:\n",
        "  #         labels[i] = (np.arange(len(landmarks[i])) + i)%2\n",
        "\n",
        "  # labels_all_modes = np.concatenate(labels, axis=0)\n",
        "  # landmarks_all_modes = np.concatenate(landmarks, axis=0)\n",
        "\n",
        "  # data=[-1]*num_modes\n",
        "  # data_labels=[-1]*num_modes\n",
        "\n",
        "  # train_data=[-1]*num_modes\n",
        "  # train_data_labels=[-1]*num_modes\n",
        "\n",
        "  # test_data=[-1]*num_modes\n",
        "  # test_data_labels=[-1]*num_modes\n",
        "\n",
        "  # vali_data=[-1]*num_modes\n",
        "  # vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "  # modes_data=[]\n",
        "  # for i in range(num_modes):\n",
        "  #     data[i] = np.zeros((num_data_per_mode[i],d))    \n",
        "  #     temp = np.random.randn(num_data_per_mode[i], d//num_modes)\n",
        "  #     temp = temp/np.sqrt(np.sum(temp**2, axis=1)[:,None])\n",
        "\n",
        "  #     data[i][:,i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "  #     data[i] += np.random.randn(num_data_per_mode[i],d)*instance_noise_stdev[i]\n",
        "  #     data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "  #     for j in range(len(data_labels[i])):\n",
        "  #         dists = pairwise_distances(np.dot(data[i][j:j+1,:], nn_transform[i]) \n",
        "  #                                   ,np.dot(landmarks_all_modes, nn_transform[i]))\n",
        "  #         j_star = np.argmin(dists[0])\n",
        "  #         data_labels[i][j]=labels_all_modes[j_star]\n",
        "  #         modes_data.append(i)\n",
        "  #     data_labels[i] = (data_labels[i] + np.int32(np.random.rand(len(data_labels[i]))<label_noise_frac[i]) ) % 2\n",
        "\n",
        "  #     data[i][:,i*d//num_modes:(i+1)*d//num_modes] = np.dot(data[i][:,i*d//num_modes:(i+1)*d//num_modes], \n",
        "  #                                                           np.array([[1., corrs[i]],[corrs[i], 1.]]))\n",
        "      \n",
        "  #     train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "  #     train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "  #     vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "  #                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "  #     vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "  #                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "  #     test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "  #     test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "  # train_data_curr = np.concatenate(train_data)\n",
        "  # train_labels_curr = np.concatenate(train_data_labels)\n",
        "  # vali_data_curr = np.concatenate(vali_data)\n",
        "  # vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "  # test_data_curr = np.concatenate(test_data)\n",
        "  # test_labels_curr = np.concatenate(test_data_labels)\n",
        "  # dist = get_dist(test_data_curr)\n",
        "  # sorted_idx = np.argsort(dist)\n",
        "  # sorted_idx = np.concatenate((sorted_idx[:250], sorted_idx[-250:]))\n",
        "  # mini_test_data = np.squeeze(test_data_curr[sorted_idx])\n",
        "  # mini_test_labels = np.squeeze(test_labels_curr[sorted_idx])\n",
        "  return train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMuCcghYkkGS"
      },
      "outputs": [],
      "source": [
        "# train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels = get_cube_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTAFOg51Djm4"
      },
      "outputs": [],
      "source": [
        "# num_modes=6 # Each mode is a d-1 dimensional hyperplane perpendicular to one of the exes, and intercept at +b or -b\n",
        "# d=num_modes//2 # Choose d to be a multiple of num_modes\n",
        "# d=3\n",
        "# num_modes=6\n",
        "# a=[2]*6\n",
        "# mode_frac = np.array([1./num_modes]*num_modes)\n",
        "# b=1.\n",
        "\n",
        "# num_train_data = 2400 \n",
        "# num_vali_data=2400\n",
        "# num_test_data=2400\n",
        "# num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "# num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "# num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "# landmarks=[-1]*num_modes\n",
        "# labels=[-1]*num_modes\n",
        "# for i in range(num_modes):\n",
        "#     landmarks[i] = np.random.randn(a[i],d)\n",
        "    \n",
        "#     landmarks[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(a[i])\n",
        "#     print(\"Colum \", str(i//2),\"of mode\", str(i), \"with\", str(2*b*((i%2)-0.5)*np.ones(a[i])))\n",
        "#     landmarks[i]/=2.\n",
        "#     labels[i] = (np.arange(len(landmarks[i])))%2\n",
        "\n",
        "# data=[-1]*num_modes\n",
        "# data_labels=[-1]*num_modes\n",
        "\n",
        "# train_data=[-1]*num_modes\n",
        "# train_data_labels=[-1]*num_modes\n",
        "\n",
        "# test_data=[-1]*num_modes\n",
        "# test_data_labels=[-1]*num_modes\n",
        "\n",
        "# vali_data=[-1]*num_modes\n",
        "# vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "# modes_data=[]\n",
        "# for i in range(num_modes):\n",
        "#     data[i] = np.random.randn(num_data_per_mode[i],d)\n",
        "#     data[i][:, i//2] = 2*b*((i%2)-0.5)*np.ones(num_data_per_mode[i])\n",
        "#     data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "#     for j in range(len(data_labels[i])):\n",
        "#         dists = pairwise_distances(data[i][j:j+1,:],landmarks[i])                                   \n",
        "#         j_star = np.argmin(dists[0])\n",
        "#         data_labels[i][j]=labels[i][j_star]\n",
        "    \n",
        "#     data[i]/=2.\n",
        "#     train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "#     train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "#     vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "#     vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                     int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "#     test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "#     test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "    \n",
        "# train_data_curr = np.concatenate(train_data)\n",
        "# train_labels_curr = np.concatenate(train_data_labels)\n",
        "# vali_data_curr = np.concatenate(vali_data)\n",
        "# vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "# test_data_curr = np.concatenate(test_data)\n",
        "# test_labels_curr = np.concatenate(test_data_labels)\n",
        "# dist = get_dist(test_data_curr)\n",
        "# sorted_idx = np.argsort(dist)\n",
        "# sorted_idx = np.concatenate((sorted_idx[:250], sorted_idx[-250:]))\n",
        "\n",
        "# mini_test_data = np.squeeze(test_data_curr[sorted_idx])\n",
        "# mini_test_labels = np.squeeze(test_labels_curr[sorted_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO4gKIHyelNN"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "# st = 0\n",
        "# end = st+250\n",
        "# x, y, z = zip(*mini_test_data[st:end,:])\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(xs = x, ys = y,zs = z, c = mini_test_labels[st:end])\n",
        "# # plt.scatter(x = y, y = z, c = train_labels_curr[st:end])\n",
        "# # plt.plot(landmarks[1][:, 1], landmarks[1][:, 2])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTddmHtchLAj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_sY6pCvhJdy"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# x, y, z = zip(*train_data_curr[400:800,:])\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(xs = x, ys = y,zs = z, c = test_labels_curr[400:800])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf8aH2MKhWvv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaS0b1pzEOFK"
      },
      "outputs": [],
      "source": [
        "# num_modes=21\n",
        "# d=2*num_modes\n",
        "\n",
        "# def get_multi_modes_data():\n",
        "#   num_modes=21\n",
        "#   d=2*num_modes # Choose d to be a multiple of num_modes\n",
        "#   a=[2]*(num_modes-1) + [40]\n",
        "#   mode_frac = np.array([0.5/(num_modes-1)]*(num_modes-1)+ [0.5])\n",
        "\n",
        "#   label_noise_frac=[0.05]*num_modes\n",
        "#   instance_noise_stdev = [0.0]*num_modes\n",
        "\n",
        "#   # These two numbers make sense only if d=2*num_modes\n",
        "#   corrs = [0.0]*num_modes\n",
        "#   nn_transform = [np.diag([1.,1.]*num_modes)]*num_modes  \n",
        "\n",
        "#   num_train_data = 640 \n",
        "#   num_vali_data=2000\n",
        "#   num_test_data=2000\n",
        "#   num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "\n",
        "\n",
        "#   num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "#   num_data_per_mode=np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "\n",
        "#   landmarks=[-1]*num_modes\n",
        "#   labels=[-1]*num_modes\n",
        "#   for i in range(num_modes):\n",
        "#       landmarks[i] = np.zeros((a[i],d))\n",
        "#       thetas = np.arange(0,2*np.pi-0.01,2*np.pi/a[i])\n",
        "#       temp = np.concatenate((np.cos(thetas)[:,None], np.sin(thetas)[:,None]),axis=1)\n",
        "      \n",
        "\n",
        "#       landmarks[i][:, i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "#       labels[i] = (np.arange(len(landmarks[i])) + i)%2\n",
        "#   labels_all_modes = np.concatenate(labels, axis=0)\n",
        "#   landmarks_all_modes = np.concatenate(landmarks, axis=0)\n",
        "\n",
        "#   data=[-1]*num_modes\n",
        "#   data_labels=[-1]*num_modes\n",
        "\n",
        "#   train_data=[-1]*num_modes\n",
        "#   train_data_labels=[-1]*num_modes\n",
        "\n",
        "#   test_data=[-1]*num_modes\n",
        "#   test_data_labels=[-1]*num_modes\n",
        "\n",
        "#   vali_data=[-1]*num_modes\n",
        "#   vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "#   modes_data=[]\n",
        "#   for i in range(num_modes):\n",
        "#       data[i] = np.zeros((num_data_per_mode[i],d))    \n",
        "#       temp = np.random.randn(num_data_per_mode[i], d//num_modes)\n",
        "#       temp = temp/np.sqrt(np.sum(temp**2, axis=1)[:,None])\n",
        "\n",
        "#       data[i][:,i*d//num_modes:(i+1)*d//num_modes] = temp\n",
        "#       data[i] += np.random.randn(num_data_per_mode[i],d)*instance_noise_stdev[i]\n",
        "#       data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "#       for j in range(len(data_labels[i])):\n",
        "#           dists = pairwise_distances(np.dot(data[i][j:j+1,:], nn_transform[i]) \n",
        "#                                     ,np.dot(landmarks_all_modes, nn_transform[i]))\n",
        "#           j_star = np.argmin(dists[0])\n",
        "#           data_labels[i][j]=labels_all_modes[j_star]\n",
        "#           modes_data.append(i)\n",
        "#       data_labels[i] = (data_labels[i] + np.int32(np.random.rand(len(data_labels[i]))<label_noise_frac[i]) ) % 2\n",
        "\n",
        "#       data[i][:,i*d//num_modes:(i+1)*d//num_modes] = np.dot(data[i][:,i*d//num_modes:(i+1)*d//num_modes], \n",
        "#                                                             np.array([[1., corrs[i]],[corrs[i], 1.]]))\n",
        "      \n",
        "#       train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "#       train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "#       vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                       int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "#       vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "#                                       int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "#       test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "#       test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "#   train_data_curr = np.concatenate(train_data)\n",
        "#   train_labels_curr = np.concatenate(train_data_labels)\n",
        "#   vali_data_curr = np.concatenate(vali_data)\n",
        "#   vali_labels_curr = np.concatenate(vali_data_labels)\n",
        "#   test_data_curr = np.concatenate(test_data)\n",
        "#   test_labels_curr = np.concatenate(test_data_labels)\n",
        "#   return train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YOEobQDibrK"
      },
      "outputs": [],
      "source": [
        "#cube data\n",
        "def get_mini_multi_modes_data(test_data_curr,num_modes, n_examples_per_mode):\n",
        "  n_data_per_mode = test_data_curr.shape[0]//num_modes\n",
        "  stride_length = n_data_per_mode\n",
        "  X = []#np.zeros((n_examples_per_mode*num_modes, d))\n",
        "  for i in range(num_modes):\n",
        "    X.append(test_data_curr[i*stride_length: (i+1)*stride_length][np.random.choice(stride_length, n_examples_per_mode, replace=False)])\n",
        "  return np.concatenate(X)\n",
        "\n",
        "#multimode data\n",
        "# def get_mini_multi_modes_data(test_data_curr,num_modes, n_examples_per_mode):\n",
        "#   n_data_per_mode = (int(test_data_curr.shape[0]/2))//num_modes\n",
        "#   stride_length = n_data_per_mode\n",
        "#   X = []#np.zeros((n_examples_per_mode*num_modes, d))\n",
        "#   for i in range(num_modes-1):\n",
        "#     X.append(test_data_curr[i*stride_length: (i+1)*stride_length][np.random.choice(stride_length, n_examples_per_mode, replace=False)])\n",
        "#   X.append(test_data_curr[1000: 2000][np.random.choice(1000, n_examples_per_mode, replace=False)])\n",
        "#   return np.concatenate(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kVQMTbyBb_H"
      },
      "outputs": [],
      "source": [
        "# def get_mini_multi_modes_data(test_data_curr,num_modes, n_examples_per_mode):\n",
        "#   n_data_per_mode = (test_data_curr.shape[0]//2)//(num_modes-1)\n",
        "#   stride_length = n_data_per_mode\n",
        "#   X = []#np.zeros((n_examples_per_mode*num_modes, d))\n",
        "#   for i in range(num_modes-1):\n",
        "#     X.append(test_data_curr[i*stride_length: (i+1)*stride_length][np.random.choice(stride_length, n_examples_per_mode, replace=False)])\n",
        "#   X.append(test_data_curr[1000: 2000][np.random.choice(1000, n_examples_per_mode, replace=False)])\n",
        "#   return np.concatenate(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUiEnLc-Ny-I"
      },
      "outputs": [],
      "source": [
        "# def get_mini_multi_modes_data(instance):\n",
        "#   instance = instance\n",
        "#   thetas = np.arange(0, 2*np.pi, 2*np.pi/instance)\n",
        "#   X = np.zeros((instance*num_modes, d))\n",
        "#   for i in range(num_modes):\n",
        "#     X[i*instance: (i+1)*instance, i*2] = np.cos(thetas)\n",
        "#     X[i*instance: (i+1)*instance, i*2+1] = np.sin(thetas)\n",
        "#   return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZUtAoX-KYnE"
      },
      "outputs": [],
      "source": [
        "class Mini(Dataset):\n",
        "      def __init__(self, X):\n",
        "          self.size = len(X)\n",
        "          self.x = X\n",
        "         \n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float()\n",
        "def get_mini_multi_mode_dataloaders(X_mini, batch_size = 100):\n",
        "  X_mini = torch.tensor(X_mini)\n",
        "  mini = Mini(X_mini)\n",
        "\n",
        "  mini_dl = torch.utils.data.DataLoader(mini,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  return mini_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFzQMtbjXCd3"
      },
      "outputs": [],
      "source": [
        "class DSphere(Dataset):\n",
        "      def __init__(self, X, Y):\n",
        "          self.size = len(Y)\n",
        "          self.x = X\n",
        "          self.y = Y\n",
        "\n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float(), self.y[idx]\n",
        "def get_mini_dsphere(X,Y,n_data, a,b):\n",
        "  \n",
        "  mini_X = []\n",
        "  mini_Y = []\n",
        "\n",
        "  rng = np.random.default_rng(0)\n",
        "  idxs = rng.choice(np.arange(0, int(n_data/2)-a),50, replace = False)\n",
        "  mini_X.extend([X[idx]  for idx in idxs])\n",
        "  mini_Y.extend([Y[idx]  for idx in idxs])\n",
        "\n",
        "  idxs = rng.choice(np.arange(int(n_data/2)-a,int(n_data)-a-b),50, replace = False)\n",
        "  mini_X.extend([X[idx]  for idx in idxs])\n",
        "  mini_Y.extend([Y[idx]  for idx in idxs])\n",
        "  \n",
        "  return mini_X, mini_Y\n",
        "\n",
        "def get_dsphere_dls(n_data, a, b,d, batch_size, is_perturbed):\n",
        "  \n",
        "  rng = np.random.default_rng(seed = 0)\n",
        "\n",
        "  n_data = n_data\n",
        "  d = d\n",
        "\n",
        "  X1 = rng.random(size = (int(n_data/2),int(d/2)))*(2) - 1 \n",
        "  X1 = X1 / np.sqrt(np.sum(np.square(X1), axis = 1, keepdims = True))\n",
        "  X1 = np.concatenate((X1, np.zeros(X1.shape)), axis = 1)\n",
        "\n",
        "  X2 = rng.random(size = (int(n_data/2),int(d/2)))*(2) - 1 \n",
        "  X2 = X2 / np.sqrt(np.sum(np.square(X2), axis = 1, keepdims = True))\n",
        "  X2 = np.concatenate(( np.zeros(X2.shape), X2), axis = 1)\n",
        "\n",
        "  X = np.concatenate((X1, X2), axis = 0)\n",
        "  Y = np.zeros((int(n_data),), dtype = int)\n",
        "  \n",
        "  # Y_true = copy.deepcopy(Y)\n",
        "\n",
        "\n",
        "  #transform Y\n",
        "  sample_idx_1 = rng.choice(a = np.arange(int(n_data/2)), size = (a,), replace = False )\n",
        "  for idx in sample_idx_1:\n",
        "    Y[idx] = rng.choice(a = [1, 0], size = (1,))\n",
        "\n",
        "  sample_idx_2 = rng.choice(a = np.arange(int(n_data/2), n_data), size = (b,), replace = False )\n",
        "  for idx in sample_idx_2:\n",
        "    Y[idx] = rng.choice(a = [1, 0], size = (1,))\n",
        "\n",
        "\n",
        "  landmarks = np.concatenate((sample_idx_1, sample_idx_2), axis = 0)  \n",
        "  mask=np.full(len(Y),False,dtype=bool)\n",
        "  mask[landmarks]=True\n",
        "  X_landmarks, Y_landmarks = np.array(X)[mask], np.array(Y)[mask]\n",
        "\n",
        "  X_data = np.array(X)[~mask]\n",
        "\n",
        "  neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "  neigh.fit(X_landmarks, Y_landmarks)\n",
        "  Y_data = neigh.predict(X_data)\n",
        "\n",
        "\n",
        "  # X_small = np.concatenate((X[sample_idx_1], X[sample_idx_2]), axis = 0)\n",
        "  # Y_small = np.concatenate((Y[sample_idx_1], Y[sample_idx_2]), axis = 0)\n",
        "\n",
        "  # neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "  # neigh.fit(X_small, Y_small)\n",
        "  # Y_true = neigh.predict(X)\n",
        "\n",
        "  mini_X, mini_Y = get_mini_dsphere(X_data, Y_data,n_data,  a, b)\n",
        "  mode1_idxs, mode2_idxs = np.arange(0,int(n_data/2)-a), np.arange(int(n_data/2)-a, n_data-a-b)\n",
        "  # print(landmarks)\n",
        "  # mode1_idxs, mode2_idxs  = list(set(mode1_idxs) - set(landmarks)), list(set(mode2_idxs) - set(landmarks))\n",
        "  X_mode1_data, Y_mode1_data = X_data[mode1_idxs], Y_data[mode1_idxs]\n",
        "  X_mode2_data, Y_mode2_data = X_data[mode2_idxs], Y_data[mode2_idxs]\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1, random_state=42)\n",
        "  X_train_m1, X_test_m1, y_train_m1, y_test_m1 = train_test_split(X_mode1_data, Y_mode1_data, test_size=0.1, random_state=42)\n",
        "  X_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X_mode2_data, Y_mode2_data, test_size=0.1, random_state=42)\n",
        "\n",
        "  return (X_train, X_test, y_train, y_test), (X_train_m1, X_test_m1, y_train_m1, y_test_m1), (X_train_m2, X_test_m2, y_train_m2, y_test_m2), (mini_X, mini_Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZtkWhsnd0Sj"
      },
      "outputs": [],
      "source": [
        "# def perturb_target(y):\n",
        "#   # if y <= 4:\n",
        "#   #   # p=[0.125, 0.125, 0.125, 0.125, 0.125]\n",
        "#   #   p = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "#   #   p[y] = 1\n",
        "#   #   return np.random.choice(5, 1,p = p)[0]\n",
        "#   # else:\n",
        "#   #   return y\n",
        "#   if y == 6 or y == 8:\n",
        "#     return 0\n",
        "#   if y >= 5:\n",
        "#     return 1\n",
        "\n",
        "#   if y <= 4:\n",
        "#     p=[.75, .25]\n",
        "#     return np.random.choice([0, 1], 1,p = p)[0]\n",
        "# def unperturb_target(y):\n",
        "#   return y\n",
        "\n",
        "# def get_mini_mnist(mnist_train_data, Y_perturbed):\n",
        "#   X, Y = [], []\n",
        "#   for idx, data in enumerate(mnist_train_data):\n",
        "#     X.append(data[0])\n",
        "#     Y.append(data[1])\n",
        "\n",
        "#   mini_X = []\n",
        "#   mini_Y = []\n",
        "\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   for i in range(10):\n",
        "#     idxs = rng.choice(np.where(np.array(Y) == i)[0],50, replace = False)\n",
        "#     mini_X.extend([X[idx]  for idx in idxs])\n",
        "#     mini_Y.extend([Y_perturbed[idx]  for idx in idxs])\n",
        "  \n",
        "#   return mini_X, mini_Y\n",
        "\n",
        "# def get_transformed_data(mnist_train_data):\n",
        "  \n",
        "#   X, Y = [], []\n",
        "#   for idx, data in enumerate(mnist_train_data):\n",
        "#     X.append(data[0])\n",
        "#     Y.append(perturb_target(data[1]))\n",
        "#   return X, Y\n",
        "\n",
        "# def get_transformed(X, Y):\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   for i in range(10):\n",
        "#     idxs = rng.choice(np.where(np.array(Y) == i)[0],50, replace = False)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oagFjEoL15t7"
      },
      "outputs": [],
      "source": [
        "def get_mini_mnist(X, Y, mode1_idxs, mode2_idxs):\n",
        "\n",
        "  mini_X = []\n",
        "  mini_Y = []\n",
        "\n",
        "  rng = np.random.default_rng(0)\n",
        "\n",
        "  \n",
        "\n",
        "  idxs = rng.choice(mode1_idxs ,50, replace = False)\n",
        "  x, y = X[idxs], Y[idxs]\n",
        "  mini_X.extend(x)\n",
        "  mini_Y.extend(y)\n",
        "\n",
        "  idxs = rng.choice(mode2_idxs ,50, replace = False)\n",
        "  x, y = X[idxs], Y[idxs]\n",
        "  mini_X.extend(x)\n",
        "  mini_Y.extend(y)\n",
        "  \n",
        "  return mini_X, mini_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG6ZO8NhM8OR"
      },
      "outputs": [],
      "source": [
        "def get_mnist_modified_data(a, b):\n",
        "  mnist_train_data = torchvision.datasets.MNIST(root = '/MNIST', train = True, download = True,  transform=ToTensor())\n",
        "\n",
        "  # X, Y = get_transformed_data(mnist_train_data)\n",
        "  a, b = a, b\n",
        "  modes = 2\n",
        "\n",
        "  X, Y = [], []\n",
        "  Y_perturbed = Y\n",
        "  for idx, data in enumerate(mnist_train_data):\n",
        "    X.append(data[0])\n",
        "    Y.append(data[1])\n",
        "  rng = np.random.default_rng(0)\n",
        "\n",
        "  landmarks = []\n",
        "  for i in range(5):\n",
        "    idxs = rng.choice(np.where(np.array(Y) == i)[0], a, replace = False)\n",
        "    landmarks.extend(idxs)\n",
        "\n",
        "  for i in range(5,10):\n",
        "    idxs = rng.choice(np.where(np.array(Y) == i)[0], b, replace = False)\n",
        "    landmarks.extend(idxs)\n",
        "\n",
        "  for i in landmarks:\n",
        "    Y[i] = rng.choice([0,1], 1, p = [.5, .5], replace = False)[0]\n",
        "\n",
        "  mask=np.full(len(Y),False,dtype=bool)\n",
        "  mask[landmarks]=True\n",
        "  X = [np.array(x).flatten() for x in X]\n",
        "  X_landmarks, Y_landmarks = np.array(X)[mask], np.array(Y)[mask]\n",
        "\n",
        "  Y_temp = np.array(Y)[~mask]\n",
        "  mode1_idxs = []\n",
        "  for i in range(5):\n",
        "    idxs = np.where(Y_temp == i)[0]\n",
        "    mode1_idxs.extend(idxs)\n",
        "\n",
        "  mode2_idxs = []\n",
        "  for i in range(5,10):\n",
        "    idxs = np.where(Y_temp == i)[0]\n",
        "    mode2_idxs.extend(idxs)\n",
        "\n",
        "\n",
        "\n",
        "  X_data = np.array(X)[~mask]\n",
        "  neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "  neigh.fit(X_landmarks, Y_landmarks)\n",
        "\n",
        "\n",
        "  Y_data = neigh.predict(X_data)\n",
        "  scaler = StandardScaler()\n",
        "  X_data = scaler.fit_transform(X_data)\n",
        "\n",
        "\n",
        "  mini_X, mini_Y = get_mini_mnist(X_data, Y_data, mode1_idxs, mode2_idxs)\n",
        "  \n",
        "  X_mode1_data, Y_mode1_data = X_data[mode1_idxs], Y_data[mode1_idxs]\n",
        "  X_mode2_data, Y_mode2_data = X_data[mode2_idxs], Y_data[mode2_idxs]\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1, random_state=42)\n",
        "  X_train_m1, X_test_m1, y_train_m1, y_test_m1 = train_test_split(X_mode1_data, Y_mode1_data, test_size=0.1, random_state=42)\n",
        "  X_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X_mode2_data, Y_mode2_data, test_size=0.1, random_state=42)\n",
        "\n",
        "  return (X_train, X_test, y_train, y_test), (X_train_m1, X_test_m1, y_train_m1, y_test_m1), (X_train_m2, X_test_m2, y_train_m2, y_test_m2), (mini_X, mini_Y)\n",
        "\n",
        "def get_dataloaders(X_train, X_test, y_train, y_test, batch_size):\n",
        "  if is_classification:\n",
        "    X_train, X_test, y_train, y_test = torch.tensor(np.array(X_train), dtype = torch.float32), torch.tensor(np.array(X_test), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.long),torch.tensor(np.array(y_test), dtype = torch.long)\n",
        "    X_train_PWC, X_test_PWC, y_train_PWC, y_test_PWC = torch.tensor(np.ones(X_train.shape), dtype = torch.float32), torch.tensor(np.ones(X_test.shape), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.long),torch.tensor(np.array(y_test), dtype = torch.long)\n",
        "  else:\n",
        "    X_train, X_test, y_train, y_test = torch.tensor(np.array(X_train), dtype = torch.float32), torch.tensor(np.array(X_test), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.float32),torch.tensor(np.array(y_test), dtype = torch.float32)\n",
        "    X_train_PWC, X_test_PWC, y_train_PWC, y_test_PWC = torch.tensor(np.ones(X_train.shape), dtype = torch.float32), torch.tensor(np.ones(X_test.shape), dtype = torch.float32),torch.tensor(np.array(y_train), dtype = torch.float32),torch.tensor(np.array(y_test), dtype = torch.float32)\n",
        "  \n",
        "  train = DATA(X_train,y_train)\n",
        "  test = DATA(X_test,y_test)\n",
        "  train_PWC = DATA(X_train_PWC,y_train)\n",
        "  test_PWC = DATA(X_test_PWC,y_test)\n",
        "  train_dl = torch.utils.data.DataLoader(train,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=True\n",
        "                                          )\n",
        "  \n",
        "  test_dl = torch.utils.data.DataLoader(test,\n",
        "                                            batch_size=len(y_test),\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  train_dl_PWC = torch.utils.data.DataLoader(train_PWC,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  \n",
        "  test_dl_PWC = torch.utils.data.DataLoader(test_PWC,\n",
        "                                            batch_size=len(y_test),\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "\n",
        "\n",
        "  return train_dl, test_dl, train_dl_PWC, test_dl_PWC\n",
        "\n",
        "def get_mini_dls(X_mini, y_mini, batch_size = 100):\n",
        "  X_mini, y_mini = torch.tensor(X_mini), torch.tensor(y_mini)\n",
        "  mnist_mini = MNIST(X_mini,y_mini)\n",
        "\n",
        "  mnist_mini_dl = torch.utils.data.DataLoader(mnist_mini,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shuffle=False\n",
        "                                          )\n",
        "  return mnist_mini_dl, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHkuKOpOaulU"
      },
      "outputs": [],
      "source": [
        "class DATA(Dataset):\n",
        "      def __init__(self, X, Y):\n",
        "          self.size = len(Y)\n",
        "          self.x = X\n",
        "          self.y = Y\n",
        "\n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float(), self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70tWh3oLnVZu"
      },
      "outputs": [],
      "source": [
        "# labels_map = {\n",
        "#     0: \"0\",\n",
        "#     1: \"1\",\n",
        "#     2: \"2\",\n",
        "#     3: \"3\",\n",
        "#     4: \"4\",\n",
        "#     5: \"5\",\n",
        "#     6: \"6\",\n",
        "#     7: \"7\",\n",
        "#     8: \"8\",\n",
        "#     9: \"9\",\n",
        "# }\n",
        "# figure = plt.figure(figsize=(8, 8))\n",
        "# cols, rows = 3, 3\n",
        "# sample_idx = 0\n",
        "# for i in range(1, cols * rows + 1):\n",
        "#     # sample_idx = torch.randint(len(mnist_train_data), size=(1,)).item()\n",
        "#     img, label = mnist_train_data[sample_idx]\n",
        "#     sample_idx += 1\n",
        "#     figure.add_subplot(rows, cols, i)\n",
        "#     plt.title(labels_map[label])\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puNEm1gA0fii"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "      def __init__(self, X, Y):\n",
        "          self.size = len(Y)\n",
        "          self.x = torch.tensor(X)\n",
        "          self.y = torch.tensor(Y)\n",
        "\n",
        "      def __len__(self):\n",
        "          return (self.size)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          #print(self.x[idx].shape,self.y[idx].shape )\n",
        "          return self.x[idx].float(), self.y[idx].float()\n",
        "\n",
        "def make_circle_dataset(n_data_points, a, b, plot = False):\n",
        "  n_data_points = n_data_points\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data_points)\n",
        "\n",
        "  x=np.zeros((len(angles),2))\n",
        "  y=np.zeros(len(angles))\n",
        "\n",
        "  a=a # Number of half cycles in the top half of the circle\n",
        "  b=b # number of half cycles in the bottom half of the circle\n",
        "\n",
        "  for idx, angle in enumerate(angles):\n",
        "      x[idx,0]=np.cos(angle)\n",
        "      x[idx,1]=np.sin(angle)\n",
        "      if angle<np.pi:\n",
        "          y[idx] = np.sin(a*angle)\n",
        "      else:\n",
        "          y[idx] = np.sin(a*np.pi+b*(angle-np.pi))   \n",
        "\n",
        "  # plt.plot(np.concatenate((angles,angles+np.pi*2)),np.concatenate((y,y)))\n",
        "  \n",
        "  if plot:\n",
        "    plt.plot(angles, y)\n",
        "    plt.figure()\n",
        "    plt.axis('equal')\n",
        "    plt.scatter(x[:,0],x[:,1],c=y)\n",
        "  return x,y\n",
        "\n",
        "def get_sorted_circle_dataset(X,Y):\n",
        "\n",
        "  angles = [angle + 2*np.pi if angle < 0 else angle for angle in np.arctan2(X[:,1],X[:,0]) ]\n",
        "\n",
        "  sorted_arg = np.argsort(angles)\n",
        "  X_sorted = X[sorted_arg]\n",
        "  Y_sorted = Y[sorted_arg]\n",
        "  Y_sorted = Y_sorted.reshape((len(Y_sorted),1))\n",
        "  return X_sorted, Y_sorted\n",
        "\n",
        "def get_tempdataloaders(X,Y):\n",
        "  batch_size = 32\n",
        "  Y = Y.reshape((len(Y),1))\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, shuffle = True)\n",
        "  train_dataloader = DataLoader(CustomDataset(X,Y), batch_size = batch_size, shuffle = True)\n",
        "  # test_dataloader = DataLoader(CustomDataset(X_test, y_test), batch_size = batch_size, shuffle = True)\n",
        "  dataloader_all = DataLoader(CustomDataset(X,Y), batch_size = len(Y), shuffle = False)\n",
        "  return train_dataloader, dataloader_all\n",
        "\n",
        "def plot_dataset(X_sorted, Y_sorted, points_idxs, save_fig):\n",
        "  n_data_points = len(X_sorted)\n",
        "  angles = np.arange(0,2*np.pi,2*np.pi/n_data_points)\n",
        "  f, axes = plt.subplots(1,2, figsize = (2*5,5))\n",
        "  axes[0].plot(angles, Y_sorted)\n",
        "  axes[0].set_xlabel(\"angle in radians\")\n",
        "  axes[0].axis('equal')\n",
        "  axes[1].scatter(X_sorted[:,0],X_sorted[:,1],c=Y_sorted)\n",
        "  \n",
        "  for i, point in enumerate(points_idxs):\n",
        "    axes[1].scatter(X_sorted[point,0],X_sorted[point,1], c = 'r')\n",
        "    axes[1].annotate(f'x{i+1}', (X_sorted[point,0],X_sorted[point,1]), xytext = (X_sorted[point,0]-.15,X_sorted[point,1]) )\n",
        "  if save_fig:\n",
        "    plt.savefig('input_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "8BsFX08lxkID",
        "outputId": "c96fa98a-075c-467e-f2b6-45403dbdc743"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "_,_ = make_circle_dataset(500, 1, 9, plot = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnr75mA8p-Kf"
      },
      "source": [
        "#Plot heatmap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_formatter(denominator=2, number=np.pi, latex='\\pi'):\n",
        "    def gcd(a, b):\n",
        "        while b:\n",
        "            a, b = b, a%b\n",
        "        return a\n",
        "    def _multiple_formatter(x, pos):\n",
        "        den = denominator\n",
        "        num = np.int(np.rint(den*x/number))\n",
        "        com = gcd(num,den)\n",
        "        (num,den) = (int(num/com),int(den/com))\n",
        "        if den==1:\n",
        "            if num==0:\n",
        "                return r'$0$'\n",
        "            if num==1:\n",
        "                return r'$%s$'%latex\n",
        "            elif num==-1:\n",
        "                return r'$-%s$'%latex\n",
        "            else:\n",
        "                return r'$%s$'%latex\n",
        "        else:\n",
        "            if num==1:\n",
        "                return r'$\\frac{%s}{%s}$'%(latex,den)\n",
        "            elif num==-1:\n",
        "                return r'$\\frac{-%s}{%s}$'%(latex,den)\n",
        "            else:\n",
        "                return r'$\\frac{%s%s}{%s}$'%(num,latex,den)\n",
        "    return _multiple_formatter"
      ],
      "metadata": {
        "id": "LxWavIM9BO_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as tck"
      ],
      "metadata": {
        "id": "NiQNE1lyEa2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBIgP-_iAtdY"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(K, YKYs, state_info, all_input = True, save_fig = True, show_fig = False, mode = 'NA'):\n",
        "  # plt.figure(figsize = (15,500))\n",
        "  # f, axes = plt.subplots(1, len(K),figsize = (15*8,15))\n",
        "  f, axes = plt.subplots(1, len(K),figsize = (10*len(K),10))\n",
        "  if len(K) == 1:\n",
        "    axes = [axes]\n",
        "  model_title = 'unassigned'\n",
        "  if mode != 'NA':\n",
        "    mode_val = f'mode={int(mode):04d}'\n",
        "  for i in range(len(K)):\n",
        "    eigen_vals = eigh(K[i], eigvals_only = True)\n",
        "    K[i] = K[i]/np.sum(eigen_vals)\n",
        "    print(np.sum(eigen_vals))\n",
        "    if all_input:\n",
        "      if K[0].shape[0] <= 100:\n",
        "        if i == len(K)-1:\n",
        "          ax = sns.heatmap(K[i], linewidth = 0, ax = axes[i], vmin = 0, vmax = 20)\n",
        "        else:\n",
        "          ax = sns.heatmap(K[i], linewidth = 0, ax = axes[i])\n",
        "      else:\n",
        "        if i == len(K)-1:\n",
        "          \n",
        "          ax = sns.heatmap(K[i][0:500:5,0:500:5], linewidth = 0, ax = axes[i], vmin = 0, vmax = .004, cbar_kws={\"shrink\": .82})\n",
        "          # ax = sns.heatmap(K[i][0:500:5,0:500:5], linewidth = 0, ax = axes[i])\n",
        "        else:\n",
        "          ax = sns.heatmap(K[i][0:500:5,0:500:5], linewidth = 0, ax = axes[i])\n",
        "    else:\n",
        "      ax =sns.heatmap(K[i][[x1_idx, x2_idx, x3_idx, x4_idx]][:,[x1_idx, x2_idx, x3_idx, x4_idx]], linewidth=.2, ax = axes[i] )\n",
        "    \n",
        "    \n",
        "    pi = np.pi\n",
        "\n",
        "    plt.xticks([0, 50, 100], ['0', '', '2'])\n",
        "    plt.yticks([ 50, 100], [ '', '2'])\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_aspect('equal')\n",
        "    m_p_t = state_info[\"model_protocol_type\"]\n",
        "    n_h_l = f'n_h_l={state_info[\"n_hidden_layers\"]}'\n",
        "    n_n = f'n_n={state_info[\"n_neurons\"]}'\n",
        "    run = f'Run={state_info[\"run\"]}'\n",
        "    epoch = f'Epoch={state_info[\"epoch\"]}'\n",
        "    step = f'step={state_info[\"step\"]}'\n",
        "    s_i = state_info['learning_status']\n",
        "    tr_loss = f'train_loss={state_info[\"train_loss\"]}'\n",
        "    tr_acc = f'train_acc={state_info[\"train_acc\"]}'\n",
        "    te_loss = f'test_loss={state_info[\"test_loss\"]}'\n",
        "    te_acc = f'test_acc={state_info[\"test_acc\"]}'\n",
        "\n",
        "    if mode == 'NA':\n",
        "      model_title = f'{m_p_t}({n_h_l},{n_n},{run},{epoch},{step},{s_i},{tr_loss},{tr_acc},{te_loss},{te_acc})'\n",
        "    else:\n",
        "      \n",
        "      model_title = f'{m_p_t}({mode_val},{n_h_l},{n_n},{run},{epoch},{step},{s_i},{tr_loss},{tr_acc},{te_loss},{te_acc})'\n",
        "\n",
        "\n",
        "\n",
        "    if i < len(K)-1:\n",
        "      # title = 'K'+str(i+1)+ ', ' +\"Y'KY_n = \"+ YKYs[i] \n",
        "      title = 'K'+str(i+1)\n",
        "      # ax.set_xlabel(title, fontsize = 75)\n",
        "      pass\n",
        "    else:\n",
        "      title = 'K'\n",
        "      # title = 'K'+', ' +\"Y'KY_n = \"+  YKYs[i] \n",
        "      # ax.set_xlabel(title, fontsize = 75)\n",
        "\n",
        "    del ax \n",
        "  # plt.suptitle(model_title,x = 0.5, y = 1.05,ha = 'center', fontsize = 90, fontweight = 20)\n",
        "  if save_fig:\n",
        "    plt.savefig(model_title + \".pdf\" ,format = \"pdf\",bbox_inches='tight', dpi = 100)\n",
        "  if show_fig:\n",
        "    plt.show()  \n",
        "  plt.clf()\n",
        "  plt.close(f)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)"
      ],
      "metadata": {
        "id": "_PK37zj7U5rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC85OWcNoA8F"
      },
      "source": [
        "#Helper functions\n",
        "\n",
        "\n",
        "*   generating x-axis for plots\n",
        "*   formatting state_infos\n",
        "*   container routine\n",
        "*   unwrap routine\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEqma-k6gI-v"
      },
      "outputs": [],
      "source": [
        "def get_x_axis_global(state_info_5_runs, x_axis_global):  \n",
        "  x_axis_global = []\n",
        "  for r in range(len(state_info_5_runs)):\n",
        "    for e in range(len(state_info_5_runs[r])):\n",
        "      for s in range(len(state_info_5_runs[r][e])):\n",
        "        x_axis_global.append(round(((int(state_info_5_runs[r][e][s]['epoch'])-1) *(n_batches) + int(state_info_5_runs[r][e][s]['step']))/n_batches, 2))\n",
        "  x_axis_global[0] = 0\n",
        "  return x_axis_global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMS7cYUiHZuN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_state_info(state_info):\n",
        "  state_info[\"run\"] = str(state_info[\"run\"])\n",
        "  state_info[\"epoch\"] = f'{int(state_info[\"epoch\"]):05d}'\n",
        "  state_info[\"step\"] = f'{int(state_info[\"step\"]):03d}'\n",
        "  state_info[\"train_loss\"] = float(\"{:.3f}\".format(state_info[\"train_loss\"]))\n",
        "  state_info[\"train_acc\"] = float(\"{:.2f}\".format(state_info[\"train_acc\"]))\n",
        "  state_info[\"test_loss\"] = float(\"{:.3f}\".format(state_info[\"test_loss\"]))\n",
        "  state_info[\"test_acc\"] = float(\"{:.2f}\".format(state_info[\"test_acc\"]))\n",
        "  state_info[\"learning_status\"] = state_info[\"learning_status\"]\n",
        "  return state_info\n",
        "\n",
        "\n",
        "def routine(hidden_layer_outs_container, state_info_container, predictions_container,hyp_container,  hidden_layer_outputs,state_info, predictions, hyp):\n",
        "  pass\n",
        "  # hidden_layer_outs_container.append(hidden_layer_outputs)\n",
        "  # state_info_container.append(copy.deepcopy(state_info))\n",
        "  # predictions_container.append(predictions)\n",
        "  # hyp_container.append(hyp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWPVgSW73aBO"
      },
      "outputs": [],
      "source": [
        "def unwrap_routine(arr_all):\n",
        "  result = [[] for _ in range(n_hidden_layers)]\n",
        "  for r in range(len(arr_all)):\n",
        "    for e in range(len(arr_all[r])):\n",
        "      for l in range(len(arr_all[r][e])):\n",
        "        result[l].append(arr_all[r][e][l])\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHzf9mTzrLYm"
      },
      "source": [
        "#Unknown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1d8gEYHpVUh"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 8, 5, padding = 'same')\n",
        "#         self.pool = nn.AvgPool2d(1, 1)\n",
        "#         self.conv2 = nn.Conv2d(8, 8, 5, padding = 'same')\n",
        "#         self.conv3 = nn.Conv2d(8, 8, 5, padding = 'same')\n",
        "#         self.fc1 = nn.Linear(8 * 32 * 32, 64)\n",
        "#         # self.fc2 = nn.Linear(128, 32)\n",
        "#         self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "#         # self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         # self.pool = nn.MaxPool2d(2, 2)\n",
        "#         # self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         # self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "#         # self.fc2 = nn.Linear(120, 84)\n",
        "#         # self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x = self.pool(F.relu(self.conv1(x)))\n",
        "#         # x = self.pool(F.relu(self.conv2(x)))\n",
        "#         # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         # x = F.relu(self.fc1(x))\n",
        "#         # x = F.relu(self.fc2(x))\n",
        "#         # x = self.fc3(x)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.conv3(x)\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         # x = self.fc2(x)\n",
        "#         x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# net = Net()\n",
        "# net.to(device)\n",
        "# import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# for epoch in range(30):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 100 == 99:    # print every 2000 mini-batches\n",
        "#           print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "#           running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITAB5txo6F8"
      },
      "source": [
        "#MLP class and train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXqBYjhMpPS_"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,inp_dim, out_dim, n_hidden_layers, n_neurons, protocol_type, bias):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        \n",
        "        self.protocol_type = protocol_type\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.bias = bias\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layers = nn.ModuleList([nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device)])\n",
        "        \n",
        "        for i in range(self.n_hidden_layers-1):\n",
        "          self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias ).to(device))\n",
        "\n",
        "        self.layers.append(nn.Linear( self.n_neurons, out_dim, bias = self.bias).to(device))\n",
        "  def forward(self, x, batch = None):\n",
        "        # x = self.flatten(x)\n",
        "        \n",
        "        # print(self.layers[-2].weight)\n",
        "\n",
        "        hidden_layer_outputs = []\n",
        "        for i in range(len(self.layers)-1):\n",
        "          x = self.layers[i](x)\n",
        "          hidden_layer_outputs.append(x)\n",
        "\n",
        "          out = torch.relu(x)\n",
        "          \n",
        "          x = out\n",
        "        x = self.layers[-1](x)\n",
        "        # x = torch.squeeze(nn.Sigmoid()(x))\n",
        "        return x, hidden_layer_outputs\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP_jnR0eB5Co"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer,  test_dataloader, mini_dl, state_info = None):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    state_info_over_batches = []\n",
        "    hidden_layer_outs_over_batches = []\n",
        "    predictions_over_batches = []\n",
        "    hyp_over_batches, hyp = [], None\n",
        "    for batch, (x,y) in enumerate(dataloader):\n",
        "        X, y = x.to(device), y.to(device)\n",
        "        pred, _ = model(X.float())\n",
        "        \n",
        "        loss = loss_fn(pred.flatten(), y.flatten())\n",
        "        \n",
        "\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        correct += (pred.softmax(1).argmax(1) == y).type(torch.float).sum().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss = loss.item()\n",
        "        # if batch%100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"Train Loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "        if int(state_info['epoch']) <= step_stop_point and (batch+1) % step_stepsize == 0:\n",
        "          \n",
        "         \n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,model, loss_fn)\n",
        "          _, _, train_loss, train_acc = evaluate(dataloader, None, model, loss_fn, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'step' : batch+1})\n",
        "      \n",
        "          state_info = format_state_info(state_info)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_over_batches, state_info_container = state_info_over_batches, predictions_container = predictions_over_batches,hyp_container = hyp_over_batches,\n",
        "            hidden_layer_outputs = hidden_layer_outputs, state_info = state_info, predictions = predictions, hyp = hyp\n",
        "            )\n",
        "          # plot_heatmap(kernels, state_info)\n",
        "    train_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"End of Epoch {state_info['epoch']} \\nTrain : \\n Accuracy = {(100*correct):>0.1f}, Loss =  {train_loss:>7f}\")\n",
        "    return predictions_over_batches, hidden_layer_outs_over_batches, state_info_over_batches\n",
        "\n",
        "def evaluate(test_dataloader,mini_dl, model, loss_fn, is_training = False):\n",
        "    size = len(test_dataloader.dataset)\n",
        "    num_batches = len(test_dataloader)\n",
        "    model.eval()\n",
        "    loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred , out = model(X.float())\n",
        "            \n",
        "            loss += loss_fn(pred.flatten(), y.flatten()).item()\n",
        "            \n",
        "            correct += (pred.softmax(1).argmax(1) == y).type(torch.float).sum().item()\n",
        "            # correct += ((pred >= .5) == y).type(torch.float).sum().item()\n",
        "    loss /= num_batches\n",
        "    correct /= size\n",
        "    if not is_training:\n",
        "      # print(f\"Test : \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")\n",
        "      # with torch.no_grad():\n",
        "      #     for X, y in mini_dl:\n",
        "      #         X, y = X.to(device), y.to(device)\n",
        "      #         pred , out = model(X.float())\n",
        "      with torch.no_grad():\n",
        "          for X in mini_dl:\n",
        "              X = X.to(device)\n",
        "              _ , out = model(X.float())\n",
        "      out = [x.clone().detach().to('cpu').numpy() for x in out]\n",
        "    temp_var = \"Training\" if is_training else \"Test\"\n",
        "    print(f\"Loss over all {temp_var} data : {loss:>5f} \\n\")\n",
        "    return pred, out, loss, correct*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ36s-5ExMP5"
      },
      "outputs": [],
      "source": [
        "# def fibo(n):\n",
        "#   print(n)\n",
        "#   if n == 1 or n==0 :\n",
        "#     return 1\n",
        "  \n",
        "#   return fibo(n-1) + fibo(n-2)\n",
        "\n",
        "# print(fibo(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDp8cIlkd865"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# for n_h_l in [5]:\n",
        " \n",
        "  # for n_n in [32]:\n",
        "def run_mlp_model(train_dataloader, test_dataloader,mini_dl, inp_dim = None, out_dim = None, n_h_l = 5,n_n = 32, n_runs = 5, n_epochs = 100, bias = True, is_mnist_data = None):\n",
        "    model_protocol_type = \"MLP\"\n",
        "    n_hidden_layers = n_h_l\n",
        "    n_neurons = n_n\n",
        "\n",
        "    state_info = {\"model_protocol_type\" : model_protocol_type,\"n_hidden_layers\":n_hidden_layers, \"n_neurons\" : n_neurons}\n",
        "    # kernel_5_runs = []\n",
        "    \n",
        "    # intermediate_outs_5_runs = []\n",
        "    hidden_layer_outs_5_runs = []\n",
        "    state_info_5_runs = []\n",
        "    predictions_5_runs = []\n",
        "    # rand_kernels_over_5 = []\n",
        "    for run in range(n_runs):\n",
        "\n",
        "      \n",
        "     \n",
        "\n",
        "      #model init\n",
        "      mlp_model = NeuralNet(inp_dim, out_dim, n_hidden_layers, n_neurons, model_protocol_type, bias).to(device)\n",
        "      loss_fn = nn.MSELoss() \n",
        "      optimizer = torch.optim.Adam(mlp_model.parameters(),lr = lr)  \n",
        "\n",
        "      #Local variables init\n",
        "      # model_learning_status = \"UnLearned\"\n",
        "      # kernels_run = []\n",
        "      hidden_layer_outs_run = []\n",
        "      state_info_run = []\n",
        "      # intermediate_outs_run = []\n",
        "      predictions_run = []\n",
        "      hyp_run, hyp = [], None\n",
        "      epochs = n_epochs\n",
        "\n",
        "\n",
        "      #Evaluate before training starts\n",
        "      predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "      _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "      state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "      # print(torch.argmax(nn.Softmax(dim = 1)(predictions), dim = 1))\n",
        "      # debug()\n",
        "      state_info.update({'run' : run+1, 'epoch' : 0,'step' : 0, 'learning_status' : 'UnLearned'})\n",
        "      state_info = format_state_info(state_info)\n",
        "      routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "              hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "              )\n",
        "      # print(state_info_run)\n",
        "      # hidden_layer_outs_run.append([hidden_layer_outputs])\n",
        "      # state_info_run.append([state_info.copy()])\n",
        "      # predictions_run.append([predictions])\n",
        "      # print(state_info)\n",
        "      # debug()\n",
        "      #Store init result\n",
        "      # kernels_run.append([kernels])\n",
        "      # state_info_run.append([state_info.copy()])\n",
        "      # intermediate_outs_run.append(np.array((hidden_layer_outputs[1][[x1_idx, x2_idx, x3_idx, x4_idx]]).detach().to('cpu')))\n",
        "     \n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : None, 'learning_status' : 'Learned'})\n",
        "          predictions_batches, hidden_layer_outs_batches, state_info_batches = train(train_dataloader,mlp_model, loss_fn, optimizer,\n",
        "                                                           test_dataloader = test_dataloader,mini_dl = mini_dl, state_info = state_info)\n",
        "          \n",
        "          hyp_over_batches = None\n",
        "          if int(state_info['epoch']) <= step_stop_point: \n",
        "            \n",
        "            # hidden_layer_outs_run.append(hidden_layer_outs_batches)\n",
        "            # state_info_run.append(state_info_batches)\n",
        "            # predictions_run.append(predictions_batches)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "                    hidden_layer_outputs = hidden_layer_outs_batches, state_info = state_info_batches, predictions = predictions_batches, hyp = hyp_over_batches\n",
        "                    )\n",
        "\n",
        "          # elif int(state_info['epoch']) <= epoch_stop_point or (int(state_info['epoch']) % epoch_stepsize) == 0:\n",
        "          elif int(state_info['epoch']) <= 10 :\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          elif int(state_info['epoch']) <= 200 and (int(state_info['epoch']) % 10) == 0:\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          elif int(state_info['epoch']) <= 1000 and (int(state_info['epoch']) % 100) == 0 :\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          elif int(state_info['epoch']) <= 10000 and (int(state_info['epoch']) % 1000) == 0 :\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "            # hidden_layer_outs_run.append([hidden_layer_outputs])\n",
        "            # state_info_run.append([state_info.copy()])\n",
        "            # predictions_run.append([predictions])\n",
        "          \n",
        "          if train_loss <= train_loss_stopping_criteria or epoch == 10000:\n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc = evaluate(test_dataloader,mini_dl,mlp_model, loss_fn)\n",
        "            _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "            break\n",
        "          print(f\"End of Epoch {state_info['epoch']} \\nTest : \\n Accuracy = {(test_acc):>0.1f}, Loss =  {test_loss:>5f}\")\n",
        "          _, _, train_loss, train_acc = evaluate(train_dataloader, None, mlp_model, loss_fn, is_training = True)\n",
        "          # print(state_info_run)\n",
        "            \n",
        "          \n",
        "\n",
        "    # intermediate_outs_5_runs.append(intermediate_outs_run)\n",
        "    # kernel_5_runs.append(kernels_run)\n",
        "      state_info_5_runs.append(state_info_run)\n",
        "      hidden_layer_outs_5_runs.append(hidden_layer_outs_run)\n",
        "      predictions_5_runs.append(predictions_run)\n",
        "\n",
        "    return hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYhzpVHvow-l"
      },
      "source": [
        "#NPF NPV class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvkj9iqqxyDg"
      },
      "outputs": [],
      "source": [
        "class NPFNeuralNetwork(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim, n_hidden_layers, n_neurons, protocol_type, bias):\n",
        "\n",
        "        super(NPFNeuralNetwork, self).__init__()\n",
        "        self.protocol_type = protocol_type\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.bias = bias\n",
        "        \n",
        "        self.layers = nn.ModuleList([])\n",
        "   \n",
        "        #For DLGN-SF, DGN-DLGN-SF\n",
        "        # if self.protocol_type == \"DLGN-SF\":\n",
        "        #   self.layers.append(nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device))\n",
        "        #   for i in range(self.n_hidden_layers-1):\n",
        "        #     self.layers.append(nn.Linear( inp_dim,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "        if self.protocol_type == \"DGN-DLGN-SF\":\n",
        "          self.layers.append(nn.Sequential(nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "                                            )\n",
        "                            )\n",
        "          \n",
        "          for i in range(self.n_hidden_layers-1):\n",
        "            self.layers.append(nn.Sequential(\n",
        "                                  nn.Linear( inp_dim,  self.n_neurons, bias = self.bias).to(device),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "                              )\n",
        "                        )\n",
        "        #For DGN, DLGN\n",
        "        else:\n",
        "          if is_DNN :\n",
        "            self.layers.append(nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device))\n",
        "            for i in range(self.n_hidden_layers-1):\n",
        "              self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "          else:\n",
        "            self.pool = nn.AvgPool2d(32, 32)\n",
        "            \n",
        "            filter_size = n_filters\n",
        "            kernel_size = 5\n",
        "            if self.protocol_type == \"DLGN-SF\":\n",
        "              self.layers.append(nn.Conv2d(3, filter_size, kernel_size, padding = 'same').to(device))\n",
        "              for i in range(n_cnn_layers-1):\n",
        "                self.layers.append(nn.Conv2d(3, filter_size, kernel_size, padding = 'same').to(device))\n",
        "              # self.layers.append(nn.Linear(40, self.n_neurons).to(device))\n",
        "              # for i in range(self.n_hidden_layers-n_cnn_layers-1):\n",
        "              #     self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "            elif self.protocol_type == \"DLGN\":\n",
        "\n",
        "              \n",
        "              self.layers.append(nn.Conv2d(3, filter_size, kernel_size, padding = 'same').to(device))\n",
        "              for i in range(n_cnn_layers-1):\n",
        "                self.layers.append(nn.Conv2d(filter_size, filter_size, kernel_size, padding = 'same').to(device))\n",
        "              self.layers.append(nn.Linear(n_filters, self.n_neurons).to(device))\n",
        "              for i in range(self.n_hidden_layers-n_cnn_layers-1):\n",
        "                  self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "      \n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden_layer_outputs = []\n",
        "        if is_DNN:\n",
        "          if self.protocol_type == \"DGN\":\n",
        "            for i in range(len(self.layers)):\n",
        "              x = self.layers[i](x)\n",
        "              hidden_layer_outputs.append(x)\n",
        "              out = torch.relu(x)\n",
        "              x = out\n",
        "            #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "          elif  self.protocol_type == \"DLGN\":\n",
        "            for i in range(len(self.layers)):\n",
        "              # print(x.size())\n",
        "              # print(self.layers[i].weight.size())\n",
        "              x = self.layers[i](x)\n",
        "              out = x\n",
        "              hidden_layer_outputs.append(out)\n",
        "              x = out\n",
        "            #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "          elif  self.protocol_type == \"DLGN-SF\":\n",
        "            for i in range(len(self.layers)):\n",
        "              out = self.layers[i](x)\n",
        "              hidden_layer_outputs.append(out)\n",
        "          elif  self.protocol_type == \"DGN-DLGN-SF\":\n",
        "            for i in range(len(self.layers)):\n",
        "              out = self.layers[i](x) \n",
        "              hidden_layer_outputs.append(out)\n",
        "        else:\n",
        "            if  self.protocol_type == \"DLGN\":\n",
        "              for i in range(n_cnn_layers):\n",
        "                x = self.layers[i](x)\n",
        "                out = x\n",
        "                hidden_layer_outputs.append(out)\n",
        "                x = out\n",
        "              x = self.pool(x)\n",
        "              x = torch.flatten(x, 1)\n",
        "              \n",
        "              for i in range(self.n_hidden_layers-n_cnn_layers):\n",
        "                \n",
        "                x = self.layers[i+n_cnn_layers](x)\n",
        "                out = x\n",
        "                hidden_layer_outputs.append(out)\n",
        "                x = out\n",
        "\n",
        "            elif  self.protocol_type == \"DLGN-SF\":\n",
        "              for i in range(n_cnn_layers):\n",
        "                out = self.layers[i](x)\n",
        "                hidden_layer_outputs.append(out)\n",
        "              # x = self.layers[0](x)\n",
        "              \n",
        "              # hidden_layer_outputs.append(x)\n",
        "\n",
        "\n",
        "              # x = torch.flatten(x, 1)\n",
        "\n",
        "              # x = self.layers[1](x)\n",
        "              # hidden_layer_outputs.append(x)\n",
        "\n",
        "              # x = self.layers[2](x)\n",
        "              # hidden_layer_outputs.append(x)\n",
        "              # # x = self.pool(self.layers[2](x))\n",
        "              # # hidden_layer_outputs.append(x)\n",
        "              # # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "              \n",
        "              # x = self.layers[3](x)\n",
        "             \n",
        "              # hidden_layer_outputs.append(x)\n",
        "            \n",
        "\n",
        "        return hidden_layer_outputs\n",
        "\n",
        "\n",
        "\n",
        "class NPVNeuralNetwork(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim, n_hidden_layers, n_neurons, bias):\n",
        "        super(NPVNeuralNetwork, self).__init__()\n",
        "  \n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.bias = bias\n",
        "        if is_DNN:\n",
        "          self.layers = nn.ModuleList([nn.Linear(inp_dim,  self.n_neurons, bias = self.bias).to(device)])\n",
        "\n",
        "          for i in range(self.n_hidden_layers-1):\n",
        "            self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "        else:\n",
        "\n",
        "          self.pool = nn.AvgPool2d(32, 32)\n",
        "          filter_size = n_filters\n",
        "          kernel_size = 5\n",
        "          self.layers = nn.ModuleList([nn.Conv2d(3, filter_size, kernel_size, padding = 'same').to(device)])\n",
        "\n",
        "          for i in range(n_cnn_layers-1):\n",
        "            self.layers.append(nn.Conv2d(filter_size, filter_size, kernel_size, padding = 'same').to(device))\n",
        "          self.layers.append(nn.Linear(n_filters, self.n_neurons).to(device))\n",
        "          for i in range(self.n_hidden_layers-n_cnn_layers-1):\n",
        "              self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "\n",
        "          self.layers.append(nn.Linear(self.n_neurons, out_dim).to(device))\n",
        "        \n",
        "        if is_DNN:\n",
        "          self.layers.append(nn.Linear( self.n_neurons, out_dim, bias = self.bias).to(device))\n",
        "\n",
        "        self.gate = Gate(beta = 4)\n",
        "      \n",
        "    def forward(self, x,act_type, gating_mask):\n",
        "        hidden_layer_outputs = []\n",
        "        if is_DNN:\n",
        "          for i in range(len(self.layers)-1):\n",
        "            x = self.layers[i](x)\n",
        "            \n",
        "            out = self.gate(act_type, x, i, gating_mask)\n",
        "            \n",
        "            hidden_layer_outputs.append(out)\n",
        "            x = out\n",
        "          x = self.layers[len(self.layers)-1](x)\n",
        "          #print(\"debug 2\", x.shape, x)\n",
        "        else:\n",
        "          for i in range(n_cnn_layers):\n",
        "            x = self.layers[i](x)\n",
        "            x = self.gate(act_type, x, i, gating_mask)\n",
        "            hidden_layer_outputs.append(x)\n",
        "\n",
        "          x = self.pool(x)\n",
        "          x = torch.flatten(x, 1)\n",
        "          \n",
        "          for i in range(self.n_hidden_layers-n_cnn_layers):\n",
        "            \n",
        "              x = self.layers[i+n_cnn_layers](x)\n",
        "              \n",
        "              x = self.gate(act_type, x, i+n_cnn_layers, gating_mask)\n",
        "              hidden_layer_outputs.append(x)\n",
        "            \n",
        "          x = self.layers[-1](x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # x = self.layers[0](x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.gate(act_type, x, 0, gating_mask)\n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # x = torch.flatten(x, 1)\n",
        "          # x = self.layers[1](x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.gate(act_type, x, 1, gating_mask)\n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # x = self.layers[2](x)\n",
        "          \n",
        "          # x = self.gate(act_type, x, 2, gating_mask)\n",
        "          \n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.layers[3](x)\n",
        "          # # x = self.pool(x)\n",
        "          # x = self.gate(act_type, x, 3, gating_mask)\n",
        "          # hidden_layer_outputs.append(x)\n",
        "          # # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "          # # x = self.layers[3](x)\n",
        "          # # x = self.gate(act_type, x,3, gating_mask)\n",
        "          # # hidden_layer_outputs.append(x)\n",
        "          \n",
        "          \n",
        "         \n",
        "          # x = self.layers[4](x)\n",
        "        return x,  hidden_layer_outputs\n",
        "\n",
        "\n",
        "def apply_gate(beta, idx, gating_mask):\n",
        "  out = beta*(gating_mask[idx])\n",
        "  \n",
        "  return out\n",
        "  \n",
        "class Gate(nn.Module):\n",
        "    def __init__(self, beta):\n",
        "        super(Gate,self).__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self,act_type, x, idx, gating_mask):\n",
        "      #Soft Relu\n",
        "      if act_type == 'soft':\n",
        "        return torch.mul(x,torch.sigmoid(apply_gate(self.beta, idx,gating_mask)))\n",
        "      elif act_type == 'hard':\n",
        "      #Hard Relu\n",
        "        temp = torch.sign(gating_mask[idx])\n",
        "        temp[temp <= 0] = 0\n",
        "        return torch.mul(x,temp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNXo7wMRooPI"
      },
      "source": [
        "#NPF, NPV Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er2qEs6hbLH3"
      },
      "outputs": [],
      "source": [
        "def get_metrics_hlo(train_dl_npf, train_dl_npv, test_dl_npf, test_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type , is_training):\n",
        "  #hidden_layer_outputs is computed for mini_dataloader(for each input)\n",
        "  _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "  predictions, hidden_layer_outputs, test_loss, test_acc = evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type)\n",
        "  return train_acc, train_loss, test_acc, test_loss, predictions, hidden_layer_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vfo8ATFok4o"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_decoupled(X1_dataloader,X2_dataloader, npf_model, npv_model, loss_fn, optimizer,act_type, test_dl_npf, test_dl_npv,mini_dl, state_info = None):\n",
        "    size = len(X1_dataloader.dataset)\n",
        "    num_batches = len(X1_dataloader)\n",
        "    correct = 0\n",
        "    train_loss = 0\n",
        "    state_info_over_batches = []\n",
        "    hidden_layer_outs_over_batches = []\n",
        "    predictions_over_batches = []\n",
        "    hyp_over_batches = []\n",
        "\n",
        "    \n",
        "    for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "        X1, y1 = X1.to(device), y1.to(device)\n",
        "        X2, y2 = X2.to(device), y2.to(device)\n",
        "        \n",
        "        npf_model_hidden_layer_outs = npf_model(X1)\n",
        "        pred, npv_model_hidden_layer_outs = npv_model(X2,act_type, npf_model_hidden_layer_outs)\n",
        "        #print(\"Debug1 \", pred.shape)\n",
        "        # print(pred.dtype,type(y1.float), y1.float.dtype)\n",
        "        if not is_classification:\n",
        "          y1 = torch.unsqueeze(y1, 1)\n",
        "        loss = loss_fn(pred, y1)\n",
        "        train_loss += loss.item()\n",
        "        correct += (pred.softmax(1).argmax(1) == y1).type(torch.float).sum().item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if batch%10 == 0:\n",
        "        #     #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        #     loss, current = loss.item(), batch * len(X1)\n",
        "        #     print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "        \n",
        "        if int(state_info['epoch']) <= step_stop_point and (batch+1) % step_stepsize == 0:\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc = evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(X1_dataloader,X2_dataloader,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'step' : batch+1})\n",
        "        \n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = None, model_protocol_type = state_info['model_protocol_type'],run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_over_batches, state_info_container = state_info_over_batches, predictions_container = predictions_over_batches,hyp_container = hyp_over_batches,\n",
        "            hidden_layer_outputs = hidden_layer_outputs, state_info = state_info, predictions = predictions, hyp = hyp\n",
        "            )\n",
        "          # if int(state_info['run']) < 2:\n",
        "            # plot_hyperplanes(npf_model, for_layers = 5 ,model_protocol_type = state_info['model_protocol_type'] ,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'])\n",
        "    train_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"End of Epoch {state_info['epoch']}Train : \\n Accuracy = {(100*correct):>0.1f}, Loss =  {train_loss:>7f}\")\n",
        "    return predictions_over_batches, hidden_layer_outs_over_batches, state_info_over_batches, hyp_over_batches\n",
        "\n",
        "def evaluate_decoupled(X1_dataloader, X2_dataloader,mini_dl, npf_model, npv_model, loss_fn,act_type, is_training = False):\n",
        "    size = len(X1_dataloader.dataset)\n",
        "    num_batches = len(X1_dataloader)\n",
        "    npf_model.eval()\n",
        "    npv_model.eval()\n",
        "    loss, correct = 0, 0\n",
        "    X_ones = torch.ones(500,2).to(device)\n",
        "    with torch.no_grad():\n",
        "        for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "            X1, y1 = X1.to(device), y1.to(device)\n",
        "            X2, y2 = X2.to(device), y2.to(device)\n",
        "            npf_model_hidden_layer_outs = npf_model(X1)\n",
        "            pred, npv_model_hidden_layer_outs = npv_model(X2,act_type, npf_model_hidden_layer_outs)\n",
        "            if not is_classification:\n",
        "              y1 = torch.unsqueeze(y1, 1)\n",
        "            loss += loss_fn(pred, y1).item()\n",
        "            \n",
        "            correct += (pred.softmax(1).argmax(1) == y1).type(torch.float).sum().item()\n",
        "    loss /= num_batches\n",
        "    correct /= size\n",
        "    if not is_training:\n",
        "    # print(f\"Test : \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")\n",
        "    # with torch.no_grad():\n",
        "    #     for X, y in mini_dl:\n",
        "    #         X, y = X.to(device), y.to(device)\n",
        "    #         pred , out = model(X.float())\n",
        "      with torch.no_grad():\n",
        "          for X in mini_dl:\n",
        "              X = X.to(device)\n",
        "              npf_model_hidden_layer_outs = npf_model(X)\n",
        "              _, npv_model_hidden_layer_outs = npv_model(X,act_type, npf_model_hidden_layer_outs)\n",
        "        \n",
        "    npf_model_hidden_layer_outs = [x.clone().detach().to('cpu').numpy() for x in npf_model_hidden_layer_outs]\n",
        "    temp_var = \"Training\" if is_training else \"Test\"\n",
        "    print(f\"Loss over all {temp_var} data : {loss:>5f} \\n\")\n",
        "    return pred, npf_model_hidden_layer_outs, loss, correct*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Az41tOliuFWj",
        "outputId": "f29ade2a-7e7f-4e6c-e74d-a7da290b1e12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nParameter containing:\\ntensor([[-1., -2.],\\n        [ 1., -1.]], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([[2., 2.],\\n        [1., 0.]], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([[ 2.,  0.],\\n        [ 2., -2.]], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([[-2., -1.],\\n        [ 0., -2.]], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([[-2., -2.]], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([ 2., -1.], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([-1.,  2.], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([1., 0.], device='cuda:0', requires_grad=True) Parameter containing:\\ntensor([0., 0.], device='cuda:0', requires_grad=True)\\nParameter containing:\\ntensor([2.], device='cuda:0', requires_grad=True)\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# npf_model = NPFNeuralNetwork(inp_dim = 2, out_dim = 1, n_hidden_layers = 3, n_neurons = 2, protocol_type = 'DLGN', bias = True).to(device)\n",
        "# npv_model = NPVNeuralNetwork(inp_dim = 2, out_dim = 1,n_hidden_layers = 3, n_neurons = 2, bias = True).to(device)\n",
        "# print(npf_model.layers[0].weight,npf_model.layers[0].bias,  npv_model.layers[0].weight, npv_model.layers[0].bias)\n",
        "# print(npf_model.layers[1].weight,npf_model.layers[1].bias,  npv_model.layers[1].weight,npv_model.layers[1].bias )\n",
        "# print( npv_model.layers[2].weight, npv_model.layers[2].bias)\n",
        "# set_seed(seed = 2022)\n",
        "# npf_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npf_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npf_model.layers[0].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "# npf_model.layers[1].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "\n",
        "# set_seed(seed = 2021)\n",
        "# npv_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npv_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "# npv_model.layers[2].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (1,2)).float().to(device))\n",
        "# npv_model.layers[0].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "# npv_model.layers[1].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "# npv_model.layers[2].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (1,)).float().to(device))\n",
        "\n",
        "# print(npf_model.layers[0].weight, npv_model.layers[0].weight)\n",
        "# print(npf_model.layers[1].weight, npv_model.layers[1].weight)\n",
        "# print( npv_model.layers[2].weight)\n",
        "\n",
        "# print(npf_model.layers[0].bias, npv_model.layers[0].bias)\n",
        "# print(npf_model.layers[1].bias, npv_model.layers[1].bias)\n",
        "# print( npv_model.layers[2].bias)\n",
        "'''\n",
        "Parameter containing:\n",
        "tensor([[-1., -2.],\n",
        "        [ 1., -1.]], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([[2., 2.],\n",
        "        [1., 0.]], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([[ 2.,  0.],\n",
        "        [ 2., -2.]], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([[-2., -1.],\n",
        "        [ 0., -2.]], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([[-2., -2.]], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([ 2., -1.], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([-1.,  2.], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([1., 0.], device='cuda:0', requires_grad=True) Parameter containing:\n",
        "tensor([0., 0.], device='cuda:0', requires_grad=True)\n",
        "Parameter containing:\n",
        "tensor([2.], device='cuda:0', requires_grad=True)\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aiIWr_FyIZg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def run_npf_npv_model(train_dl_npf ,train_dl_npv , test_dl_npf, test_dl_npv,mini_dl, protocol = 'DGN',learning_type = 'BOTH',act_type = 'soft',inp_dim = None, out_dim = None, n_h_l = 5,n_n = 32, n_runs = 5, n_epochs = 250,lr = 5e-5, bias = True, epsilon = 1):\n",
        "\n",
        "    model_protocol_type = protocol\n",
        "    n_hidden_layers = n_h_l\n",
        "    n_neurons = n_n\n",
        "\n",
        "    state_info = {\"model_protocol_type\" : model_protocol_type,\"n_hidden_layers\":n_hidden_layers, \"n_neurons\" : n_neurons}\n",
        "    hidden_layer_outs_5_runs = []\n",
        "    state_info_5_runs = []\n",
        "    predictions_5_runs = []\n",
        "    hyp_5_runs = []\n",
        "    act_type = act_type\n",
        "    for run in range(n_runs):\n",
        "      set_seed(run + 10)\n",
        "      #Model Init\n",
        "      npf_model = NPFNeuralNetwork(inp_dim, out_dim, n_hidden_layers, n_neurons, model_protocol_type, bias).to(device)\n",
        "      npv_model = NPVNeuralNetwork(inp_dim, out_dim,n_hidden_layers, n_neurons, bias).to(device)\n",
        "      # torch.save(npv_model.state_dict(), \"npv_init_weights\")\n",
        "      #tmep code for ICLR toy example\n",
        "      # set_seed(seed = 2022)\n",
        "      # npf_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npf_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npf_model.layers[0].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "      # npf_model.layers[1].bias = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,)).float().to(device))\n",
        "      # set_seed(seed = 2021)\n",
        "      # npv_model.layers[0].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npv_model.layers[1].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (2,2)).float().to(device))\n",
        "      # npv_model.layers[2].weight = nn.Parameter(torch.randint(low = -2, high = 3, size = (1,2)).float().to(device))\n",
        "      # npv_model.layers[0].bias = nn.Parameter(torch.randint(low = 0, high = 1, size = (2,)).float().to(device))\n",
        "      # npv_model.layers[1].bias = nn.Parameter(torch.randint(low = 0, high = 1, size = (2,)).float().to(device))\n",
        "      # npv_model.layers[2].bias = nn.Parameter(torch.randint(low = 0, high = 1, size = (1,)).float().to(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      loss_fn = nn.CrossEntropyLoss() if is_classification else nn.MSELoss() \n",
        "      print(learning_type)\n",
        "      \n",
        "      if learning_type == 'BOTH' or learning_type == 'BOTH_ONPV':\n",
        "      \n",
        "        optimizer = torch.optim.Adam([\n",
        "                        {'params': npf_model.parameters()},\n",
        "                        {'params': npv_model.parameters()}],\n",
        "                        lr = lr) \n",
        "      elif learning_type == 'ONPF':\n",
        "        optimizer = torch.optim.Adam([\n",
        "                        {'params': npf_model.parameters()}],\n",
        "                        lr = lr)\n",
        "      elif learning_type == 'ONPV':\n",
        "        optimizer = torch.optim.Adam([\n",
        "                        {'params': npv_model.parameters()}],\n",
        "                        lr = lr)\n",
        "      # optimizer = torch.optim.Adam([\n",
        "      #                 {'params': npf_model.parameters()},\n",
        "      #                               ],\n",
        "      #                 lr = 3e-6)\n",
        "      #Make the non linear layer of NPF model untrainable\n",
        "      #NOTE: Unchecked for different hidden layers(Currently working for n_h_l = 5)\n",
        "      if model_protocol_type == 'DGN-DLGN-SF':\n",
        "        for idx, param in enumerate(npf_model.parameters()):\n",
        "          if idx % 4 == 0 or idx-1 % 4 == 0:\n",
        "            param.requires_grad = False\n",
        "      if learning_type == 'BOTH_ONPV':\n",
        "        act_type = 'soft'\n",
        "      #Local Variable Init\n",
        "      hidden_layer_outs_run = []\n",
        "      state_info_run = []\n",
        "      predictions_run = []\n",
        "      hyp_run = []\n",
        "      epochs = n_epochs\n",
        "\n",
        "      #Evaluate before training starts\n",
        "      # all_per_sample_gradients = compute_pred_gradient(dl_one = train_dl_npf,model = [npf_model, npv_model],act_type = act_type, optimizer = optimizer)\n",
        "      predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl ,npf_model, npv_model, loss_fn, act_type)\n",
        "      _, _, train_loss, train_acc = evaluate_decoupled(train_dl_npf, train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "      state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "      state_info.update({'run' : run+1, 'epoch' : 0,'step' : 0,'learning_status' : 'UnLearned'})\n",
        "      state_info = format_state_info(state_info)\n",
        "      hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "\n",
        "      routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "              hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "              )\n",
        "      \n",
        "      if plot_hyperplanes_figures == True and run < 1:\n",
        "          plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          \n",
        "          # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "      for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : None,'learning_status' : 'Learned'})\n",
        "        predictions_batches, hidden_layer_outs_batches, state_info_batches, hyp_over_batches = train_decoupled(train_dl_npf,\n",
        "                                                                                             train_dl_npv,\n",
        "                                                                                             npf_model,\n",
        "                                                                                             npv_model,\n",
        "                                                                                             loss_fn,\n",
        "                                                                                             optimizer,\n",
        "                                                                                             act_type,\n",
        "                                                                                             test_dl_npf = test_dl_npf,\n",
        "                                                                                             test_dl_npv = test_dl_npv,\n",
        "                                                                                             mini_dl = mini_dl,\n",
        "                                                                                             state_info = state_info)\n",
        "        \n",
        "        if int(state_info['epoch']) <= step_stop_point: \n",
        "            # hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container = hyp_run,\n",
        "                    hidden_layer_outputs = hidden_layer_outs_batches, state_info = state_info_batches, predictions = predictions_batches, hyp = hyp_over_batches\n",
        "                    )\n",
        "        \n",
        "        elif int(state_info['epoch']) <= 10 :\n",
        "            \n",
        "            predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "            _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "            state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "            state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "            state_info = format_state_info(state_info)\n",
        "            hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "            \n",
        "        elif int(state_info['epoch']) <= 200 and (int(state_info['epoch']) % 10) == 0:\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          # if plot_hyperplanes_figures == True and run < 10:\n",
        "          #   plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "           \n",
        "          \n",
        "        elif int(state_info['epoch']) <= 1000 and (int(state_info['epoch']) % 100) == 0 :\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "      \n",
        "          \n",
        "        elif int(state_info['epoch']) <= 20000 and (int(state_info['epoch']) % 1000) == 0 :\n",
        "          \n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "\n",
        "        elif int(state_info['epoch']) <= 5000 and int(state_info['epoch']) >4000 and (int(state_info['epoch']) % 100) == 0 :\n",
        "          \n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            # plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 21)\n",
        "          \n",
        "        if learning_type == 'BOTH_ONPV' and int(state_info['epoch']) == step_stop_point:\n",
        "            for idx, param in enumerate(npf_model.parameters()):\n",
        "                param.requires_grad = False\n",
        "            act_type = 'soft'\n",
        "        print(f\"End of Epoch {state_info['epoch']}Test : \\n Accuracy = {(test_acc):>0.1f}, Loss =  {test_loss:>5f}\")\n",
        "        _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "        # if epoch == 20000:\n",
        "        \n",
        "        #   break\n",
        "        if train_loss <= train_loss_stopping_criteria:\n",
        "          predictions, hidden_layer_outputs, test_loss, test_acc =  evaluate_decoupled(test_dl_npf, test_dl_npv,mini_dl,npf_model, npv_model, loss_fn, act_type)\n",
        "          _, _, train_loss, train_acc =evaluate_decoupled(train_dl_npf ,train_dl_npv,mini_dl, npf_model, npv_model, loss_fn, act_type, is_training = True)\n",
        "          state_info.update({'train_loss' : train_loss, 'train_acc' : train_acc, 'test_loss' : test_loss, 'test_acc' : test_acc})\n",
        "          state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : n_batches, 'learning_status' : 'Learned'})\n",
        "          state_info = format_state_info(state_info)\n",
        "          hyp = plot_posneg_hyperplanes(npf_model, for_layers = n_hidden_layers ,epsilon = epsilon, model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "          routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,hyp_container=hyp_run,\n",
        "                    hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions], hyp = [hyp]\n",
        "                    )\n",
        "          if plot_hyperplanes_figures == True and run < 1:\n",
        "            plot_hyperplanes(npf_model, for_layers = n_hidden_layers ,model_protocol_type = model_protocol_type,run = state_info['run'],  epoch = state_info['epoch'], step = state_info['step'], for_mode = 1)\n",
        "            \n",
        "          break\n",
        "\n",
        "      state_info_5_runs.append(state_info_run)\n",
        "      hidden_layer_outs_5_runs.append(hidden_layer_outs_run)\n",
        "      predictions_5_runs.append(predictions_run)\n",
        "      hyp_5_runs.append(hyp_run)\n",
        "    return hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs, hyp_5_runs\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsxDmHwDl8Sb"
      },
      "source": [
        "#Kernel Plotting Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJVjibIx240t"
      },
      "outputs": [],
      "source": [
        "def get_start_point(epoch_stop_point, epoch_stepsize):\n",
        "  for i in range(epoch_stop_point+1, 500):\n",
        "    if i % epoch_stepsize == 0:\n",
        "      return i\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMqXlCDtRVt_"
      },
      "outputs": [],
      "source": [
        "# from pyparsing.util import col\n",
        "#-------------------------------------------------------Plotting Functions--------------------------------------------------------\n",
        "\n",
        "#For step wise analysis\n",
        "def plot_kernel_values_stepwise(kernel_name, kernel_5_runs,state_info_5_runs, x_idxs_pair):\n",
        "  kernel_map = {'K1':0, 'K2':1, 'K3':2,'K4':3, 'K5':4, 'K': 5}\n",
        "  # labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "  kernel_idx = kernel_map[kernel_name]\n",
        "\n",
        "  f, axes = plt.subplots(1,len(kernel_5_runs),figsize = (7*len(kernel_5_runs), 7))\n",
        "  if len(kernel_5_runs) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  plt.suptitle(f'{kernel_name}_step',ha = 'center', fontsize = 15)\n",
        "\n",
        "  for run in range(len(kernel_5_runs)):\n",
        "    for idx, (row, col) in enumerate(x_idxs_pair):\n",
        "      values = []\n",
        "      \n",
        "      for epoch in range(len(kernel_5_runs[run])):\n",
        "        for step in range(len(kernel_5_runs[run][epoch])):\n",
        "          values.append(kernel_5_runs[run][epoch][step][kernel_idx][row,col])\n",
        "          \n",
        "        if epoch == step_stop_point:\n",
        "          break\n",
        "      if idx >= len(x_idxs_pair)/2:\n",
        "        #high freq\n",
        "        linestyle = 'solid'\n",
        "        color = 'red'\n",
        "      else:\n",
        "        #low freq\n",
        "        linestyle = 'dashed'\n",
        "        color = 'blue'\n",
        "      x = [0] + list(np.arange(step_stepsize, len(values)*step_stepsize, step_stepsize))\n",
        "      axes[run].plot(x,values, linestyle = linestyle, color = color)\n",
        "      axes[run].set_xlabel(f'Run {run+1}', fontsize = 12)\n",
        "      if kernel_name == 'K':\n",
        "        axes[run].set_ylabel(f'Log Scale', fontsize = 12)\n",
        "    line1 = Line2D([0,1],[0,1],linestyle='solid', color='r')\n",
        "    line2 = Line2D([0,1],[0,1],linestyle='dashed', color='b')\n",
        "    axes[run].legend([line1, line2],['high freq', 'low freq'], loc = 'best')\n",
        "    \n",
        "  plt.savefig(f'{kernel_name}_step' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "  plt.show()\n",
        "#For epoch wise analysis\n",
        "def plot_kernel_values_epochwise(kernel_name, kernel_5_runs, state_info_5_runs, x_idxs_pair):\n",
        "  kernel_map = {'K1':0, 'K2':1, 'K3':2,'K4':3, 'K5':4, 'K': 5}\n",
        "  kernel_idx = kernel_map[kernel_name]\n",
        "  # labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "  \n",
        "\n",
        "  f, axes = plt.subplots(1,len(kernel_5_runs),figsize = (7*len(kernel_5_runs), 7))\n",
        "  if len(kernel_5_runs) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  plt.suptitle(f'{kernel_name}_epoch',ha = 'center', fontsize = 15)\n",
        "  \n",
        "  for run in range(len(kernel_5_runs)):\n",
        "    for idx, (row, col) in enumerate(x_idxs_pair):\n",
        "      values = []\n",
        "      state = []\n",
        "      # print(len(kernel_5_runs[run]))\n",
        "      for epoch in range(len(kernel_5_runs[run])):\n",
        "          # print(idx, row, col)\n",
        "          # print(kernel_5_runs[run][epoch])\n",
        "          values.append(kernel_5_runs[run][epoch][-1][kernel_idx][row,col])\n",
        "          state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "      if idx >= len(x_idxs_pair)/2:\n",
        "        #high freq region\n",
        "        linestyle = 'solid'\n",
        "        color = 'red'\n",
        "      else:\n",
        "        #low freq region\n",
        "        linestyle = 'dashed'\n",
        "        color = 'blue'\n",
        "      # x = [0] + list(np.arange(epoch_stepsize, len(values)*epoch_stepsize, epoch_stepsize))\n",
        "      axes[run].plot(state,values, linestyle = linestyle, color = color)\n",
        "      axes[run].set_xlabel(f'Run {run+1}', fontsize = 12)\n",
        "      if kernel_name == 'K':\n",
        "        axes[run].set_ylabel(f'Log Scale', fontsize = 12)\n",
        "    line1 = Line2D([0,1],[0,1],linestyle='solid', color='r')\n",
        "    line2 = Line2D([0,1],[0,1],linestyle='dashed', color='b')\n",
        "    axes[run].legend([line1, line2],['high freq', 'low freq'], loc = 'best')\n",
        "    \n",
        "  plt.savefig(f'{kernel_name}_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "  plt.show()\n",
        "\n",
        "  # Plot Activation graph stepwise\n",
        "def plot_stepwise_activations(hidden_layer_no, hidden_layer_outs_5_runs):\n",
        "  temp = []\n",
        "  y_label = ''\n",
        "  NUM_COLORS = n_neurons\n",
        "  LINE_STYLES = ['solid', 'dashed', 'dashdot', 'dotted']\n",
        "  NUM_STYLES = len(LINE_STYLES)\n",
        "  cm = plt.get_cmap('gist_rainbow')\n",
        "\n",
        "  hidden_layer_no = hidden_layer_no\n",
        "  for run in range(len(hidden_layer_outs_5_runs)):\n",
        "    f, axes = plt.subplots(1, 4,figsize = (7*5,7))\n",
        "    values = []\n",
        "    for epoch in range(len(hidden_layer_outs_5_runs[run])):\n",
        "      for step in range(len(hidden_layer_outs_5_runs[run][epoch])):\n",
        "        sub_h_l_outs = hidden_layer_outs_5_runs[run][epoch][step][hidden_layer_no-1][[x1_idx, x2_idx, x3_idx, x4_idx]]#[:,[x1_idx, x2_idx, x3_idx, x4_idx]]\n",
        "        values.append(sub_h_l_outs)\n",
        "      if epoch == step_stop_point:\n",
        "        break\n",
        "\n",
        "    values = np.array(values)\n",
        "    # x = [0] + list(np.arange(step_stepsize, len(values[0])*step_stepsize, step_stepsize))\n",
        "    plt.suptitle(f'Run {run+1} for Hidden Layer No. {hidden_layer_no}',ha = 'center', fontsize = 15)\n",
        "    for i in range(4):\n",
        "      lines = axes[i].plot(values[:][:,i])\n",
        "      axes[i].set_title(f'x_{i+1}')\n",
        "      axes[i].set_xlabel(f'Step', fontsize = 12)\n",
        "      axes[i].set_ylabel(y_label, fontsize = 12)\n",
        "      # axes[i].set_xticks(np.arange(0,len(values), 1.0))\n",
        "\n",
        "      for j in range(NUM_COLORS):\n",
        "\n",
        "        lines[j].set_color(cm(j//NUM_STYLES*float(NUM_STYLES)/NUM_COLORS))\n",
        "        lines[j].set_linestyle(LINE_STYLES[j%NUM_STYLES])\n",
        "\n",
        "      # axes[i].set_color(cm(i//NUM_STYLES*float(NUM_STYLES)/NUM_COLORS))\n",
        "      # axes[i].set_linestyle(LINE_STYLES[i%NUM_STYLES])\n",
        "    plt.savefig(f'Run_{run+1}_step.png',format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "\n",
        "# Plot Activation graph epochwise\n",
        "def plot_epochwise_activations(hidden_layer_no, hidden_layer_outs_5_runs):\n",
        "  temp = []\n",
        "  y_label = ''\n",
        "\n",
        "  NUM_COLORS = n_neurons\n",
        "  LINE_STYLES = ['solid', 'dashed', 'dashdot', 'dotted']\n",
        "  NUM_STYLES = len(LINE_STYLES)\n",
        "  cm = plt.get_cmap('gist_rainbow')\n",
        "\n",
        "  hidden_layer_no = hidden_layer_no\n",
        "  for run in range(len(hidden_layer_outs_5_runs)):\n",
        "    f, axes = plt.subplots(1, 4,figsize = (7*5,7))\n",
        "    values = []\n",
        "    for epoch in range(len(hidden_layer_outs_5_runs[run])):\n",
        "        sub_h_l_outs = hidden_layer_outs_5_runs[run][epoch][-1][hidden_layer_no-1][[x1_idx, x2_idx, x3_idx, x4_idx]]\n",
        "        values.append(sub_h_l_outs)\n",
        "      \n",
        "    values = np.array(values)\n",
        "    plt.suptitle(f'Run {run+1} for Hidden Layer No. {hidden_layer_no}',ha = 'center', fontsize = 15)\n",
        "    # x = [0] +list(np.arange(1, epoch_stop_point+1 , 1)) +list(np.arange(start_point, n_epochs+1, epoch_stepsize))\n",
        "    for i in range(4):\n",
        "      lines = axes[i].plot(values[:][:,i])\n",
        "      axes[i].set_title(f'x_{i+1}')\n",
        "      \n",
        "      axes[i].set_xlabel(f'Epoch', fontsize = 12)\n",
        "      axes[i].set_ylabel(y_label, fontsize = 12)\n",
        "      axes[i].set_xticks(np.arange(0,len(values), 10.0))\n",
        "\n",
        "      for j in range(NUM_COLORS):\n",
        "        lines[j].set_color(cm(j//NUM_STYLES*float(NUM_STYLES)/NUM_COLORS))\n",
        "        lines[j].set_linestyle(LINE_STYLES[j%NUM_STYLES])\n",
        "    plt.savefig(f'Run_{run+1}_epoch.png',format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# #For step wise loss analysis\n",
        "# def plot_stepwise_loss(state_info_5_runs):\n",
        "#   f, axes = plt.subplots(1, 5,figsize = (7*5,7))\n",
        "#   for run in range(len(state_info_5_runs)):\n",
        "#     values = []\n",
        "    \n",
        "#     for epoch in range(len(state_info_5_runs[run])):\n",
        "#       for step in range(len(state_info_5_runs[run][epoch])):\n",
        "#         train_loss = state_info_5_runs[run][epoch][step]['train_loss']\n",
        "#         test_loss = state_info_5_runs[run][epoch][step]['test_loss']\n",
        "\n",
        "#         values.append([train_loss, test_loss])\n",
        "        \n",
        "#       if epoch == step_stop_point:\n",
        "#         break\n",
        "#     values = np.array(values)\n",
        "#     x = [0] + list(np.arange(step_stepsize, len(values)*step_stepsize, step_stepsize))\n",
        "#     plt.suptitle('Loss v/s step',ha = 'center', fontsize = 15)\n",
        "#     # for i in range(len(values)):\n",
        "#     axes[run].plot(x,values[:,0], label = 'train loss')\n",
        "#     axes[run].plot(x,values[:,1], label = 'test loss')\n",
        "#     axes[run].legend(loc=\"best\")\n",
        "#     axes[run].set_title(f'Run {run+1}')\n",
        "#     axes[run].set_xlabel(f'Step', fontsize = 12)\n",
        "#     axes[run].set_ylabel(f'Loss', fontsize = 12)\n",
        "#       # axes[run].set_xticks(np.arange(0,len(values), 2.0))\n",
        "      \n",
        "\n",
        "#   plt.savefig('loss_step' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#   plt.show()\n",
        "\n",
        "# # For epoch wise loss analysis\n",
        "# def plot_epochwise_loss(state_info_5_runs):\n",
        "#   f, axes = plt.subplots(1, 5,figsize = (7*5,7))\n",
        "#   for run in range(len(state_info_5_runs)):\n",
        "#     values = []\n",
        "#     state = []\n",
        "#     for epoch in range(len(state_info_5_runs[run])):\n",
        "#       train_loss = state_info_5_runs[run][epoch][-1]['train_loss']\n",
        "#       test_loss = state_info_5_runs[run][epoch][-1]['test_loss']\n",
        "#       values.append([train_loss, test_loss])\n",
        "#       state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "      \n",
        "#     values = np.array(values)\n",
        "#     # x = [0] +list(np.arange(1, epoch_stop_point+1 , 1)) +list(np.arange(start_point, n_epochs+1, epoch_stepsize))\n",
        "#     plt.suptitle('Loss v/s epoch',ha = 'center', fontsize = 15)\n",
        "#     # for i in range(len(values)):\n",
        "#     axes[run].plot(state,values[:,0], label = 'train loss')\n",
        "#     axes[run].plot(state,values[:,1], label = 'test loss')\n",
        "#     axes[run].legend(loc=\"best\")\n",
        "#     axes[run].set_title(f'Run {run+1}')\n",
        "#     axes[run].set_xlabel(f'Epoch', fontsize = 12)\n",
        "#     axes[run].set_ylabel(f'Loss', fontsize = 12)\n",
        "#       # axes[run].set_xticks(np.arange(0,len(values), 10.0))\n",
        "\n",
        "#   plt.savefig('loss_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#   plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvFtZzCflhGB"
      },
      "outputs": [],
      "source": [
        "def plot_routine(state_info_5_runs, is_stepwise,x_label, train_quant, test_quant,supertitle, figname):\n",
        "  f, axes = plt.subplots(1, n_runs,figsize = (7*n_runs,7))\n",
        "  if n_runs == 1:\n",
        "    axes = [axes]\n",
        "  for run in range(len(state_info_5_runs)):\n",
        "    values = []\n",
        "    state = []\n",
        "    for epoch in range(len(state_info_5_runs[run])):\n",
        "      if is_stepwise:\n",
        "        for step in range(len(state_info_5_runs[run][epoch])):\n",
        "          train_q = state_info_5_runs[run][epoch][step][train_quant]\n",
        "          test_q = state_info_5_runs[run][epoch][step][test_quant]\n",
        "          values.append([train_q, test_q])\n",
        "          state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "        \n",
        "        if epoch == step_stop_point:\n",
        "          break\n",
        "      else:\n",
        "        \n",
        "        train_q = state_info_5_runs[run][epoch][-1][train_quant]\n",
        "        test_q = state_info_5_runs[run][epoch][-1][test_quant]\n",
        "        values.append([train_q, test_q])\n",
        "        state.append(int(state_info_5_runs[run][epoch][-1]['epoch']))\n",
        "    \n",
        "    values = np.array(values)\n",
        "    if is_stepwise:\n",
        "      x = [0] + list(np.arange(step_stepsize, len(values)*step_stepsize, step_stepsize))\n",
        "    else:\n",
        "      x = state\n",
        "    plt.suptitle(supertitle,ha = 'center', fontsize = 15)\n",
        "    # for i in range(len(values)):\n",
        "    axes[run].plot(x,values[:,0], 'b-', label = train_quant)\n",
        "    if test_quant == 'test_acc' or test_quant == 'test_loss':\n",
        "      axes[run].plot(x,values[:,1],'r-', label = test_quant)\n",
        "    axes[run].annotate(str(values[-1,0]),xy=(x[-1]-.5,values[-1,0]))\n",
        "    axes[run].annotate(str(values[-1,1]),xy=(x[-1]-.5,values[-1,1]))\n",
        "    axes[run].legend(loc=\"best\")\n",
        "    axes[run].set_title(f'Run {run+1}')\n",
        "    axes[run].set_xlabel(f'{x_label}', fontsize = 12)\n",
        "    # axes[run].set_ylabel(f'Loss', fontsize = 12)\n",
        "      # axes[run].set_xticks(np.arange(0,len(values), 10.0))\n",
        "\n",
        "  plt.savefig(figname + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_stepwise_loss(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = True,x_label = 'Step', train_quant = 'train_loss', test_quant = 'test_loss',supertitle = 'Loss v/s Step', figname='loss_step')\n",
        "def plot_epochwise_loss(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = False,x_label = 'Epoch', train_quant = 'train_loss', test_quant = 'test_loss',supertitle = 'Loss v/s Epoch', figname='loss_epoch')\n",
        "def plot_stepwise_acc(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = True,x_label = 'Step', train_quant = 'train_acc', test_quant = 'test_acc',supertitle = 'Acc v/s Step', figname='acc_step')\n",
        "def plot_epochwise_acc(state_info_5_runs):\n",
        "  plot_routine(state_info_5_runs, is_stepwise = False,x_label = 'Epoch', train_quant = 'train_acc', test_quant = 'test_acc',supertitle = 'Acc v/s Epoch', figname='acc_epoch')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eigen_vals_kernels = []"
      ],
      "metadata": {
        "id": "Ak5umD8_0HcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCAAjeQX1EhA"
      },
      "outputs": [],
      "source": [
        "def find_nearest(array, value):\n",
        "    array = np.asarray(array)\n",
        "    idx = (np.abs(array - value)).argmin()\n",
        "    return idx\n",
        "\n",
        "def get_4_inp_idxs(X_sorted, point1_degrees = 40, point2_degrees = 300, diff_in_degrees = 10):\n",
        "\n",
        "  temp = np.array([(angle + 2*np.pi)*180/np.pi if angle < 0 else angle*180/np.pi for angle in np.arctan2(X_sorted[:,1],X_sorted[:,0]) ], dtype = int)\n",
        "  value1 = point1_degrees\n",
        "  x1_idx = find_nearest(temp, value1)\n",
        "  x2_idx = find_nearest(temp, value1+diff_in_degrees)\n",
        "\n",
        "  value2 = point2_degrees\n",
        "  x3_idx = find_nearest(temp, value2)\n",
        "  x4_idx = find_nearest(temp, value2+diff_in_degrees)\n",
        "  return x1_idx, x2_idx, x3_idx, x4_idx\n",
        "\n",
        "def get_kernels(protocol, hidden_layer_outputs):\n",
        "  hidden_layer_outputs = copy.deepcopy(hidden_layer_outputs)\n",
        "  n_hidden_layer = len(hidden_layer_outputs)\n",
        "  for i in range(n_hidden_layer):\n",
        "    # print(np.array(hidden_layer_outputs[i]).shape)\n",
        "    if protocol == \"MLP\":\n",
        "      temp = np.sign(hidden_layer_outputs[i])\n",
        "    else:\n",
        "      temp = torch.sigmoid(torch.tensor(hidden_layer_outputs[i]))\n",
        "      temp = np.array(temp)\n",
        "    temp[temp <= 0] = 0\n",
        "    # print(temp)\n",
        "    hidden_layer_outputs[i] = temp#.detach().to(\"cpu\"))\n",
        "\n",
        "  hidden_layer_kernels = []\n",
        "  for i in range(n_hidden_layer):\n",
        "    \n",
        "    \n",
        "    hidden_layer_kernels.append(np.matmul(hidden_layer_outputs[i],hidden_layer_outputs[i].T))\n",
        "\n",
        "  lambda_matrix = hidden_layer_kernels[0]\n",
        "  for i in range(1,n_hidden_layer):\n",
        "    lambda_matrix = np.multiply(lambda_matrix, hidden_layer_kernels[i])\n",
        "  # lambda_matrix = np.log10(lambda_matrix+1)\n",
        "  kernels = hidden_layer_kernels + [lambda_matrix]\n",
        "  \n",
        "  #Normalizing overlap matrix by its trace\n",
        "  # eigen_vals = eigh(lambda_matrix, eigvals_only = True)\n",
        "  # lambda_matrix = lambda_matrix/np.sum(eigen_vals)\n",
        "  \n",
        "  return [lambda_matrix]#kernels\n",
        "def get_YKYs(kernels):\n",
        "  \n",
        "  # YKYs = []\n",
        "  # for i in range(len(kernels)):\n",
        "  #   YKY = np.matmul(np.matmul(Y_sorted.T, kernels[i]), Y_sorted)\n",
        "  #   eigen_vals = eigh(kernels[i], eigvals_only = True)\n",
        "  #   YKYs.append(str(round(YKY[0][0]/eigen_vals[-1], 2)))\n",
        "  YKYs = ['NA']*len(kernels)\n",
        "  return YKYs\n",
        "\n",
        "def build_kernels(protocol, hidden_layer_outs_5_runs):\n",
        "  h_l_o = hidden_layer_outs_5_runs\n",
        "  kernel_5_runs = []\n",
        "  # start_end = [0, -1]\n",
        "  for run in range(len(h_l_o)):\n",
        "      kernel_epochs = []\n",
        "      for epoch in range(len(h_l_o[run])):\n",
        "      # for epoch in start_end:\n",
        "        kernel_step = []\n",
        "        for step in range(len(h_l_o[run][epoch])):\n",
        "        # for step in [-1]:\n",
        "          kernels = get_kernels(protocol, h_l_o[run][epoch][step])\n",
        "          \n",
        "          kernel_step.append(kernels)\n",
        "        kernel_epochs.append(kernel_step)\n",
        "      kernel_5_runs.append(kernel_epochs)\n",
        "  return kernel_5_runs\n",
        "# def build_kernels(protocol, hidden_layer_outs_5_runs, state_info_5_runs):\n",
        "#   h_l_o = hidden_layer_outs_5_runs\n",
        "#   kernel_5_runs = []\n",
        "#   instance = 100\n",
        "  \n",
        "#   s_i = state_info_5_runs\n",
        "#   for run in range(1):\n",
        "#       kernel_epochs = []\n",
        "#       for epoch in range(len(h_l_o[run])):\n",
        "#         kernel_step = []\n",
        "#         if int(state_info_5_runs[run][epoch][-1]['epoch'])>= 200:\n",
        "#           for step in range(len(h_l_o[run][epoch])):\n",
        "            \n",
        "#             kernels = get_kernels(protocol, h_l_o[run][epoch][step])\n",
        "#             for mode in range(num_modes):   \n",
        "            \n",
        "#               curr_mode_kernels = np.array(kernels)[:][:,mode*instance : (mode+1)*instance][ :,:,mode*instance : (mode+1)*instance]\n",
        "              \n",
        "#               YKYs = get_YKYs(curr_mode_kernels)\n",
        "#               plot_heatmap(curr_mode_kernels,YKYs, s_i[run][epoch][step], all_input = True, save_fig = True, show_fig = False, mode = mode+1)\n",
        "#               del curr_mode_kernels\n",
        "#               gc.collect()\n",
        "#             del kernels\n",
        "#             gc.collect()\n",
        "#     #     kernel_step.append(kernels)\n",
        "#       #   kernel_epochs.append(kernel_step)\n",
        "#       # kernel_5_runs.append(kernel_epochs)\n",
        "#   return kernel_5_runs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Y4JNYHKQ8h"
      },
      "outputs": [],
      "source": [
        "# build_kernels(protocol, hidden_layer_outs_5_runs, state_info_5_runs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAszjRQsoDG2"
      },
      "outputs": [],
      "source": [
        "def plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  start_end = [0, 3, -1]\n",
        "  for run in range(1):\n",
        "  # for run in [0, 2]:\n",
        "    # run = 2\n",
        "    # for epoch in range(len(kernel_5_runs[run])):\n",
        "    for epoch in start_end:\n",
        "      # for step in range(len(kernel_5_runs[run][epoch])):\n",
        "      for step in [-1]:\n",
        "        # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "        kernels = kernel_5_runs[run][epoch][step]\n",
        "        YKYs = get_YKYs(kernels)\n",
        "        plot_heatmap(kernels,YKYs, s_i[run][epoch][step], all_input = for_all_inputs, save_fig = save_fig, show_fig = show_fig)\n",
        "      if (not for_all_inputs) and epoch == 2:\n",
        "        print(run, epoch)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s_i = state_info_5_runs\n",
        "# start_end = [0, 3,4, 5, 6, 7 ,8, -1]\n",
        "# for run in range(1):\n",
        "# # for run in [0, 2]:\n",
        "#   # run = 2\n",
        "#   # for epoch in range(len(kernel_5_runs[run])):\n",
        "#   for epoch in start_end:\n",
        "#     # for step in range(len(kernel_5_runs[run][epoch])):\n",
        "#     for step in [-1]:\n",
        "#       print(s_i[run][epoch][step])"
      ],
      "metadata": {
        "id": "RY7CuMJGn86T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu_1uUR57hm0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', \n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "\n",
        "def plot_y_prediction(Y_sorted, predictions_run):\n",
        "  init_notebook_mode(connected=True)\n",
        "\n",
        "  #from plotly sliders\n",
        "  data = [dict(\n",
        "          visible = False,\n",
        "          line=dict(color='#00CED1', width=2),\n",
        "          name = ' = '+str(\"step\"),\n",
        "          x = np.arange(0,500,1),\n",
        "          y = d.to('cpu').flatten(),\n",
        "          #  y =Y_sorted.flatten()\n",
        "        ) for d in predictions_run]\n",
        "  data[10]['visible'] = True\n",
        "\n",
        "  data2 = [dict(\n",
        "          visible = False,\n",
        "          line=dict(color='#00CED1', width=2),\n",
        "          name = ' = '+str(\"step\"),\n",
        "          x = np.arange(0,500,1),\n",
        "          y = Y_sorted.flatten(),\n",
        "        ) for d in predictions_run]\n",
        "  data[10]['visible'] = True\n",
        "\n",
        "  #configure added to visualize in colab\n",
        "  configure_plotly_browser_state()\n",
        "\n",
        "  steps = []\n",
        "  for i in range(len(data)):\n",
        "      step = dict(\n",
        "          method = 'restyle',  \n",
        "          args = ['visible', [False] * len(data)],\n",
        "      )\n",
        "      step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
        "      steps.append(step)\n",
        "\n",
        "  sliders = [dict(\n",
        "      active = 10,\n",
        "      currentvalue = {\"prefix\": \"Frequency: \"},\n",
        "      pad = {\"t\": 50},\n",
        "      steps = steps\n",
        "  )]\n",
        "\n",
        "  layout = dict(sliders=sliders)\n",
        "\n",
        "  fig = dict(data=data, layout=layout)\n",
        "\n",
        "  fig2 = dict(data=data2, layout=layout)\n",
        "  configure_plotly_browser_state()\n",
        "\n",
        "  iplot(fig, filename='Sine Wave Slider')\n",
        "  iplot(fig2, filename='Sine Wave Slider')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQBvVtdXmGid"
      },
      "source": [
        "#Plot Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5J4Z3ml63Du"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-lH0wiPbaZt"
      },
      "outputs": [],
      "source": [
        "# plot_labels = ['ReLU', 'DLGN-BOTH-PWC','DLGN-ONPV-PWC', 'DLGN-BOTH-ONPV-PWC']\n",
        "# plot_label = 2\n",
        "# plot_predictions(None, state_collections[plot_label][1], state_collections[plot_label][0],for_all_inputs = True, save_fig = True, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_predictions(None, state_collections[0][1], state_collections[0][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[1][1], state_collections[1][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[2][1], state_collections[2][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[3][1], state_collections[3][0],for_all_inputs = True, save_fig = False, show_fig = True)"
      ],
      "metadata": {
        "id": "92YyDda2kS-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J37AGK_2f18f"
      },
      "outputs": [],
      "source": [
        "def plot_predictions_old(mode, predictions_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  test_data = test_data_curr\n",
        "  test_labels = test_labels_curr\n",
        "  num_data = len(test_data)\n",
        "  # mode = mode\n",
        "  # size_low_mode = num_data/2/20\n",
        "  # if mode == 21:\n",
        "  #   strt = int(num_data/2)\n",
        "  #   end = num_data\n",
        "  #   reqd_mode  = test_data[strt:end,-2:]\n",
        "  # else:\n",
        "  #   strt = int((mode-1)*size_low_mode)\n",
        "  #   end = int(mode*size_low_mode)\n",
        "  #   reqd_mode  = test_data[strt:end,(mode-1)*2:mode*2]\n",
        "  start_end = [0, 3, -1]\n",
        "  for run in range(1):\n",
        "    # for epoch in range(len(predictions_5_runs[run])):\n",
        "    for epoch in start_end:\n",
        "      # for step in range(len(predictions_5_runs[run][epoch])):\n",
        "      for step in [-1]:\n",
        "        fig, ax = plt.subplots(1,2, figsize = (10, 5))\n",
        "        # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "        preds = predictions_5_runs[run][epoch][step]\n",
        "        # preds =  preds.softmax(1).argmax(1).detach().to('cpu').numpy()\n",
        "        preds =  preds.detach().to('cpu').numpy()\n",
        "        angle = np.arange(0, 360, 360/500)\n",
        "        ax[1].plot(angle, preds)\n",
        "        ax[0].plot(angle, test_labels)\n",
        "        \n",
        "       \n",
        "        # ax[0].scatter(reqd_mode[:,0],reqd_mode[:,1],c =  test_labels[strt:end])\n",
        "        # ax[1].scatter(reqd_mode[:,0],reqd_mode[:,1],c = preds[strt:end])\n",
        "        # curr_acc = accuracy_score(test_labels[strt:end], preds[strt:end])\n",
        "        curr_acc = 'NA'\n",
        "        state_info = state_info_5_runs[run][epoch][step]\n",
        "        title = state_info['model_protocol_type']+','+'Run=' + str(state_info[\"run\"])+',' +'Epoch = '+str(state_info['epoch'])+','+'step='+ str(state_info['step']) +',' + \"tr_loss=\" + str(state_info['train_loss']) +',' +'tr_acc='+str(state_info['train_acc'])+','+'te_acc='+str(state_info['test_acc'])+','+'curr_mode_acc='+str(curr_acc)\n",
        "        plt.suptitle(title)\n",
        "\n",
        "        if show_fig:\n",
        "          plt.show()\n",
        "        if save_fig:\n",
        "          fig.savefig(f'Preds({title}).png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        "        \n",
        "        plt.close(fig)\n",
        "\n",
        "def plot_predictions(mode, predictions_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  test_data = test_data_curr\n",
        "  test_labels = test_labels_curr\n",
        "  num_data = len(test_data)\n",
        "  \n",
        "  start_end = [0, 3, -1]\n",
        "  plot_labels = ['Initial Epoch', 'Mid($3^{rd}$) epoch', 'Final($8000^{th}$) epoch']\n",
        "  plot_labels_name = ['ReLU', 'DLGN-BOTH-PWC','DLGN-ONPV-PWC', 'DLGN-BOTH-ONPV-PWC']\n",
        "  fig, ax = plt.subplots(1,1, figsize = (10, 5))\n",
        "  \n",
        "  angle = np.arange(0, 360, 360/500)\n",
        "  ax.plot(angle, test_labels, 'black', label = 'True label function', linewidth = 1.5)\n",
        "  for run in range(1):\n",
        "    # for epoch in range(len(predictions_5_runs[run])):\n",
        "    for idx, epoch in enumerate(start_end):\n",
        "      # for step in range(len(predictions_5_runs[run][epoch])):\n",
        "      for step in [-1]:\n",
        "        \n",
        "        # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "        preds = predictions_5_runs[run][epoch][step]\n",
        "        # preds =  preds.softmax(1).argmax(1).detach().to('cpu').numpy()\n",
        "        preds =  preds.detach().to('cpu').numpy()\n",
        "       \n",
        "        ax.plot(angle, preds,linestyle='dashed', label = plot_labels[idx], linewidth = 2.5 )\n",
        "       \n",
        "        \n",
        "       \n",
        "        # ax[0].scatter(reqd_mode[:,0],reqd_mode[:,1],c =  test_labels[strt:end])\n",
        "        # ax[1].scatter(reqd_mode[:,0],reqd_mode[:,1],c = preds[strt:end])\n",
        "        # curr_acc = accuracy_score(test_labels[strt:end], preds[strt:end])\n",
        "  \n",
        "  plt.xlabel('angle in radians')\n",
        "  plt.xticks([0, 180, 360], ['0', '$\\pi$', '$2\\pi$'])\n",
        "  title = f'{plot_labels_name[plot_label]} Predictions'\n",
        "  # plt.suptitle(title)\n",
        "  plt.legend()\n",
        "  if show_fig:\n",
        "    plt.show()\n",
        "  if save_fig:\n",
        "    fig.savefig(f'Preds({title}).pdf',format = 'pdf', bbox_inches='tight', dpi = 200)\n",
        "  \n",
        "  plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpXbBBPVw3Sx"
      },
      "outputs": [],
      "source": [
        "# plot_loss_iclr( save_fig = True, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state_collections = [0] + state_collections"
      ],
      "metadata": {
        "id": "aknBkeelwe79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(state_collections)"
      ],
      "metadata": {
        "id": "V7XAwHqfl38H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW_PYbtevFX0"
      },
      "outputs": [],
      "source": [
        "def plot_loss_iclr( save_fig = True, show_fig = False):\n",
        "  s_i = state_info_5_runs\n",
        "  test_data = test_data_curr\n",
        "  test_labels = test_labels_curr\n",
        "  num_data = len(test_data)\n",
        "  \n",
        "  \n",
        "  plot_labels = ['ReLU', 'DLGN','DLGN-Fixed-Gating', 'DLGN-Hybrid']\n",
        "  fig, ax = plt.subplots(1,1, figsize = (10, 5))\n",
        "  \n",
        "  \n",
        "  for i in range(4):\n",
        "    loss = []\n",
        "    x = []\n",
        "    for run in range(1):\n",
        "      for epoch in range(len(state_collections[i][0][run])):\n",
        "        for step in [-1]:\n",
        "        # for step in range(len(predictions_5_runs[run][epoch])):\n",
        "        \n",
        "          \n",
        "          # kernels = get_kernels(h_l_o[run][epoch][step])\n",
        "          x.append(int(state_collections[i][0][run][epoch][step]['epoch']))\n",
        "          loss.append(state_collections[i][0][run][epoch][step]['train_loss'])\n",
        "          # preds =  preds.softmax(1).argmax(1).detach().to('cpu').numpy()\n",
        "          \n",
        "        \n",
        "    ax.plot(x, loss, label = plot_labels[i] )\n",
        "        \n",
        "          \n",
        "        \n",
        "          # ax[0].scatter(reqd_mode[:,0],reqd_mode[:,1],c =  test_labels[strt:end])\n",
        "          # ax[1].scatter(reqd_mode[:,0],reqd_mode[:,1],c = preds[strt:end])\n",
        "          # curr_acc = accuracy_score(test_labels[strt:end], preds[strt:end])\n",
        "    \n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('MSE loss')\n",
        "  title = 'Loss curves'\n",
        "  plt.suptitle(title)\n",
        "  plt.legend()\n",
        "  if show_fig:\n",
        "    plt.show()\n",
        "  if save_fig:\n",
        "    fig.savefig(f'Loss({title}).pdf',format = 'pdf', bbox_inches='tight', dpi = 200)\n",
        "  \n",
        "  plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkjaTiLSmMUT"
      },
      "source": [
        "#*CNN* routine funtions for generting hyperplanes and local usefulness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB2mbIIVuX_n"
      },
      "outputs": [],
      "source": [
        "def multiply_higher_dimensional_matrices(a, b):\n",
        "  a_shape, b_shape = a.size(), b.size()\n",
        "  ab = []\n",
        "  \n",
        "  for r in range(a_shape[0]):\n",
        "    temp = []\n",
        "    for c in range(b_shape[1]):\n",
        "      temp.append(torch.sum(torch.matmul(a[r,:], b[:,c]), axis = 0))\n",
        "    temp = torch.stack(temp)\n",
        "    ab.append(temp)\n",
        "  return torch.stack(ab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WOZquNTLtuW"
      },
      "outputs": [],
      "source": [
        "def convert_cnn_weights(layer_weights, layer_biases, protocol_type = 'DLGN'):\n",
        "  l_w, l_b = get_cnn_flattend_weights(torch.tensor(layer_weights[0]).to(device)), get_cnn_flattend_bias(torch.tensor(layer_biases[0]).to(device))\n",
        "  l_ws = [l_w]\n",
        "  l_bs = [l_b]\n",
        "  \n",
        "  for i in range(1, n_hidden_layers):\n",
        "    \n",
        "    curr_l_w =  torch.tensor(layer_weights[i]).to(device)\n",
        "    curr_l_b = torch.tensor(layer_biases[i]).to(device)\n",
        "    \n",
        "    if protocol_type == 'DLGN':\n",
        "\n",
        "        if len(curr_l_w.shape) > 2: \n",
        "          flattend_curr_l_w = get_cnn_flattend_weights(curr_l_w)\n",
        "          l_w = torch.matmul(input = flattend_curr_l_w ,other = l_w) #multiply_higher_dimensional_matrices(curr_layer_weight, layer_weight)\n",
        "          l_b =  torch.matmul(input= flattend_curr_l_w, other =  l_b) + get_cnn_flattend_bias(curr_l_b)\n",
        "        else:\n",
        "          l_w = torch.matmul(input= curr_l_w, other =  l_w)\n",
        "          l_b = torch.matmul(input= curr_l_w, other =  l_b) + curr_l_b\n",
        "\n",
        "    elif protocol_type == 'DLGN-SF':\n",
        "      layer_weight = curr_layer_weight\n",
        "      layer_bias = curr_layer_bias\n",
        "\n",
        "    \n",
        "    l_ws.append(l_w)\n",
        "    l_bs.append(l_b)\n",
        "  return l_ws, l_bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJMpGRE8heqa"
      },
      "outputs": [],
      "source": [
        "def get_cnn_flattend_weights(layer_weight):\n",
        "  \n",
        "  kernel_size = 5\n",
        "  filter_size = 8\n",
        "  ks = kernel_size\n",
        "  padding = int((ks - 1)/2)\n",
        "  start,end = 0, 32#-padding, 32-int(ks/2)   \n",
        "  hyperplanes = [ [] for i in range(filter_size)]\n",
        "  count = 0\n",
        "  \n",
        "  for i in range(start, end, 1):\n",
        "    for j in range(start, end, 1):\n",
        "      map_idx = []\n",
        "      for fs_i in range(ks):\n",
        "        for fs_j in range(ks):\n",
        "          map_idx.append( 36 * (i+fs_i) + j+fs_j )#(i+fs_i,j+fs_j))\n",
        "      for filter in range(filter_size):\n",
        "        # print(f\"filter n { filter}\")\n",
        "        for channel_n in range(layer_weight.shape[1]):\n",
        "          # print(f\"channel n { channel_n}\")\n",
        "          channel =  torch.zeros(size = (36*36,)).to(device)\n",
        "          channel[map_idx] = layer_weight[filter][channel_n].flatten()\n",
        "          channel = channel.reshape((36, 36))\n",
        "          \n",
        "          channel = channel[2:-2, 2:-2]\n",
        "          channel = channel.flatten()\n",
        "          \n",
        "          if channel_n == 0:\n",
        "            temp = channel\n",
        "            \n",
        "          else:\n",
        "            temp = torch.cat((temp, channel))\n",
        "        hyperplanes[filter].append(temp)\n",
        "        \n",
        "      \n",
        "  for idx in range(len(hyperplanes)):\n",
        "    hyperplanes[idx] = torch.stack(hyperplanes[idx]).to(device)\n",
        "  \n",
        "  hyperplanes = torch.stack(hyperplanes).to(device)\n",
        "\n",
        "  # hyperplanes = torch.cat([torch.tensor(x for x in hyperplanes], dim=0)\n",
        "  hyperplanes = hyperplanes.reshape((-1, hyperplanes.shape[-1]))\n",
        "  return hyperplanes\n",
        "\n",
        "def get_cnn_flattend_bias(bias):\n",
        "  return bias.repeat(32*32,1).T.flatten().float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qac5xEWvgL-s"
      },
      "outputs": [],
      "source": [
        "# a, b = unknown(weights_biases_all_runs, .05, test_data_curr, test_labels_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0LLheNdSirl"
      },
      "outputs": [],
      "source": [
        "# plot_hyp_posneg(posneg_pairs_all_runs = a, epsilon = .05)\n",
        "# plot_hyp_posneg_ratio(posneg_pairs_all_runs = a, epsilon = .05)\n",
        "# plot_hyp_local_usefulness(local_usefulness_all_runs = b, epsilon = .05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ZgmhfMqAMa"
      },
      "outputs": [],
      "source": [
        "# layer_weights = weights_biases_all_runs[0][0][0][0]\n",
        "# layer_biases = weights_biases_all_runs[0][0][0][1]\n",
        "# for i in range(n_hidden_layers):\n",
        "#   temp = get_cnn_flattend_weights(layer_weights[i])\n",
        "#   print(temp.dtype)\n",
        "#   print(get_cnn_flattend_bias(layer_biases[i]).dtype)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDPQ9-SYklDB"
      },
      "outputs": [],
      "source": [
        "# def unknown_routine(layer_weights, layer_biases, data, i, temp_data):\n",
        "#   if is_DNN:\n",
        "#         weight_mags = np.linalg.norm(layer_weights[i], axis = 1)\n",
        "#         weight_mags = np.expand_dims(weight_mags, axis =1)\n",
        "#         num = np.absolute(np.matmul(layer_weights[i], data.T) + np.expand_dims(layer_biases[i], axis = 1))\n",
        "#         d = num / weight_mags #n_nodes x n_examples\n",
        "#         d = d.T # n_examples x n_nodes \n",
        "#         # d = np.absolute(np.multiply(np.expand_dims(inp_plt_x, axis = 1), np.expand_dims(layer_slopes[i], axis = 0)) + np.expand_dims(inp_plt_y*(-1), axis = 1) + np.expand_dims(layer_intercepts[i], axis = 0)) / np.expand_dims(np.sqrt(layer_slopes[i]**2 + 1), axis = 0)\n",
        "#   else:\n",
        "#     l_weights = get_cnn_flattend_weights(layer_weights[i])\n",
        "#     weight_mags = np.linalg.norm(l_weights, axis = -1)\n",
        "#     weight_mags = np.expand_dims(weight_mags, axis =-1)\n",
        "#     num = np.absolute(np.matmul(l_weights, temp_data.T) + np.expand_dims(layer_biases[i], axis = (1,2)))\n",
        "#     d = num / weight_mags #n_nodes x n_examples\n",
        "#     d = np.reshape(d, (-1, temp_data.shape[0]))\n",
        "#     d = d.T #n_examples x n_nodes\n",
        "#     # d = d[:,np.random.randint(0, d.shape[1], size = (500,))]\n",
        "#   return d\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alCUaO77Ozz0"
      },
      "outputs": [],
      "source": [
        "# mask = torch.ones((2000)).long()\n",
        "# dat = torch.rand((2000, 3072))\n",
        "# dat[mask,:].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzTIkLg-Oy08"
      },
      "outputs": [],
      "source": [
        "# print(bool_mask[:,j].shape)\n",
        "#           print(temp_data[bool_mask[:,j], :].T.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMMBi0ADoTfJ"
      },
      "outputs": [],
      "source": [
        "# posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs, .015, test_data_curr, test_labels_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZD8G3lLCMkx"
      },
      "outputs": [],
      "source": [
        "# a, b = unknown(weights_biases_all_runs, .25, test_data_curr, test_labels_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmYi4aZXOl82"
      },
      "outputs": [],
      "source": [
        "def  get_hyperplanes_params(model,protocol_type,  for_layers, for_mode):\n",
        "  layer_weights = []\n",
        "  layer_slopes = []\n",
        "  layer_intercepts = []\n",
        "  layer_biases = []\n",
        "  # if for_mode == 1:\n",
        "  #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,0:2]\n",
        "  # elif for_mode == 21:\n",
        "  #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,-2:]\n",
        "  layer_weight = model.layers[0].weight.detach().clone()\n",
        "  layer_bias = model.layers[0].bias.detach().clone()\n",
        "  if is_DNN:\n",
        "    pass\n",
        "    # layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "    # layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "    # layer_slopes.append(layer_slope)\n",
        "    # layer_intercepts.append(layer_intercept)\n",
        "\n",
        "  layer_weights.append(layer_weight)\n",
        "  layer_biases.append(layer_bias)\n",
        "  for i in range(1, for_layers):\n",
        "    curr_layer_weight =  model.layers[i].weight.detach().clone()\n",
        "    curr_layer_bias = model.layers[i].bias.detach().clone()\n",
        "    if protocol_type == 'DLGN':\n",
        "      if is_DNN:\n",
        "        layer_weight = torch.matmul(input= curr_layer_weight, other =  layer_weight)\n",
        "        layer_bias = torch.matmul(input= curr_layer_weight, other =  layer_bias) + curr_layer_bias\n",
        "      else:\n",
        "        layer_weight = curr_layer_weight #multiply_higher_dimensional_matrices(curr_layer_weight, layer_weight)\n",
        "        layer_bias =  curr_layer_bias\n",
        "    elif protocol_type == 'DLGN-SF':\n",
        "      layer_weight = curr_layer_weight\n",
        "      layer_bias = curr_layer_bias\n",
        "\n",
        "    if is_DNN:\n",
        "      pass\n",
        "      # layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "      # layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "    \n",
        "      # layer_slopes.append(layer_slope)\n",
        "      # layer_intercepts.append(layer_intercept)\n",
        "\n",
        "    layer_weights.append(layer_weight)\n",
        "    layer_biases.append(layer_bias)\n",
        "  return layer_weights, layer_biases, layer_slopes, layer_intercepts\n",
        "\n",
        "\n",
        "def plot_posneg_hyperplanes(model, for_layers, epsilon, model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "  # layer_weights, layer_biases, _, _ = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "  return [None, None]#[layer_weights, layer_biases]\n",
        "\n",
        "\n",
        "def unknown(weights_biases_all_runs, epsilon, data, labels):\n",
        "  step = -1\n",
        "  w_idx, b_idx = 0, 1\n",
        "  \n",
        "  posneg_pairs_all_runs = []\n",
        "  local_usefulness_all_runs = []\n",
        "\n",
        "  for run in range(len(weights_biases_all_runs)):\n",
        "    posneg_pairs_all_epochs = []\n",
        "    local_usefulness_all_epochs = []\n",
        "\n",
        "    for epoch in range(len(weights_biases_all_runs[run])):\n",
        "      posneg_pairs_all_step = []\n",
        "      local_usefulness_all_step = []\n",
        "      for step in range(len(weights_biases_all_runs[run][epoch])):\n",
        "        layer_weights = weights_biases_all_runs[run][epoch][step][w_idx]\n",
        "        layer_biases = weights_biases_all_runs[run][epoch][step][b_idx]\n",
        "        \n",
        "        if not is_DNN:\n",
        "          layer_weights,layer_biases  = convert_cnn_weights(layer_weights, layer_biases, protocol_type = 'DLGN')\n",
        "\n",
        "        posneg_pairs_all_epochs.append(get_posneg_pairs(layer_weights, layer_biases, epsilon, data, labels))\n",
        "        local_usefulness_all_epochs.append(get_local_usefulness(layer_weights, layer_biases, epsilon, data, labels))\n",
        "\n",
        "      \n",
        "    posneg_pairs_all_runs.append(posneg_pairs_all_epochs)\n",
        "    local_usefulness_all_runs.append(local_usefulness_all_epochs)\n",
        "  return posneg_pairs_all_runs, local_usefulness_all_runs\n",
        "\n",
        "def get_posneg_pairs(layer_weights, layer_biases, epsilon, data, labels):\n",
        "  posneg_pairs_all_layers = []\n",
        "  temp_data = data.reshape((data.shape[0], -1))\n",
        "  # for i in range(data.shape[0]):\n",
        "  #   temp_data.append(np.ravel(data[i]))\n",
        "  temp_data = torch.tensor(temp_data, device = device)\n",
        "  \n",
        "  labels = torch.tensor(labels, device = device)\n",
        "  for i in range(n_hidden_layers):     \n",
        "      # print(layer_weights[i].shape)\n",
        "      # debug()\n",
        "      \n",
        "      if is_DNN:\n",
        "        # print(layer_weights[i].size())\n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = 1, keepdim = True)\n",
        "        # print(weight_mags.size())\n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis = 1))\n",
        "        # print(num.size())\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        d = d.T\n",
        "        # print(d.size())\n",
        "      else:\n",
        "        # l_weights = get_cnn_flattend_weights(layer_weights[i])\n",
        "        \n",
        "        # l_weights = torch.tensor(l_weights, device = device).float()\n",
        "        \n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = -1)\n",
        "        \n",
        "        \n",
        "        weight_mags = torch.unsqueeze(weight_mags, axis =-1)\n",
        "        \n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis = 1))\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        # d = torch.reshape(d, (-1, temp_data.size()[0]))\n",
        "        d = d.T #n_examples x n_nodes\n",
        "        \n",
        "        # d = d[:,np.random.randint(0, d.shape[1], size = (500,))]\n",
        "      \n",
        "      \n",
        "      bool_mask = d <= epsilon\n",
        "      posneg_pairs = []\n",
        "      if is_DNN:\n",
        "        for j in range(d.size()[1]):\n",
        "          data_points_within_epsilon = labels[:,None][bool_mask[:,j]]\n",
        "          n_pos_within_epsilon = torch.sum(data_points_within_epsilon)\n",
        "          n_neg_within_epsilon = torch.tensor(len(data_points_within_epsilon), device = device) - n_pos_within_epsilon\n",
        "          posneg_pairs.append((n_pos_within_epsilon.item(), n_neg_within_epsilon.item()))\n",
        "        posneg_pairs_all_layers.append(posneg_pairs)\n",
        "      else:\n",
        "        for j in range(d.size()[-1]):\n",
        "          \n",
        "          data_points_within_epsilon = labels[:,None][bool_mask[:,j]]\n",
        "         \n",
        "          n_pos_within_epsilon = torch.sum(data_points_within_epsilon)\n",
        "          n_neg_within_epsilon = torch.tensor(len(data_points_within_epsilon), device = device) - n_pos_within_epsilon\n",
        "          posneg_pairs.append((n_pos_within_epsilon.item(), n_neg_within_epsilon.item()))\n",
        "\n",
        "        posneg_pairs_all_layers.append(posneg_pairs)\n",
        "      \n",
        "  return posneg_pairs_all_layers\n",
        "def get_local_usefulness(layer_weights, layer_biases, epsilon, data, labels):\n",
        "  local_usefulness_all = []\n",
        "  signed_labels = torch.tensor(labels, device = device)\n",
        "  signed_labels[signed_labels == 0] = -1 \n",
        "  temp_data = data.reshape((data.shape[0], -1))\n",
        "  temp_data = torch.tensor(temp_data, device = device)\n",
        " \n",
        "  \n",
        "  for i in range(n_hidden_layers):\n",
        "      if is_DNN:\n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = -1, keepdim = True)\n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis =1))\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        # d = torch.reshape(d, (-1, temp_data.shape[0]))\n",
        "        d = d.T #n_examples x n_nodes\n",
        "        # d = np.absolute(np.multiply(np.expand_dims(inp_plt_x, axis = 1), np.expand_dims(layer_slopes[i], axis = 0)) + np.expand_dims(inp_plt_y*(-1), axis = 1) + np.expand_dims(layer_intercepts[i], axis = 0)) / np.expand_dims(np.sqrt(layer_slopes[i]**2 + 1), axis = 0)\n",
        "      else:\n",
        "        # l_weights = get_cnn_flattend_weights(layer_weights[i])\n",
        "        # l_weights = torch.tensor(l_weights, device = device).float()\n",
        "        weight_mags = torch.linalg.norm(layer_weights[i], axis = -1)\n",
        "        weight_mags = torch.unsqueeze(weight_mags, axis =-1)\n",
        "        num = torch.absolute(torch.matmul(layer_weights[i], temp_data.T) + torch.unsqueeze(layer_biases[i], axis =1))\n",
        "        d = num / weight_mags #n_nodes x n_examples\n",
        "        # d = torch.reshape(d, (-1, temp_data.shape[0]))\n",
        "        d = d.T #n_examples x n_nodes\n",
        "        # d = d[:,np.random.randint(0, d.shape[1], size = (500,))]\n",
        "        \n",
        "        \n",
        "       \n",
        "      local_usefulness_per_node = []\n",
        "      bool_mask = d <= epsilon\n",
        "   \n",
        "      l_u = torch.tensor(0, dtype = torch.float, device = device)\n",
        "      for j in range(d.size()[1]):\n",
        "        if is_DNN:\n",
        "        \n",
        "          wx_b = torch.matmul(layer_weights[i][j], temp_data[bool_mask[:,j]].T) + torch.unsqueeze(layer_biases[i], axis =1)[j]\n",
        "          signed_wx_b = torch.sign(wx_b)\n",
        "          \n",
        "          \n",
        "          temp_labels = (signed_labels[:,None][bool_mask[:,j]]).T\n",
        "         \n",
        "          # print(signed_wx_b.shape, np.multiply(signed_wx_b, labels).shape)\n",
        "          local_usefulness = torch.absolute(torch.sum(torch.multiply(signed_wx_b, temp_labels))/temp_labels.shape[1])\n",
        "          local_usefulness_per_node.append(local_usefulness.item())\n",
        "          \n",
        "        else:\n",
        "          # l_weights = torch.reshape(l_weights, (-1,temp_data.size()[1] ))\n",
        "          \n",
        "          # print(bool_mask[:,j].shape)\n",
        "          # print(temp_data[bool_mask[:,j], :].T.shape)\n",
        "          \n",
        "\n",
        "          # print(layer_weights[i][j].shape, temp_data[bool_mask[:,j]].T.shape, torch.unsqueeze(layer_biases[i], axis =1)[j])\n",
        "          \n",
        "          wx_b = torch.matmul(layer_weights[i][j], temp_data[bool_mask[:,j]].T) + torch.unsqueeze(layer_biases[i], axis =1)[j]\n",
        "          signed_wx_b = torch.sign(wx_b)\n",
        "          \n",
        "          temp_labels = (signed_labels[:,None][bool_mask[:,j]]).T\n",
        "\n",
        "          # print(signed_wx_b.shape, np.multiply(signed_wx_b, labels).shape)\n",
        "          local_usefulness = torch.absolute(torch.sum(torch.multiply(signed_wx_b, temp_labels))/temp_labels.shape[1])\n",
        "          # print(local_usefulness)\n",
        "          # debug()\n",
        "          \n",
        "          local_usefulness_per_node.append(local_usefulness.item())\n",
        "          # if (j != 0 and j%784 == 0):\n",
        "            \n",
        "          #   local_usefulness_per_node.append(l_u.item()/784)\n",
        "          #   l_u = torch.tensor(0, dtype = torch.float, device = device)\n",
        "\n",
        "          # l_u += local_usefulness\n",
        "      \n",
        "      # local_usefulness_per_node = np.reshape(local_usefulness_per_node, (8, -1))\n",
        "      # local_usefulness_per_node = np.partition(local_usefulness_per_node, kth = -800, axis = -1)\n",
        "      # local_usefulness_per_node = local_usefulness_per_node[ -800:]\n",
        "      # local_usefulness_per_node = np.mean(local_usefulness_per_node, axis = 1)\n",
        "      local_usefulness_all.append(local_usefulness_per_node)\n",
        "  return np.array(local_usefulness_all)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M72vt8AmiUl"
      },
      "source": [
        "#Commented previous hyperplane codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE_8guUjmnGr"
      },
      "outputs": [],
      "source": [
        "# def plot_posneg_hyperplanessssss(model, for_layers, epsilon, model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "#   layer_weights, layer_biases, _, _ = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "#   # debug()\n",
        "#   if for_mode == 1:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "#   elif for_mode == 21:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,-2], test_data_curr[:,-1]\n",
        "#   n_posneg_pairs_all = []\n",
        "#   for i in range(for_layers):\n",
        "#     # for neuron in model.n_neurons:\n",
        "     \n",
        "#       weight_mags = np.linalg.norm(layer_weights[i], axis = 1)\n",
        "#       weight_mags = np.expand_dims(weight_mags, axis =1)\n",
        "#       num = np.absolute(np.matmul(layer_weights[i], test_data_curr.T) + np.expand_dims(layer_biases[i], axis = 1))\n",
        "#       d = num / weight_mags #n_nodes x n_examples\n",
        "#       d = d.T\n",
        "#       # d = np.absolute(np.multiply(np.expand_dims(inp_plt_x, axis = 1), np.expand_dims(layer_slopes[i], axis = 0)) + np.expand_dims(inp_plt_y*(-1), axis = 1) + np.expand_dims(layer_intercepts[i], axis = 0)) / np.expand_dims(np.sqrt(layer_slopes[i]**2 + 1), axis = 0)\n",
        "    \n",
        "#       # d = np.absolute((layer_slopes[i][neuron]*inp_plt_x + inp_plt_y*(-1) + layer_intercepts[i][neuron]) / np.sqrt(layer_slopes[i][neuron]**2 + 1))\n",
        "#       n_posneg_pairs = []\n",
        "#       for j in range(d.shape[1]):\n",
        "#         bool_mask = d <= epsilon\n",
        "#         n_pos_within_epsilon = np.sum(test_labels_curr[:,np.newaxis][bool_mask[:,j]])\n",
        "#         n_neg_within_epsilon = len(test_labels_curr[:,np.newaxis][bool_mask[:,j]]) - n_pos_within_epsilon\n",
        "#         n_posneg_pairs.append((n_pos_within_epsilon, n_neg_within_epsilon))\n",
        "\n",
        "#       n_posneg_pairs_all.append(d)\n",
        "#   return n_posneg_pairs_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # plot_hyperplanes(npf_model, model_protocol_type + \" \" + model_learning_status)\n",
        "# def plot_hyperplanes(model, for_layers,model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "#   # plot_posneg_hyperplanes(model, for_layers,model_protocol_type = model_protocol_type,run = run,  epoch = epoch, step = step, for_mode = 1)\n",
        "#   # for_layers = 1#len(model.layers)-1\n",
        "#   layer_weights, layer_biases, layer_slopes, layer_intercepts = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "\n",
        "#   if for_mode == 1:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "#   elif for_mode == 21:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,-2], test_data_curr[:,-1]\n",
        "\n",
        "#   fig = plt.figure(figsize=(10*model.n_hidden_layers, 10))\n",
        "#   outer = gridspec.GridSpec(1, for_layers, wspace=0.5, hspace=0.1)\n",
        "\n",
        "#   start, stop = -6.3, 6.3\n",
        "#   x_axis = np.linspace(start = start, stop = stop, num = 2, endpoint=True)\n",
        "\n",
        "#   # f_p, axes_p = plt.subplots(1,for_layers, figsize = (8/1.5 * for_layers, 16/1.5)  )\n",
        "  \n",
        "#   for i in range(for_layers):\n",
        "\n",
        "#     inner = gridspec.GridSpecFromSubplotSpec(8, 4, subplot_spec=outer[i], wspace=0.1, hspace=0.1)\n",
        "    \n",
        "#     # debug()\n",
        "\n",
        "#     # plt.figure(figsize = (10,10))\n",
        "#     #fig, ax = plt.subplots(figsize = (10,10))\n",
        "\n",
        "\n",
        "#     # f, axes = plt.subplots(8, 4, figsize = (8/1.5,16/1.5))\n",
        "#     for r in range(8):\n",
        "#       for c in range(4):\n",
        "#         ax = plt.Subplot(fig, inner[r*4 + c])\n",
        "\n",
        "#         ax.scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1)\n",
        "#         ax.set_xlim((start,stop))\n",
        "#         ax.set_ylim((start,stop))\n",
        "#         y_axis = layer_slopes[i][r*4 + c]*x_axis + layer_intercepts[i][r*4 + c]\n",
        "\n",
        "#         #calc #+ve pts and -ve pts around epsilon dist from the hyperplane\n",
        "        \n",
        "\n",
        "\n",
        "#         #plt.plot(x_axis, y_axis)\n",
        "#         ax.plot( x_axis, y_axis, 'r')\n",
        "#         weight_vec = layer_weights[i][r*4 + c]\n",
        "       \n",
        "#         weight_vec_mag = torch.sqrt(torch.sum(torch.square(weight_vec)))\n",
        "#         origin = np.array([[0, 0, 0],[0, 0, 0]])\n",
        "#         ax.quiver( np.array([weight_vec[0]]) / weight_vec_mag ,np.array([weight_vec[1]]) / weight_vec_mag, angles='xy', scale_units='xy', scale=1, color =  ['b'], width = .01, headwidth = 12, headlength = 8)\n",
        "#         ax.set_aspect(True)\n",
        "#         # print(layer_weights[i][r*4 + c])\n",
        "#         # ax.fill_between(x_axis, y_axis, y_axis +  (weight_vec[1]/weight_vec_mag - layer_intercepts[i][r*4 + c]), alpha = .4)\n",
        "#         inter =  (weight_vec[1] / weight_vec_mag) - layer_slopes[i][r*4 + c]* (weight_vec[0] / weight_vec_mag)\n",
        "#         ax.fill_between(x_axis, y_axis,  layer_slopes[i][r*4 + c]*x_axis + inter, alpha = .4)\n",
        "\n",
        "#         # ax.fill_between(x_axis, y_axis, np.array([0, weight_vec[0]]) / weight_vec_mag, alpha = .2)\n",
        "#         fig.add_subplot(ax)\n",
        "   \n",
        "#   fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\")\n",
        "#   fig.savefig('Hyperplanes '+model_protocol_type +', ' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.png\",format = 'png', bbox_inches='tight', dpi = 100)\n",
        "  \n",
        "#   plt.cla()\n",
        "#   plt.clf()\n",
        "#   plt.close('all')\n",
        "\n",
        "#   plot_hyperplanes_all(model, for_layers,model_protocol_type = model_protocol_type,run = run,  epoch = epoch, step = step, for_mode = 1)\n",
        "  \n",
        "# def plot_hyperplanes_all(model, for_layers,model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "#   layer_weights, layer_biases, layer_slopes, layer_intercepts = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "\n",
        "#   if for_mode == 1:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "#   elif for_mode == 21:\n",
        "#     inp_plt_x, inp_plt_y  = test_data_curr[:,-2], test_data_curr[:,-1]\n",
        "\n",
        "#   x_axis = np.linspace(start = -6.3, stop = 6.3, num = 2, endpoint=True)\n",
        "#   # f_p, axes_p = plt.subplots(1,for_layers, figsize = (8/1.5 * for_layers, 16/1.5)  )\n",
        "#   start, stop = -6.3, 6.3\n",
        "#   fig, ax = plt.subplots(nrows=1, ncols=for_layers, figsize=(5*for_layers,5))\n",
        "#   for i in range(for_layers):\n",
        "#     ax[i].scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1)\n",
        "#     for r in range(8):\n",
        "#       for c in range(4):\n",
        "        \n",
        "#         ax[i].set_xlim((start,stop))\n",
        "#         ax[i].set_ylim((start,stop))\n",
        "        \n",
        "#         # ax[i].set_xlim((-1.2,1.2))\n",
        "#         # ax[i].set_ylim((-1.2,1.2))\n",
        "#         y_axis = layer_slopes[i][r*4 + c]*x_axis + layer_intercepts[i][r*4 + c]\n",
        "#         #plt.plot(x_axis, y_axis)\n",
        "#         ax[i].plot( x_axis, y_axis, 'r')\n",
        "#     # plt.show()\n",
        "#     fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\")\n",
        "#     fig.savefig('Hyperplanes_all '+model_protocol_type +', '+f'Mode={for_mode}' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.png\",format = 'png', bbox_inches='tight', dpi = 100)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRbcS_Wfm7-l"
      },
      "source": [
        "#Plotting functions for hyperplanes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRj7Gjibj_1y",
        "outputId": "7a47eda2-ee64-44e9-95f2-8f5cff90807f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "a = [2]\n",
        "type(a) == list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQrD0K9uhYtE"
      },
      "outputs": [],
      "source": [
        "def plot_hyp_total_posneg(posneg_pairs_all_runs,n_neurons, epsilon, temp_mask = None):  \n",
        "  n_parts = 1\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers #np.array(posneg_pairs_all_runs[0]).shape[1]\n",
        "\n",
        "  fig = plt.figure(figsize=(5*n_parts, n_layers*5))\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "  posneg_id = ['positive', 'negative']\n",
        "  unwraped_posneg  = unwrap_routine(posneg_pairs_all_runs)\n",
        "  subfigs = fig.subfigures(nrows=n_layers, ncols=1)\n",
        "  \n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "        subfig.suptitle(f'Layer {layer_no+1}')\n",
        "\n",
        "        axs = subfig.subplots(nrows=1, ncols = n_parts)\n",
        "      # for inner_row, inner_ax in enumerate(axs):\n",
        "        if n_parts == 1:\n",
        "          axs = [axs]\n",
        "        for i, ax in enumerate(axs):\n",
        "          # (27, 8192, 2)\n",
        "          sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "          sub_negatives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "          vicinity_mask = np.logical_and((sub_positives[0] + sub_negatives[0] )<= n_points_vicinity, (sub_positives[0] + sub_negatives[0]) <= 500)\n",
        "          ratio =  sub_positives / sub_negatives\n",
        "          ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "          mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "          intermediate = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "          intermediate += np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "          \n",
        "          if temp_mask != None:\n",
        "            \n",
        "            ax.plot(xi,  intermediate[:,temp_mask[layer_no]])\n",
        "          else:\n",
        "          # print(intermediate[:,:].shape)\n",
        "            # color = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange']\n",
        "            # for c in range(8):\n",
        "                # ax.plot(xi,  intermediate[:,c*int((8192/8)):(c+1)*int(8192/8)],  color = color[c], alpha = .5)\n",
        "\n",
        "            ax.plot(xi,  intermediate[:,:])\n",
        "          ax.set_xticks(xi,x_axis_global)\n",
        "          plt.setp(ax.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "          ax.set_title(f\"# of {posneg_id[0]} pts,{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "        \n",
        "        plt.gca().set_prop_cycle(None)\n",
        "\n",
        "  title = f'epsilon = {epsilon}, n_vicinity_pts = {n_points_vicinity}, Ratio criteria: pos/neg = {pos_neg_ratio_criteria}, neg/pos = {1/pos_neg_ratio_criteria} '\n",
        "  fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_total_posneg_epsilon = {epsilon}.png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        "def plot_hyp_posneg(posneg_pairs_all_runs,n_neurons,  epsilon, temp_mask = None):  \n",
        "  n_parts = 4\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers #np.array(posneg_pairs_all_runs[0]).shape[1]\n",
        "\n",
        "  fig = plt.figure(figsize=(10*n_parts, n_layers*10))\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "  posneg_id = ['positive', 'negative']\n",
        "  unwraped_posneg  = unwrap_routine(posneg_pairs_all_runs)\n",
        "  subfigs = fig.subfigures(nrows=n_layers, ncols=1)\n",
        " \n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "      subfig.suptitle(f'Layer {layer_no+1}')\n",
        "\n",
        "      axs = subfig.subplots(nrows=2, ncols=4)\n",
        "      for inner_row, inner_ax in enumerate(axs):\n",
        "        for i, ax in enumerate(inner_ax):\n",
        "          # (27, 8192, 2)\n",
        "          sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "          sub_negatives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "          vicinity_mask = sub_positives[0] + sub_negatives[0] >= n_points_vicinity\n",
        "          ratio =  sub_positives / sub_negatives\n",
        "          ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "          \n",
        "          mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "          intermediate = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),inner_row]\n",
        "          # ax.plot(xi,  intermediate[:,mask])\n",
        "          # print(intermediate[:,:].shape)\n",
        "          ax.plot(xi,  intermediate[:,:])\n",
        "          ax.set_xticks(xi,x_axis_global)\n",
        "          plt.setp(ax.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "          ax.set_title(f\"# of {posneg_id[inner_row]} pts,{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "        \n",
        "        plt.gca().set_prop_cycle(None)\n",
        "\n",
        "  title = f'epsilon = {epsilon}, n_vicinity_pts = {n_points_vicinity}, Ratio criteria: pos/neg = {pos_neg_ratio_criteria}, neg/pos = {1/pos_neg_ratio_criteria} '\n",
        "  fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_posneg_epsilon = {epsilon}.png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        " \n",
        "  plot_hyp_total_posneg(posneg_pairs_all_runs,n_neurons_list, epsilon, temp_mask)\n",
        "def plot_hyp_posneg_ratio(posneg_pairs_all_runs,n_neurons, epsilon):  \n",
        "  n_parts = 1\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "\n",
        "  fig = plt.figure(figsize=(10*n_parts, n_layers*5))\n",
        "\n",
        "  posneg_id = ['positive', 'negative']\n",
        "  unwraped_posneg  = unwrap_routine(posneg_pairs_all_runs)\n",
        "  i = 0\n",
        "  # create 3x1 subfigs\n",
        "  subfigs = fig.subfigures(nrows=n_layers, ncols=1)\n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "      subfig.suptitle(f'Layer {layer_no+1}')\n",
        "      axs = subfig.subplots(nrows=1, ncols=1)\n",
        "      sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),0]\n",
        "      sub_negatives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts),1]\n",
        "      vicinity_mask = sub_positives[0] + sub_negatives[0] >= n_points_vicinity\n",
        "      # print(vicinity_mask.shape)\n",
        "      ratio =  sub_positives / sub_negatives\n",
        "      ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "      mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "      # axs.plot(xi, np.vstack([ratio[0,mask],ratio[:,mask]]))   #shifting 0 bcoz of log scale\n",
        "      # axs.plot(xi,ratio[:,mask])\n",
        "      axs.plot(xi,ratio[:,:])\n",
        "      axs.set_xticks(xi,x_axis_global)\n",
        "      plt.setp(axs.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "      # axs.set_xscale(\"log\")\n",
        "      # axs.set_yscale(\"symlog\")\n",
        "      axs.set_adjustable(\"datalim\")\n",
        "      axs.set_xlabel(\"epochs\")\n",
        "      axs.set_ylabel(\"#pos / #neg \")\n",
        "      axs.set_title(f\"{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "\n",
        "  title = f'epsilon = {epsilon}, n_vicinity_pts = {n_points_vicinity}, Ratio criteria: pos/neg = {pos_neg_ratio_criteria}, neg/pos = {1/pos_neg_ratio_criteria} '\n",
        "  fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_posneg_ratio_epsilon = {epsilon}.png',format = 'png', bbox_inches='tight', dpi = 100)\n",
        "\n",
        "def plot_hyp_local_usefulness(local_usefulness_all_runs,n_neurons, epsilon):  \n",
        "  n_parts = 1\n",
        "  run = 0\n",
        "  # n_points_vicinity = 10\n",
        "  # pos_neg_ratio_criteria = 5\n",
        "  n_layers = n_hidden_layers\n",
        "  xi = np.arange(0,len(x_axis_global), 1)\n",
        "  fig = plt.figure(figsize=( n_layers*9,7))\n",
        "  unwraped_posneg  = unwrap_routine(local_usefulness_all_runs)\n",
        "  i = 0\n",
        "  subfigs = fig.subfigures(nrows=1, ncols= n_layers)\n",
        "  if type(subfigs) != np.ndarray:\n",
        "    subfigs = [subfigs]\n",
        "  temp_mask = []\n",
        "  for layer_no, subfig in enumerate(subfigs):\n",
        "      subfig.suptitle(f'Layer {layer_no+1}')\n",
        "      axs = subfig.subplots(nrows=1, ncols=1)\n",
        "      sub_positives = np.array(unwraped_posneg[layer_no])[:, int(i*n_neurons[layer_no]/n_parts):int((i+1)*n_neurons[layer_no]/n_parts)]\n",
        "      \n",
        "      \n",
        "      # vicinity_mask = sub_positives[-1, :] >= .4\n",
        "      # temp_mask.append(vicinity_mask)\n",
        "      # sub_positives = sub_positives[:, vicinity_mask]\n",
        "      # ratio =  sub_positives / sub_negatives\n",
        "      # ratio_mask =  np.logical_or(np.logical_or(ratio[0,:] >= pos_neg_ratio_criteria , ratio[0,:] <= (1/pos_neg_ratio_criteria)), np.isnan(ratio[0,:]))\n",
        "      # mask = np.logical_and(ratio_mask, vicinity_mask)\n",
        "      # axs.semilogx([0] + x_axis_global, np.vstack([sub_positives[0,:], sub_positives]))   #shifting 0 bcoz of log scale\n",
        "\n",
        "      # color = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange']\n",
        "      # for c in range(8):\n",
        "      #   axs.plot(xi,  sub_positives[:,c*int((8192/8)):(c+1)*int(8192/8)],  color = color[c], alpha = .5)\n",
        "      axs.plot(xi,  sub_positives)\n",
        "\n",
        "      # axs.set_xscale(\"log\")\n",
        "      # axs.set_yscale(\"log\")\n",
        "      axs.set_xticks(xi,x_axis_global)\n",
        "\n",
        "      axs.set_adjustable(\"datalim\")\n",
        "      plt.setp(axs.get_xticklabels(), rotation=44, horizontalalignment='right', fontsize='small')\n",
        "      axs.set_xlabel(\"epochs\")\n",
        "      axs.set_ylabel(\"local usefulness\")\n",
        "      # axs.set_title(f\"{int(i*n_neurons[layer_no]/n_parts)} to {int((i+1)*n_neurons[layer_no]/n_parts)}\")\n",
        "\n",
        "  title = f'epsilon = {epsilon} '\n",
        "  # fig.suptitle(title, y = 1.01)\n",
        "  fig.savefig(f'hyp_posneg_local_usefulness = {epsilon}.pdf',format = 'pdf', bbox_inches='tight', dpi = 100)\n",
        "\n",
        "  return temp_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBQqrPIPfzOJ"
      },
      "outputs": [],
      "source": [
        "def plot_hyperplanes(weights_biases_all_runs, state_info_5_runs, n_rows, n_cols):\n",
        "  w_idx, b_idx = 0, 1\n",
        "  pseudo_step = -1\n",
        "  first_last = [0,3, -1]\n",
        "  for pseudo_run in range(len(weights_biases_all_runs)):\n",
        "    # for pseudo_epoch in range(len(weights_biases_all_runs[pseudo_run])):\n",
        "    for pseudo_epoch in first_last:\n",
        "      \n",
        "      layer_weights = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][w_idx]\n",
        "      layer_biases = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][b_idx]\n",
        "      \n",
        "      inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "  \n",
        "      fig = plt.figure(figsize = (n_hidden_layers*n_cols*1.5, n_rows*1.5))\n",
        "      outer = gridspec.GridSpec(1, n_hidden_layers, wspace=0.2, hspace=0.1, figure = fig)\n",
        "\n",
        "      # start, stop = -1.2, 1.2\n",
        "      pad = .5\n",
        "      start_x, stop_x = np.min(train_data_curr[:,0])-pad, np.max(train_data_curr[:,0])+pad\n",
        "      start_y, stop_y = np.min(train_data_curr[:,1])-pad, np.max(train_data_curr[:,1])+pad\n",
        "      x_axis = np.linspace(start = start_x, stop = stop_x, num = 2, endpoint=True)\n",
        "\n",
        "      # f_p, axes_p = plt.subplots(1,for_layers, figsize = (8/1.5 * for_layers, 16/1.5)  )\n",
        "      # n_rows, n_cols = 8, 2\n",
        "      for layer_no in range(n_hidden_layers):\n",
        "        inner = gridspec.GridSpecFromSubplotSpec(n_rows, n_cols, subplot_spec=outer[layer_no], wspace=0.1, hspace=0.3)\n",
        "        \n",
        "        ax = plt.Subplot(fig, outer[layer_no])\n",
        "        ax.set_title(\"Layer {}\".format(layer_no+1), fontsize = 18)\n",
        "        ax.axis('off')\n",
        "        fig.add_subplot(ax)\n",
        "        for r in range(n_rows):\n",
        "          for c in range(n_cols):\n",
        "            # ax = plt.Subplot(fig, inner[r*n_cols + c])\n",
        "            ax = fig.add_subplot(inner[r*n_cols + c])\n",
        "           \n",
        "            ax.scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1, alpha = .3)\n",
        "            ax.set_xlim((start_x,stop_x))\n",
        "            # ax.set_ylim((-1.2,4.2))\n",
        "            ax.set_ylim((start_y,stop_y))\n",
        "            layer_slope = -(layer_weights[layer_no][r*n_cols + c,0]/layer_weights[layer_no][r*n_cols + c,1])\n",
        "            layer_intercept = -(layer_biases[layer_no][r*n_cols + c] / layer_weights[layer_no][r*n_cols + c,1])\n",
        "\n",
        "            layer_slope = layer_slope.detach().to('cpu').numpy()\n",
        "            layer_intercept = layer_intercept.detach().to('cpu').numpy()\n",
        "            y_axis = layer_slope*x_axis + layer_intercept\n",
        "\n",
        "            #calc #+ve pts and -ve pts around epsilon dist from the hyperplane\n",
        "            ax.plot( x_axis, y_axis, 'r')\n",
        "            weight_vec = layer_weights[layer_no][r*n_cols + c].detach().to('cpu').numpy()\n",
        "            weight_vec_mag = np.sqrt(np.sum(np.square(weight_vec)))\n",
        "            origin = np.array([[0, 0, 0],[0, 0, 0]])\n",
        "            ax.quiver( np.array([weight_vec[0]]) / weight_vec_mag ,np.array([weight_vec[1]]) / weight_vec_mag, angles='xy', scale_units='xy', scale=1, color =  ['b'], width = .01, headwidth = 12, headlength = 8)\n",
        "            # ax.quiver( 3*np.array([weight_vec[0]]) / weight_vec_mag ,3*np.array([weight_vec[1]]) / weight_vec_mag, angles='xy', scale_units='xy', scale=1, color =  ['b'], width = .01, headwidth = 12, headlength = 8)\n",
        "            ax.set_aspect(True)\n",
        "            # print(layer_weights[i][r*4 + c])\n",
        "            # ax.fill_between(x_axis, y_axis, y_axis +  (weight_vec[1]/weight_vec_mag - layer_intercepts[i][r*4 + c]), alpha = .4)\n",
        "            inter =  (10*weight_vec[1] / weight_vec_mag) - layer_slope* (10*weight_vec[0] / weight_vec_mag)\n",
        "            ax.fill_between(x_axis, y_axis,  layer_slope*x_axis + inter, alpha = .4)\n",
        "            # ax.grid()\n",
        "            if c >= 1:\n",
        "              ax.set_yticks([])\n",
        "            # ax.fill_between(x_axis, y_axis, np.array([0, weight_vec[0]]) / weight_vec_mag, alpha = .2)\n",
        "            # fig.add_subplot(ax)\n",
        "      \n",
        "      run = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"run\"]\n",
        "      epoch = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"epoch\"]\n",
        "      step = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"step\"]\n",
        "      model_protocol_type = state_info_5_runs[0][0][0][\"model_protocol_type\"]\n",
        "      # fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\", fontsize = 8)\n",
        "      fig.savefig('Hyperplanes '+model_protocol_type +', ' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.pdf\",format = 'pdf', bbox_inches='tight', dpi = 100)\n",
        "\n",
        "      plt.cla()\n",
        "      plt.clf()\n",
        "      plt.close('all')\n",
        "\n",
        "\n",
        "def plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs, n_rows, n_cols):\n",
        "  w_idx, b_idx = 0, 1\n",
        "  pseudo_step = -1\n",
        "  first_last = [0,3, -1]\n",
        "  for pseudo_run in range(len(weights_biases_all_runs)):\n",
        "    # for pseudo_epoch in range(len(weights_biases_all_runs[pseudo_run])):\n",
        "    for pseudo_epoch in first_last:\n",
        "      layer_weights = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][w_idx]\n",
        "      layer_biases = weights_biases_all_runs[pseudo_run][pseudo_epoch][pseudo_step][b_idx]\n",
        "      \n",
        "          \n",
        "      inp_plt_x, inp_plt_y  = test_data_curr[:,0], test_data_curr[:,1]\n",
        "      pad = 2\n",
        "      start_x, stop_x = np.min(train_data_curr[:,0])-pad, np.max(train_data_curr[:,0])+pad\n",
        "      start_y, stop_y = np.min(train_data_curr[:,1])-pad, np.max(train_data_curr[:,1])+pad\n",
        "      x_axis = np.linspace(start = start_x, stop = stop_x, num = 2, endpoint=True)\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=n_hidden_layers, figsize=(5*n_hidden_layers,5))\n",
        "\n",
        "      for layer_no in range(n_hidden_layers):\n",
        "        ax[layer_no].scatter(inp_plt_x, inp_plt_y , c = test_labels_curr,marker = '.' ,linewidths = 0.1, alpha = .8)\n",
        "        ax[layer_no].set_title(f\"Layer {layer_no+1}\")\n",
        "        ax[layer_no].grid()\n",
        "        for r in range(n_rows):\n",
        "          for c in range(n_cols):\n",
        "            ax[layer_no].set_xlim((start_x,stop_x))\n",
        "            ax[layer_no].set_ylim((start_y,stop_y))\n",
        "            # print(r*n_cols + c, r*n_cols + c)\n",
        "            layer_slope = -(layer_weights[layer_no][r*n_cols + c,0]/layer_weights[layer_no][r*n_cols + c,1])\n",
        "            layer_intercept = -(layer_biases[layer_no][r*n_cols + c] / layer_weights[layer_no][r*n_cols + c,1])\n",
        "            layer_slope = layer_slope.detach().to('cpu').numpy()\n",
        "            layer_intercept = layer_intercept.detach().to('cpu').numpy()\n",
        "            y_axis = layer_slope*x_axis + layer_intercept\n",
        "            ax[layer_no].plot( x_axis, y_axis, 'r')\n",
        "            # ax[layer_no].grid()\n",
        "\n",
        "           \n",
        "            \n",
        "      # print(r, e, s)\n",
        "        run = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"run\"]\n",
        "        epoch = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"epoch\"]\n",
        "        step = state_info_5_runs[pseudo_run][pseudo_epoch][pseudo_step][\"step\"]\n",
        "        model_protocol_type = state_info_5_runs[0][0][0][\"model_protocol_type\"]\n",
        "        # fig.suptitle(model_protocol_type +', '+ f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\"+', ' + f\"Step = {str(step)}\")\n",
        "        fig.savefig('Hyperplanes_all '+model_protocol_type +', ' +', '+f'Run = {run}' + \", \" + f\"Epoch = {str(epoch)}\" +', ' + f\"Step = {str(step)}.pdf\",format = 'pdf', bbox_inches='tight', dpi = 100)\n",
        "    \n",
        "      plt.cla()\n",
        "      plt.clf()\n",
        "      plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFL83-XRRaLd"
      },
      "outputs": [],
      "source": [
        "# plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs)\n",
        "# plot_hyperplanes(weights_biases_all_runs, state_info_5_runs, n_rows = 8, n_cols = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbqVgGWLnG1X"
      },
      "source": [
        "#Saving and downloading funtions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMcSnqDY2dD4"
      },
      "outputs": [],
      "source": [
        "def save_results(file_name):\n",
        "  os.system(f\"zip -R {file_name}.zip '*.png' '*.pdf'\")\n",
        "  os.system('mkdir All_Protocols')\n",
        "  os.system(f'mv {file_name}.zip /content/All_Protocols/')\n",
        "  os.system(\"rm *\")\n",
        "\n",
        "def download_results():\n",
        "  path = '/content/All_Protocols/'\n",
        "  for f in os.listdir(path):\n",
        "    files.download(f'{path}{f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d7Nbr4dDTe1"
      },
      "outputs": [],
      "source": [
        "# def get_input_idxs(Y_sorted):\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   low_freq_idxs = sorted(rng.choice(np.arange(0, int(len(Y_sorted)/2)),10, replace = False))\n",
        "#   high_freq_idxs = sorted(rng.choice(np.arange(int(len(Y_sorted)/2), len(Y_sorted)),10, replace = False))\n",
        "#   x_idxs = low_freq_idxs + high_freq_idxs\n",
        "#   x_idxs_pair = [(idx, idx) for idx in x_idxs]\n",
        "#   return x_idxs_pair\n",
        "\n",
        "# def get_input_idxs(size):\n",
        "#   rng = np.random.default_rng(0)\n",
        "#   low_freq_idxs = sorted(rng.choice(np.arange(0,int(size/num_modes)*20),10, replace = False))\n",
        "#   high_freq_idxs = sorted(rng.choice(np.arange(20*int(size/num_modes), num_modes*int(size/num_modes)),10, replace = False))\n",
        "#   x_idxs = low_freq_idxs + high_freq_idxs\n",
        "#   x_idxs_pair = [(idx, idx) for idx in x_idxs]\n",
        "#   return x_idxs_pair\n",
        "\n",
        "def get_input_idxs(size):\n",
        "  rng = np.random.default_rng(0)\n",
        "  low_freq_idxs = sorted(rng.choice(np.arange(0,250),50, replace = False))\n",
        "  high_freq_idxs = sorted(rng.choice(np.arange(250, 500),50, replace = False))\n",
        "  x_idxs = low_freq_idxs + high_freq_idxs\n",
        "  x_idxs_pair = [(idx, idx) for idx in x_idxs]\n",
        "  return x_idxs_pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REhPzmLCncJx"
      },
      "source": [
        "#Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzA8qRI1w5j"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"binary_cifar_dataset\"\n",
        "#########NOTE:change start stop axis for hyperplane figs############\n",
        "dataset_func, dataset_args = None, None\n",
        "is_DNN = True\n",
        "is_classification = True\n",
        "if dataset_name == 'parabola_dataset':\n",
        "  dataset_func = parabola_dataset\n",
        "elif dataset_name == 'L1_dataset':\n",
        "  dataset_func = L1_dataset\n",
        "elif dataset_name == 'boxed_dataset':\n",
        "  dataset_func = boxed_dataset\n",
        "elif dataset_name == 'boxed_complex_dataset':\n",
        "  dataset_func = boxed_complex_dataset\n",
        "elif dataset_name == 'binary_mnist_dataset':\n",
        "  dataset_func = binary_mnist_dataset\n",
        "elif dataset_name == 'binary_cifar_dataset':\n",
        "  dataset_func = binary_cifar_dataset\n",
        "  is_DNN = False\n",
        "  dataset_args = {'flattened' : False, 'is_binary' : False}\n",
        "elif dataset_name == 'union_dataset':\n",
        "  dataset_func = union_dataset\n",
        "elif dataset_name == 'circle_dataset':\n",
        "  dataset_func = circle_dataset\n",
        "  is_classification = False\n",
        "  dataset_args = {'n_data' : 500,'a' : 1, 'b' : 9, 'plot' : True }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2lsXg6s2saU",
        "outputId": "8272fb77-4ed0-4947-c706-8ced3103e5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "(50000, 3, 32, 32) (50000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1563"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "seed = 1020 #1010 for all\n",
        "set_seed(seed = seed)\n",
        "\n",
        "\n",
        "train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels = dataset_func(dataset_args)\n",
        "n_data, inp_dim, n_classes = train_data_curr.shape[0], train_data_curr.shape[1], len(np.unique(train_labels_curr))\n",
        "\n",
        "# mini = get_mini_multi_modes_data(test_data_curr,num_modes = 6, n_examples_per_mode = 10)\n",
        "\n",
        "train_dataloader, test_dataloader, train_dl_PWC, test_dl_PWC = get_dataloaders(train_data_curr, test_data_curr, train_labels_curr, test_labels_curr, batch_size = 32)\n",
        "mini_dl = get_mini_multi_mode_dataloaders(mini_test_data, batch_size = len(mini_test_data))\n",
        "x_idxs_pair = get_input_idxs(len(mini_test_data))\n",
        "n_batches = len(train_dataloader)\n",
        "n_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u95Ct6WJoL_e"
      },
      "source": [
        "#CNN model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDdyDeiDkgbl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def compute_pred_gradient(dl_one,label, model,act_type, optimizer):\n",
        "  all_per_sample_gradients = []\n",
        "\n",
        "  for s in range(dl_one.size()[0]):\n",
        "    \n",
        "    x, y = dl_one[s], label[s]\n",
        "    X, y = x.to(device), y.to(device)\n",
        "    \n",
        "    X = torch.unsqueeze(X, axis = 0)\n",
        "    \n",
        "    pred = model(X.float())\n",
        "    class_grad = []\n",
        "    \n",
        "    # optimizer.zero_grad()\n",
        "    for i in range(pred.size()[-1]):\n",
        "      \n",
        "      pred[:,i].backward(retain_graph=True)\n",
        "      para = []\n",
        "      for p in model.parameters():\n",
        "        if p.grad != None:\n",
        "          para.append(p.grad.flatten().detach().clone())\n",
        "\n",
        "      # for p in para:\n",
        "      #   print(p.size())\n",
        "      \n",
        "      class_grad.append(torch.hstack(para))\n",
        "    \n",
        "    per_sample_gradients = torch.vstack(class_grad)\n",
        "\n",
        "      \n",
        "  \n",
        "    all_per_sample_gradients.append(per_sample_gradients)\n",
        "    optimizer.zero_grad()\n",
        "  all_per_sample_gradients = torch.stack(all_per_sample_gradients)\n",
        "  \n",
        "  return all_per_sample_gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wxnn4oYAJ66"
      },
      "outputs": [],
      "source": [
        "class Linear_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.out_dim = 10\n",
        "        for i in range(self.out_dim):\n",
        "          self.layers.append(nn.Linear(511362, 1))\n",
        "          # self.layers.append(nn.Linear(28354, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = []\n",
        "      # print(x.size())\n",
        "      \n",
        "      for i in range(self.out_dim):\n",
        "        \n",
        "        out.append( self.layers[i](x[:,i,:]))\n",
        "\n",
        "      return torch.hstack(out)\n",
        "linear_classifier = Linear_Classifier()\n",
        "linear_classifier.to(device)\n",
        "\n",
        "linear_criterion = nn.CrossEntropyLoss()\n",
        "linear_optimizer = optim.Adam(linear_classifier.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOjn1meMPZzd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# train_acc, test_acc = [], []\n",
        "# for epoch in range(1000):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "        \n",
        "#         data_grad = compute_pred_gradient(dl_one = data[0],label = data[1], model = net,act_type = None, optimizer = optimizer)\n",
        "       \n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         # inputs, labels = data\n",
        "#         inputs, labels = data_grad, data[1].to(device)\n",
        "#         # zero the parameter gradients\n",
        "#         linear_optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = linear_classifier(inputs)\n",
        "        \n",
        "            \n",
        "        \n",
        "#         loss = linear_criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         linear_optimizer.step()\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "        \n",
        "#         if i+1 % 100 == 0:    # print every 2000 mini-batches\n",
        "#           print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "#           running_loss = 0.0\n",
        "#     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} ')\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "  \n",
        "# #     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "# #     with torch.no_grad():\n",
        "# #         for data in test_dataloader:\n",
        "# #             images, labels = data\n",
        "# #             # calculate outputs by running images through the network\n",
        "# #             images, labels = images.to(device), labels.to(device)\n",
        "# #             outputs = linear_classifier(images)\n",
        "# #             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "# #             # debug()\n",
        "# #             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "# #             total += labels.size(0)\n",
        "# #             correct += (predicted == labels).sum().item()\n",
        "# #     test_acc.append(100 * correct / total)\n",
        "# #     print(f'Accuracy of the network on the {total} test images: {100 * correct / total} ')\n",
        "# #     correct = 0\n",
        "# #     total = 0\n",
        "    \n",
        "# #     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "# #     with torch.no_grad():\n",
        "# #         for data in train_dataloader:\n",
        "# #             images, labels = data\n",
        "# #             # calculate outputs by running images through the network\n",
        "# #             images, labels = images.to(device), labels.to(device)\n",
        "# #             outputs = linear_classifier(images)\n",
        "# #             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "# #             # debug()\n",
        "# #             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "# #             total += labels.size(0)\n",
        "# #             correct += (predicted == labels).sum().item()\n",
        "# #     train_acc.append(100 * correct / total)\n",
        "# #     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} %')\n",
        "    \n",
        "# # print('Finished Training')\n",
        "# # print(max(train_acc), max(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7q3TVstj3qV"
      },
      "outputs": [],
      "source": [
        "# torch.save({\n",
        "#             'modelA_state_dict': linear_classifier.state_dict(),\n",
        "#             'modelB_state_dict': net.state_dict(),\n",
        "#             'optimizerA_state_dict': linear_optimizer.state_dict(),\n",
        "#             'optimizerB_state_dict': optimizer.state_dict(),\n",
        "#             'running_loss' : running_loss,\n",
        "#             'acc' : 100 * correct / total\n",
        "#             }, 'NTK_regression.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvN6leCHPACx"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load('NTK_regression.tar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwhTWkwBCJs5"
      },
      "outputs": [],
      "source": [
        "# linear_classifier.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "# # linear_optimizer.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "# net.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizerB_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRlFr1Rqs21F"
      },
      "outputs": [],
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.pool = nn.AvgPool2d(32, 32)\n",
        "#         self.n_filters = n_filters\n",
        "#         self.conv1 = nn.Conv2d(3, self.n_filters, 5, padding = 'same')\n",
        "#         self.conv2 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "#         self.conv3 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "#         self.conv4 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "#         self.conv5 = nn.Conv2d(self.n_filters, self.n_filters, 5, padding = 'same')\n",
        "#         # self.conv1 = nn.Linear(784, 32)\n",
        "#         # self.conv2 =  nn.Linear(32, 32)\n",
        "#         # self.conv3 =  nn.Linear(32, 32)\n",
        "#         # self.conv4 =  nn.Linear(32, 32)\n",
        "#         self.fc1 = nn.Linear(n_filters, 64)\n",
        "#         # self.fc2 = nn.Linear(64, 2)\n",
        "#         self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "#         # self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         # self.pool = nn.MaxPool2d(2, 2)\n",
        "#         # self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         # self.fc1 = nn.Linear(16 * 5 * 5, 32)\n",
        "#         # self.fc2 = nn.Linear(120, 84)\n",
        "#         # self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x = self.pool(F.relu(self.conv1(x)))\n",
        "#         # x = self.pool(F.relu(self.conv2(x)))\n",
        "#         # x = self.pool(F.relu(self.conv3(x)))\n",
        "#         # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         # x = F.relu(self.fc1(x))\n",
        "#         # # x = F.relu(self.fc2(x))\n",
        "#         # x = self.fc3(x)\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = F.relu(self.conv5(x))\n",
        "#         x = self.pool(x)\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         # x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "\n",
        "#         # x = self.pool(self.conv1(x))\n",
        "#         # x = self.pool(self.conv2(x))\n",
        "#         # x = self.pool(self.conv3(x))\n",
        "#         # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         # x = self.fc1(x)\n",
        "#         # # x = F.relu(self.fc2(x))\n",
        "#         # x = self.fc3(x)\n",
        "\n",
        "#         # x = self.conv1(x)\n",
        "#         # x = self.conv2(x)\n",
        "#         # x = self.conv3(x)\n",
        "#         # x = self.pool(x)\n",
        "#         # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         # x = self.fc1(x)\n",
        "#         # # x = self.fc2(x)\n",
        "#         # x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# net = Net()\n",
        "# net.to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.0009)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gylTOEYu_wn",
        "outputId": "f40f3303-ab6a-44e0-a0d8-8f0aa4724e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 19 08:31:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    27W /  70W |    632MiB / 15109MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# train_acc, test_acc = [], []\n",
        "# for epoch in range(1000):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "               \n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         # inputs, labels = data\n",
        "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs)\n",
        "        \n",
        "            \n",
        "        \n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "        \n",
        "#         if i+1 % 100 == 0:    # print every 2000 mini-batches\n",
        "#           print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "#           running_loss = 0.0\n",
        "#     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} ')\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "  \n",
        "#     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "#     with torch.no_grad():\n",
        "#         for data in test_dataloader:\n",
        "#             images, labels = data[0], data[1]\n",
        "#             # calculate outputs by running images through the network\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = net(images)\n",
        "#             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "#             # debug()\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#     test_acc.append(100 * correct / total)\n",
        "#     print(f'Accuracy of the network on the {total} test images: {100 * correct / total} ')\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "    \n",
        "# #     # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "# #     with torch.no_grad():\n",
        "# #         for data in train_dataloader:\n",
        "# #             images, labels = data\n",
        "# #             # calculate outputs by running images through the network\n",
        "# #             images, labels = images.to(device), labels.to(device)\n",
        "# #             outputs = linear_classifier(images)\n",
        "# #             # the class with the highest energy is what we choose as prediction\n",
        "            \n",
        "# #             # debug()\n",
        "# #             _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "# #             total += labels.size(0)\n",
        "# #             correct += (predicted == labels).sum().item()\n",
        "# #     train_acc.append(100 * correct / total)\n",
        "# #     print(f'Accuracy of the network on the {total} train images: {100 * correct / total} %')\n",
        "    \n",
        "# # print('Finished Training')\n",
        "# # print(max(train_acc), max(test_acc))\n"
      ],
      "metadata": {
        "id": "4roFN9PahFXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XMjOfpnusU_",
        "outputId": "f830bfc2-909f-4d0f-efe6-d49f6c7e58f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 19 08:31:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    27W /  70W |    632MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHVRuUJEnp0c"
      },
      "source": [
        "#Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XvUBVBUvob2Y",
        "outputId": "5b9b1bde-f0ae-462c-8780-b42f816e8f55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMLP, DLGN, DLGN_ONPV, DLGN_BOTH_ONPV\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "'''\n",
        "1. lr = 5e-3 for boxed and circle data set\n",
        "2. build_kernels(...): \n",
        "    a. Now returns only overlap matrix\n",
        "    b. Normalized overlap matrix by its trace\n",
        "3. plot_kernels(...): function changed to plot kernels at only critical point \n",
        "\n",
        "5. run_npf_npv(...): changed jump in 100 to 200 epochs to 10 \n",
        "6. To save memory while running CIFAR 10\n",
        "    a. plot_hypposneg_(..) returns None\n",
        "    b. routine(..) funtion does not store in container\n",
        "7. # x = self.gate(act_type, x, i+n_cnn_layers, gating_mask) has been commented for DLGN-SF to work\n",
        "8. changing Adam to SGD for npf_npv\n",
        "\n",
        "def plot_posneg_hyperplanes(model, for_layers, epsilon, model_protocol_type = 'DLGN',run = 1,  epoch = 0, step = 0, for_mode = 1):\n",
        "  # layer_weights, layer_biases, _, _ = get_hyperplanes_params(model,model_protocol_type, for_layers, for_mode)\n",
        "  return [None, None]#[layer_weights, layer_biases]\n",
        "#---Hyper params------\n",
        "\n",
        "\n",
        "Cirle Dataset:\n",
        "lr = 5e-3\n",
        "batch_size = 32\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 16\n",
        "\n",
        "Boxed Dataset:\n",
        "lr = 5e-4\n",
        "batch_size = 32\n",
        "n_hidden_layers = 5\n",
        "n_neurons = 16\n",
        "\n",
        "\n",
        "\n",
        "CIFAR-10\n",
        "lr = 2e-4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "MLP, DLGN, DLGN_ONPV, DLGN_BOTH_ONPV\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "gBzeygoDVWJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b94a7a-e8fe-4791-d052-864ab33049e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 19 08:31:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    27W /  70W |    632MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEzL4SKCVV99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r *"
      ],
      "metadata": {
        "id": "m3mzlqMBSMBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuS_Dxsje6fC"
      },
      "outputs": [],
      "source": [
        "state_collections = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JsX46dmH3Utm",
        "outputId": "c7dae185-aad0-4b2b-f628-e12b01f7ae00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "End of Epoch 186Train : \n",
            " Accuracy = 84.7, Loss =  0.422033\n",
            "End of Epoch 186Test : \n",
            " Accuracy = 71.3, Loss =  0.936012\n",
            "Loss over all Training data : 0.442743 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "End of Epoch 187Train : \n",
            " Accuracy = 84.8, Loss =  0.421768\n",
            "End of Epoch 187Test : \n",
            " Accuracy = 71.3, Loss =  0.936012\n",
            "Loss over all Training data : 0.417287 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "End of Epoch 188Train : \n",
            " Accuracy = 84.4, Loss =  0.429515\n",
            "End of Epoch 188Test : \n",
            " Accuracy = 71.3, Loss =  0.936012\n",
            "Loss over all Training data : 0.386171 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "End of Epoch 189Train : \n",
            " Accuracy = 85.0, Loss =  0.414079\n",
            "End of Epoch 189Test : \n",
            " Accuracy = 71.3, Loss =  0.936012\n",
            "Loss over all Training data : 0.442635 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "End of Epoch 190Train : \n",
            " Accuracy = 84.7, Loss =  0.423523\n",
            "Loss over all Test data : 0.906722 \n",
            "\n",
            "Loss over all Training data : 0.410307 \n",
            "\n",
            "End of Epoch 00190Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.410401 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "End of Epoch 191Train : \n",
            " Accuracy = 85.2, Loss =  0.411194\n",
            "End of Epoch 191Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.417733 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "End of Epoch 192Train : \n",
            " Accuracy = 84.5, Loss =  0.433510\n",
            "End of Epoch 192Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.426184 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "End of Epoch 193Train : \n",
            " Accuracy = 84.8, Loss =  0.419629\n",
            "End of Epoch 193Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.417539 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "End of Epoch 194Train : \n",
            " Accuracy = 84.2, Loss =  0.436185\n",
            "End of Epoch 194Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.409805 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "End of Epoch 195Train : \n",
            " Accuracy = 84.2, Loss =  0.435740\n",
            "End of Epoch 195Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.473779 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "End of Epoch 196Train : \n",
            " Accuracy = 84.2, Loss =  0.434879\n",
            "End of Epoch 196Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.391123 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "End of Epoch 197Train : \n",
            " Accuracy = 84.9, Loss =  0.418191\n",
            "End of Epoch 197Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.401484 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "End of Epoch 198Train : \n",
            " Accuracy = 84.6, Loss =  0.427347\n",
            "End of Epoch 198Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.409367 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "End of Epoch 199Train : \n",
            " Accuracy = 83.5, Loss =  0.451735\n",
            "End of Epoch 199Test : \n",
            " Accuracy = 72.3, Loss =  0.906722\n",
            "Loss over all Training data : 0.397208 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "End of Epoch 200Train : \n",
            " Accuracy = 84.9, Loss =  0.414947\n",
            "Loss over all Test data : 0.920603 \n",
            "\n",
            "Loss over all Training data : 0.370658 \n",
            "\n",
            "End of Epoch 00200Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.370746 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "End of Epoch 201Train : \n",
            " Accuracy = 85.0, Loss =  0.413548\n",
            "End of Epoch 201Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.387514 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "End of Epoch 202Train : \n",
            " Accuracy = 85.2, Loss =  0.405980\n",
            "End of Epoch 202Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.373299 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "End of Epoch 203Train : \n",
            " Accuracy = 85.0, Loss =  0.415176\n",
            "End of Epoch 203Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.376346 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "End of Epoch 204Train : \n",
            " Accuracy = 85.2, Loss =  0.408896\n",
            "End of Epoch 204Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.377792 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "End of Epoch 205Train : \n",
            " Accuracy = 85.4, Loss =  0.405785\n",
            "End of Epoch 205Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.379113 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "End of Epoch 206Train : \n",
            " Accuracy = 85.7, Loss =  0.396464\n",
            "End of Epoch 206Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.383125 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "End of Epoch 207Train : \n",
            " Accuracy = 84.3, Loss =  0.433194\n",
            "End of Epoch 207Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.410769 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "End of Epoch 208Train : \n",
            " Accuracy = 84.5, Loss =  0.425376\n",
            "End of Epoch 208Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.434351 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "End of Epoch 209Train : \n",
            " Accuracy = 83.9, Loss =  0.438496\n",
            "End of Epoch 209Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.407513 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "End of Epoch 210Train : \n",
            " Accuracy = 84.5, Loss =  0.430901\n",
            "End of Epoch 210Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.426640 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "End of Epoch 211Train : \n",
            " Accuracy = 84.6, Loss =  0.424496\n",
            "End of Epoch 211Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.403570 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "End of Epoch 212Train : \n",
            " Accuracy = 85.1, Loss =  0.410058\n",
            "End of Epoch 212Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.371733 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "End of Epoch 213Train : \n",
            " Accuracy = 84.7, Loss =  0.418244\n",
            "End of Epoch 213Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.401700 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "End of Epoch 214Train : \n",
            " Accuracy = 84.4, Loss =  0.433149\n",
            "End of Epoch 214Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.395960 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "End of Epoch 215Train : \n",
            " Accuracy = 85.0, Loss =  0.412667\n",
            "End of Epoch 215Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.416561 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "End of Epoch 216Train : \n",
            " Accuracy = 84.6, Loss =  0.426144\n",
            "End of Epoch 216Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.379748 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "End of Epoch 217Train : \n",
            " Accuracy = 85.2, Loss =  0.406698\n",
            "End of Epoch 217Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.400808 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "End of Epoch 218Train : \n",
            " Accuracy = 84.3, Loss =  0.434514\n",
            "End of Epoch 218Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.452654 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "End of Epoch 219Train : \n",
            " Accuracy = 85.2, Loss =  0.412396\n",
            "End of Epoch 219Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.347132 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "End of Epoch 220Train : \n",
            " Accuracy = 85.3, Loss =  0.411576\n",
            "End of Epoch 220Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.386549 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "End of Epoch 221Train : \n",
            " Accuracy = 85.3, Loss =  0.404200\n",
            "End of Epoch 221Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.391499 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "End of Epoch 222Train : \n",
            " Accuracy = 85.0, Loss =  0.417361\n",
            "End of Epoch 222Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.421735 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "End of Epoch 223Train : \n",
            " Accuracy = 84.6, Loss =  0.424182\n",
            "End of Epoch 223Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.422284 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "End of Epoch 224Train : \n",
            " Accuracy = 84.0, Loss =  0.442354\n",
            "End of Epoch 224Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.413858 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "End of Epoch 225Train : \n",
            " Accuracy = 82.9, Loss =  0.479016\n",
            "End of Epoch 225Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.498567 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "End of Epoch 226Train : \n",
            " Accuracy = 82.5, Loss =  0.482169\n",
            "End of Epoch 226Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.435041 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "End of Epoch 227Train : \n",
            " Accuracy = 84.2, Loss =  0.433157\n",
            "End of Epoch 227Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.409605 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "End of Epoch 228Train : \n",
            " Accuracy = 84.6, Loss =  0.426881\n",
            "End of Epoch 228Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.375288 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "End of Epoch 229Train : \n",
            " Accuracy = 83.7, Loss =  0.447790\n",
            "End of Epoch 229Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.417084 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "End of Epoch 230Train : \n",
            " Accuracy = 84.0, Loss =  0.440491\n",
            "End of Epoch 230Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.408354 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "End of Epoch 231Train : \n",
            " Accuracy = 84.5, Loss =  0.425934\n",
            "End of Epoch 231Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.386000 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "End of Epoch 232Train : \n",
            " Accuracy = 81.9, Loss =  0.501950\n",
            "End of Epoch 232Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.364275 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "End of Epoch 233Train : \n",
            " Accuracy = 84.1, Loss =  0.438261\n",
            "End of Epoch 233Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.431138 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "End of Epoch 234Train : \n",
            " Accuracy = 83.4, Loss =  0.456675\n",
            "End of Epoch 234Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.443502 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "End of Epoch 235Train : \n",
            " Accuracy = 84.1, Loss =  0.442959\n",
            "End of Epoch 235Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.380810 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "End of Epoch 236Train : \n",
            " Accuracy = 84.9, Loss =  0.418278\n",
            "End of Epoch 236Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.421033 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "End of Epoch 237Train : \n",
            " Accuracy = 83.9, Loss =  0.444693\n",
            "End of Epoch 237Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.429395 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "End of Epoch 238Train : \n",
            " Accuracy = 83.7, Loss =  0.446166\n",
            "End of Epoch 238Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.430154 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "End of Epoch 239Train : \n",
            " Accuracy = 83.8, Loss =  0.444335\n",
            "End of Epoch 239Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.435574 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "End of Epoch 240Train : \n",
            " Accuracy = 83.6, Loss =  0.457895\n",
            "End of Epoch 240Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.428119 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "End of Epoch 241Train : \n",
            " Accuracy = 84.4, Loss =  0.428801\n",
            "End of Epoch 241Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.392833 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "End of Epoch 242Train : \n",
            " Accuracy = 83.8, Loss =  0.446065\n",
            "End of Epoch 242Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.408795 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "End of Epoch 243Train : \n",
            " Accuracy = 84.5, Loss =  0.428618\n",
            "End of Epoch 243Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.400037 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "End of Epoch 244Train : \n",
            " Accuracy = 83.7, Loss =  0.444417\n",
            "End of Epoch 244Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.430547 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "End of Epoch 245Train : \n",
            " Accuracy = 83.6, Loss =  0.454219\n",
            "End of Epoch 245Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.407514 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "End of Epoch 246Train : \n",
            " Accuracy = 83.2, Loss =  0.464883\n",
            "End of Epoch 246Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.442725 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "End of Epoch 247Train : \n",
            " Accuracy = 83.1, Loss =  0.461354\n",
            "End of Epoch 247Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.397127 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "End of Epoch 248Train : \n",
            " Accuracy = 84.3, Loss =  0.434059\n",
            "End of Epoch 248Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.410258 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "End of Epoch 249Train : \n",
            " Accuracy = 84.1, Loss =  0.436008\n",
            "End of Epoch 249Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.392439 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "End of Epoch 250Train : \n",
            " Accuracy = 85.0, Loss =  0.416841\n",
            "End of Epoch 250Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.434106 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "End of Epoch 251Train : \n",
            " Accuracy = 84.7, Loss =  0.422887\n",
            "End of Epoch 251Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.387162 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "End of Epoch 252Train : \n",
            " Accuracy = 83.9, Loss =  0.447599\n",
            "End of Epoch 252Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.396983 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "End of Epoch 253Train : \n",
            " Accuracy = 83.0, Loss =  0.470191\n",
            "End of Epoch 253Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.421718 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "End of Epoch 254Train : \n",
            " Accuracy = 83.4, Loss =  0.460496\n",
            "End of Epoch 254Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.414513 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "End of Epoch 255Train : \n",
            " Accuracy = 83.8, Loss =  0.445062\n",
            "End of Epoch 255Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.404141 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "End of Epoch 256Train : \n",
            " Accuracy = 84.2, Loss =  0.432339\n",
            "End of Epoch 256Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.403150 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "End of Epoch 257Train : \n",
            " Accuracy = 84.2, Loss =  0.434104\n",
            "End of Epoch 257Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.441354 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "End of Epoch 258Train : \n",
            " Accuracy = 84.2, Loss =  0.437316\n",
            "End of Epoch 258Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.380691 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "End of Epoch 259Train : \n",
            " Accuracy = 84.4, Loss =  0.429098\n",
            "End of Epoch 259Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.413063 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "End of Epoch 260Train : \n",
            " Accuracy = 84.0, Loss =  0.444832\n",
            "End of Epoch 260Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.454340 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "End of Epoch 261Train : \n",
            " Accuracy = 84.0, Loss =  0.440120\n",
            "End of Epoch 261Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.410316 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "End of Epoch 262Train : \n",
            " Accuracy = 83.3, Loss =  0.459910\n",
            "End of Epoch 262Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.410440 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "End of Epoch 263Train : \n",
            " Accuracy = 82.3, Loss =  0.484548\n",
            "End of Epoch 263Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.459904 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "End of Epoch 264Train : \n",
            " Accuracy = 83.3, Loss =  0.464524\n",
            "End of Epoch 264Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.426783 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "End of Epoch 265Train : \n",
            " Accuracy = 83.9, Loss =  0.444362\n",
            "End of Epoch 265Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.415898 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "End of Epoch 266Train : \n",
            " Accuracy = 84.7, Loss =  0.424060\n",
            "End of Epoch 266Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.402366 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "End of Epoch 267Train : \n",
            " Accuracy = 84.4, Loss =  0.426307\n",
            "End of Epoch 267Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.391168 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "End of Epoch 268Train : \n",
            " Accuracy = 84.3, Loss =  0.433001\n",
            "End of Epoch 268Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.375483 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "End of Epoch 269Train : \n",
            " Accuracy = 83.2, Loss =  0.467597\n",
            "End of Epoch 269Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.419260 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "End of Epoch 270Train : \n",
            " Accuracy = 83.9, Loss =  0.439552\n",
            "End of Epoch 270Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.409587 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "End of Epoch 271Train : \n",
            " Accuracy = 84.3, Loss =  0.436254\n",
            "End of Epoch 271Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.391582 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "End of Epoch 272Train : \n",
            " Accuracy = 83.8, Loss =  0.450416\n",
            "End of Epoch 272Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.468008 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "End of Epoch 273Train : \n",
            " Accuracy = 83.1, Loss =  0.463219\n",
            "End of Epoch 273Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.402440 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "End of Epoch 274Train : \n",
            " Accuracy = 83.7, Loss =  0.451344\n",
            "End of Epoch 274Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.454439 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "End of Epoch 275Train : \n",
            " Accuracy = 84.0, Loss =  0.440177\n",
            "End of Epoch 275Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.440773 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "End of Epoch 276Train : \n",
            " Accuracy = 84.7, Loss =  0.420573\n",
            "End of Epoch 276Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.429265 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "End of Epoch 277Train : \n",
            " Accuracy = 84.6, Loss =  0.423975\n",
            "End of Epoch 277Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.407387 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "End of Epoch 278Train : \n",
            " Accuracy = 65.6, Loss =  0.963933\n",
            "End of Epoch 278Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.751756 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "End of Epoch 279Train : \n",
            " Accuracy = 79.9, Loss =  0.556283\n",
            "End of Epoch 279Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.441641 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "End of Epoch 280Train : \n",
            " Accuracy = 83.3, Loss =  0.464210\n",
            "End of Epoch 280Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.425542 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "End of Epoch 281Train : \n",
            " Accuracy = 84.0, Loss =  0.441571\n",
            "End of Epoch 281Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.401200 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "End of Epoch 282Train : \n",
            " Accuracy = 83.5, Loss =  0.459380\n",
            "End of Epoch 282Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.410313 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "End of Epoch 283Train : \n",
            " Accuracy = 84.0, Loss =  0.441310\n",
            "End of Epoch 283Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.439774 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "End of Epoch 284Train : \n",
            " Accuracy = 84.5, Loss =  0.428128\n",
            "End of Epoch 284Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.438365 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "End of Epoch 285Train : \n",
            " Accuracy = 82.7, Loss =  0.477851\n",
            "End of Epoch 285Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.406957 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "End of Epoch 286Train : \n",
            " Accuracy = 82.9, Loss =  0.474593\n",
            "End of Epoch 286Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.502762 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "End of Epoch 287Train : \n",
            " Accuracy = 82.4, Loss =  0.479650\n",
            "End of Epoch 287Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.445182 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "End of Epoch 288Train : \n",
            " Accuracy = 74.4, Loss =  0.716809\n",
            "End of Epoch 288Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.674276 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "End of Epoch 289Train : \n",
            " Accuracy = 81.2, Loss =  0.523729\n",
            "End of Epoch 289Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.522771 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "End of Epoch 290Train : \n",
            " Accuracy = 83.8, Loss =  0.448025\n",
            "End of Epoch 290Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.427966 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "End of Epoch 291Train : \n",
            " Accuracy = 83.5, Loss =  0.462212\n",
            "End of Epoch 291Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.443890 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "End of Epoch 292Train : \n",
            " Accuracy = 83.6, Loss =  0.452930\n",
            "End of Epoch 292Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.396133 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "End of Epoch 293Train : \n",
            " Accuracy = 84.4, Loss =  0.430653\n",
            "End of Epoch 293Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.412099 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "End of Epoch 294Train : \n",
            " Accuracy = 83.2, Loss =  0.464403\n",
            "End of Epoch 294Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.418493 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "End of Epoch 295Train : \n",
            " Accuracy = 84.6, Loss =  0.425548\n",
            "End of Epoch 295Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.369866 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "End of Epoch 296Train : \n",
            " Accuracy = 83.8, Loss =  0.444866\n",
            "End of Epoch 296Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.491561 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "End of Epoch 297Train : \n",
            " Accuracy = 84.6, Loss =  0.422964\n",
            "End of Epoch 297Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.370274 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "End of Epoch 298Train : \n",
            " Accuracy = 83.5, Loss =  0.452865\n",
            "End of Epoch 298Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.409513 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "End of Epoch 299Train : \n",
            " Accuracy = 83.3, Loss =  0.459462\n",
            "End of Epoch 299Test : \n",
            " Accuracy = 72.9, Loss =  0.920603\n",
            "Loss over all Training data : 0.438081 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "End of Epoch 300Train : \n",
            " Accuracy = 83.7, Loss =  0.454823\n",
            "Loss over all Test data : 0.900399 \n",
            "\n",
            "Loss over all Training data : 0.407010 \n",
            "\n",
            "End of Epoch 00300Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.407219 \n",
            "\n",
            "Epoch 301\n",
            "-------------------------------\n",
            "End of Epoch 301Train : \n",
            " Accuracy = 84.2, Loss =  0.435188\n",
            "End of Epoch 301Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.387605 \n",
            "\n",
            "Epoch 302\n",
            "-------------------------------\n",
            "End of Epoch 302Train : \n",
            " Accuracy = 85.5, Loss =  0.403660\n",
            "End of Epoch 302Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.379031 \n",
            "\n",
            "Epoch 303\n",
            "-------------------------------\n",
            "End of Epoch 303Train : \n",
            " Accuracy = 85.0, Loss =  0.412965\n",
            "End of Epoch 303Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.444868 \n",
            "\n",
            "Epoch 304\n",
            "-------------------------------\n",
            "End of Epoch 304Train : \n",
            " Accuracy = 84.5, Loss =  0.424635\n",
            "End of Epoch 304Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.386144 \n",
            "\n",
            "Epoch 305\n",
            "-------------------------------\n",
            "End of Epoch 305Train : \n",
            " Accuracy = 84.7, Loss =  0.417086\n",
            "End of Epoch 305Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.380953 \n",
            "\n",
            "Epoch 306\n",
            "-------------------------------\n",
            "End of Epoch 306Train : \n",
            " Accuracy = 83.5, Loss =  0.454812\n",
            "End of Epoch 306Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.414382 \n",
            "\n",
            "Epoch 307\n",
            "-------------------------------\n",
            "End of Epoch 307Train : \n",
            " Accuracy = 84.7, Loss =  0.424583\n",
            "End of Epoch 307Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.426448 \n",
            "\n",
            "Epoch 308\n",
            "-------------------------------\n",
            "End of Epoch 308Train : \n",
            " Accuracy = 84.2, Loss =  0.440365\n",
            "End of Epoch 308Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.414368 \n",
            "\n",
            "Epoch 309\n",
            "-------------------------------\n",
            "End of Epoch 309Train : \n",
            " Accuracy = 84.4, Loss =  0.434414\n",
            "End of Epoch 309Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.435873 \n",
            "\n",
            "Epoch 310\n",
            "-------------------------------\n",
            "End of Epoch 310Train : \n",
            " Accuracy = 83.2, Loss =  0.459425\n",
            "End of Epoch 310Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.429295 \n",
            "\n",
            "Epoch 311\n",
            "-------------------------------\n",
            "End of Epoch 311Train : \n",
            " Accuracy = 84.6, Loss =  0.426642\n",
            "End of Epoch 311Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.381546 \n",
            "\n",
            "Epoch 312\n",
            "-------------------------------\n",
            "End of Epoch 312Train : \n",
            " Accuracy = 84.5, Loss =  0.431595\n",
            "End of Epoch 312Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.392576 \n",
            "\n",
            "Epoch 313\n",
            "-------------------------------\n",
            "End of Epoch 313Train : \n",
            " Accuracy = 85.2, Loss =  0.407300\n",
            "End of Epoch 313Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.364274 \n",
            "\n",
            "Epoch 314\n",
            "-------------------------------\n",
            "End of Epoch 314Train : \n",
            " Accuracy = 85.4, Loss =  0.401890\n",
            "End of Epoch 314Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.344464 \n",
            "\n",
            "Epoch 315\n",
            "-------------------------------\n",
            "End of Epoch 315Train : \n",
            " Accuracy = 83.7, Loss =  0.453353\n",
            "End of Epoch 315Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.419417 \n",
            "\n",
            "Epoch 316\n",
            "-------------------------------\n",
            "End of Epoch 316Train : \n",
            " Accuracy = 83.9, Loss =  0.446439\n",
            "End of Epoch 316Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.382190 \n",
            "\n",
            "Epoch 317\n",
            "-------------------------------\n",
            "End of Epoch 317Train : \n",
            " Accuracy = 85.0, Loss =  0.411092\n",
            "End of Epoch 317Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.385045 \n",
            "\n",
            "Epoch 318\n",
            "-------------------------------\n",
            "End of Epoch 318Train : \n",
            " Accuracy = 84.6, Loss =  0.425690\n",
            "End of Epoch 318Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.380840 \n",
            "\n",
            "Epoch 319\n",
            "-------------------------------\n",
            "End of Epoch 319Train : \n",
            " Accuracy = 84.9, Loss =  0.417814\n",
            "End of Epoch 319Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.397457 \n",
            "\n",
            "Epoch 320\n",
            "-------------------------------\n",
            "End of Epoch 320Train : \n",
            " Accuracy = 84.8, Loss =  0.418662\n",
            "End of Epoch 320Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.378334 \n",
            "\n",
            "Epoch 321\n",
            "-------------------------------\n",
            "End of Epoch 321Train : \n",
            " Accuracy = 84.5, Loss =  0.426884\n",
            "End of Epoch 321Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.407786 \n",
            "\n",
            "Epoch 322\n",
            "-------------------------------\n",
            "End of Epoch 322Train : \n",
            " Accuracy = 84.7, Loss =  0.422334\n",
            "End of Epoch 322Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.382030 \n",
            "\n",
            "Epoch 323\n",
            "-------------------------------\n",
            "End of Epoch 323Train : \n",
            " Accuracy = 84.3, Loss =  0.428710\n",
            "End of Epoch 323Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.379962 \n",
            "\n",
            "Epoch 324\n",
            "-------------------------------\n",
            "End of Epoch 324Train : \n",
            " Accuracy = 85.4, Loss =  0.404917\n",
            "End of Epoch 324Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.386847 \n",
            "\n",
            "Epoch 325\n",
            "-------------------------------\n",
            "End of Epoch 325Train : \n",
            " Accuracy = 85.0, Loss =  0.410951\n",
            "End of Epoch 325Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.356108 \n",
            "\n",
            "Epoch 326\n",
            "-------------------------------\n",
            "End of Epoch 326Train : \n",
            " Accuracy = 86.0, Loss =  0.387825\n",
            "End of Epoch 326Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.379858 \n",
            "\n",
            "Epoch 327\n",
            "-------------------------------\n",
            "End of Epoch 327Train : \n",
            " Accuracy = 82.9, Loss =  0.469989\n",
            "End of Epoch 327Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.398083 \n",
            "\n",
            "Epoch 328\n",
            "-------------------------------\n",
            "End of Epoch 328Train : \n",
            " Accuracy = 83.8, Loss =  0.443608\n",
            "End of Epoch 328Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.405197 \n",
            "\n",
            "Epoch 329\n",
            "-------------------------------\n",
            "End of Epoch 329Train : \n",
            " Accuracy = 85.2, Loss =  0.409609\n",
            "End of Epoch 329Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.359939 \n",
            "\n",
            "Epoch 330\n",
            "-------------------------------\n",
            "End of Epoch 330Train : \n",
            " Accuracy = 85.9, Loss =  0.391143\n",
            "End of Epoch 330Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.396652 \n",
            "\n",
            "Epoch 331\n",
            "-------------------------------\n",
            "End of Epoch 331Train : \n",
            " Accuracy = 85.4, Loss =  0.403215\n",
            "End of Epoch 331Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.382997 \n",
            "\n",
            "Epoch 332\n",
            "-------------------------------\n",
            "End of Epoch 332Train : \n",
            " Accuracy = 83.7, Loss =  0.444606\n",
            "End of Epoch 332Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.444859 \n",
            "\n",
            "Epoch 333\n",
            "-------------------------------\n",
            "End of Epoch 333Train : \n",
            " Accuracy = 83.2, Loss =  0.466604\n",
            "End of Epoch 333Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.378399 \n",
            "\n",
            "Epoch 334\n",
            "-------------------------------\n",
            "End of Epoch 334Train : \n",
            " Accuracy = 85.2, Loss =  0.408189\n",
            "End of Epoch 334Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.382739 \n",
            "\n",
            "Epoch 335\n",
            "-------------------------------\n",
            "End of Epoch 335Train : \n",
            " Accuracy = 85.1, Loss =  0.410468\n",
            "End of Epoch 335Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.376967 \n",
            "\n",
            "Epoch 336\n",
            "-------------------------------\n",
            "End of Epoch 336Train : \n",
            " Accuracy = 84.8, Loss =  0.417503\n",
            "End of Epoch 336Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.397477 \n",
            "\n",
            "Epoch 337\n",
            "-------------------------------\n",
            "End of Epoch 337Train : \n",
            " Accuracy = 85.7, Loss =  0.392816\n",
            "End of Epoch 337Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.381799 \n",
            "\n",
            "Epoch 338\n",
            "-------------------------------\n",
            "End of Epoch 338Train : \n",
            " Accuracy = 85.4, Loss =  0.403624\n",
            "End of Epoch 338Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.368580 \n",
            "\n",
            "Epoch 339\n",
            "-------------------------------\n",
            "End of Epoch 339Train : \n",
            " Accuracy = 85.6, Loss =  0.397889\n",
            "End of Epoch 339Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.392975 \n",
            "\n",
            "Epoch 340\n",
            "-------------------------------\n",
            "End of Epoch 340Train : \n",
            " Accuracy = 83.5, Loss =  0.451631\n",
            "End of Epoch 340Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.430284 \n",
            "\n",
            "Epoch 341\n",
            "-------------------------------\n",
            "End of Epoch 341Train : \n",
            " Accuracy = 84.6, Loss =  0.423553\n",
            "End of Epoch 341Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.391533 \n",
            "\n",
            "Epoch 342\n",
            "-------------------------------\n",
            "End of Epoch 342Train : \n",
            " Accuracy = 85.0, Loss =  0.414022\n",
            "End of Epoch 342Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.411088 \n",
            "\n",
            "Epoch 343\n",
            "-------------------------------\n",
            "End of Epoch 343Train : \n",
            " Accuracy = 85.0, Loss =  0.412015\n",
            "End of Epoch 343Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.413559 \n",
            "\n",
            "Epoch 344\n",
            "-------------------------------\n",
            "End of Epoch 344Train : \n",
            " Accuracy = 83.4, Loss =  0.456400\n",
            "End of Epoch 344Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.457887 \n",
            "\n",
            "Epoch 345\n",
            "-------------------------------\n",
            "End of Epoch 345Train : \n",
            " Accuracy = 83.5, Loss =  0.456657\n",
            "End of Epoch 345Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.413899 \n",
            "\n",
            "Epoch 346\n",
            "-------------------------------\n",
            "End of Epoch 346Train : \n",
            " Accuracy = 83.1, Loss =  0.465935\n",
            "End of Epoch 346Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.424979 \n",
            "\n",
            "Epoch 347\n",
            "-------------------------------\n",
            "End of Epoch 347Train : \n",
            " Accuracy = 83.4, Loss =  0.456752\n",
            "End of Epoch 347Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.467717 \n",
            "\n",
            "Epoch 348\n",
            "-------------------------------\n",
            "End of Epoch 348Train : \n",
            " Accuracy = 82.5, Loss =  0.481946\n",
            "End of Epoch 348Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.424224 \n",
            "\n",
            "Epoch 349\n",
            "-------------------------------\n",
            "End of Epoch 349Train : \n",
            " Accuracy = 84.0, Loss =  0.439492\n",
            "End of Epoch 349Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.409731 \n",
            "\n",
            "Epoch 350\n",
            "-------------------------------\n",
            "End of Epoch 350Train : \n",
            " Accuracy = 84.6, Loss =  0.417935\n",
            "End of Epoch 350Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.383620 \n",
            "\n",
            "Epoch 351\n",
            "-------------------------------\n",
            "End of Epoch 351Train : \n",
            " Accuracy = 83.4, Loss =  0.455749\n",
            "End of Epoch 351Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.437441 \n",
            "\n",
            "Epoch 352\n",
            "-------------------------------\n",
            "End of Epoch 352Train : \n",
            " Accuracy = 84.2, Loss =  0.438532\n",
            "End of Epoch 352Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.433211 \n",
            "\n",
            "Epoch 353\n",
            "-------------------------------\n",
            "End of Epoch 353Train : \n",
            " Accuracy = 85.0, Loss =  0.412756\n",
            "End of Epoch 353Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.391050 \n",
            "\n",
            "Epoch 354\n",
            "-------------------------------\n",
            "End of Epoch 354Train : \n",
            " Accuracy = 85.1, Loss =  0.408417\n",
            "End of Epoch 354Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.357438 \n",
            "\n",
            "Epoch 355\n",
            "-------------------------------\n",
            "End of Epoch 355Train : \n",
            " Accuracy = 85.0, Loss =  0.409853\n",
            "End of Epoch 355Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.366743 \n",
            "\n",
            "Epoch 356\n",
            "-------------------------------\n",
            "End of Epoch 356Train : \n",
            " Accuracy = 84.2, Loss =  0.437003\n",
            "End of Epoch 356Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.456274 \n",
            "\n",
            "Epoch 357\n",
            "-------------------------------\n",
            "End of Epoch 357Train : \n",
            " Accuracy = 84.5, Loss =  0.428406\n",
            "End of Epoch 357Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.404701 \n",
            "\n",
            "Epoch 358\n",
            "-------------------------------\n",
            "End of Epoch 358Train : \n",
            " Accuracy = 85.2, Loss =  0.406395\n",
            "End of Epoch 358Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.354938 \n",
            "\n",
            "Epoch 359\n",
            "-------------------------------\n",
            "End of Epoch 359Train : \n",
            " Accuracy = 85.7, Loss =  0.392236\n",
            "End of Epoch 359Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.375742 \n",
            "\n",
            "Epoch 360\n",
            "-------------------------------\n",
            "End of Epoch 360Train : \n",
            " Accuracy = 84.1, Loss =  0.438243\n",
            "End of Epoch 360Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.489901 \n",
            "\n",
            "Epoch 361\n",
            "-------------------------------\n",
            "End of Epoch 361Train : \n",
            " Accuracy = 82.0, Loss =  0.493840\n",
            "End of Epoch 361Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.445730 \n",
            "\n",
            "Epoch 362\n",
            "-------------------------------\n",
            "End of Epoch 362Train : \n",
            " Accuracy = 83.5, Loss =  0.449336\n",
            "End of Epoch 362Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.428383 \n",
            "\n",
            "Epoch 363\n",
            "-------------------------------\n",
            "End of Epoch 363Train : \n",
            " Accuracy = 83.5, Loss =  0.456472\n",
            "End of Epoch 363Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.432690 \n",
            "\n",
            "Epoch 364\n",
            "-------------------------------\n",
            "End of Epoch 364Train : \n",
            " Accuracy = 83.3, Loss =  0.457858\n",
            "End of Epoch 364Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.443771 \n",
            "\n",
            "Epoch 365\n",
            "-------------------------------\n",
            "End of Epoch 365Train : \n",
            " Accuracy = 83.6, Loss =  0.452768\n",
            "End of Epoch 365Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.457583 \n",
            "\n",
            "Epoch 366\n",
            "-------------------------------\n",
            "End of Epoch 366Train : \n",
            " Accuracy = 81.6, Loss =  0.504717\n",
            "End of Epoch 366Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.465004 \n",
            "\n",
            "Epoch 367\n",
            "-------------------------------\n",
            "End of Epoch 367Train : \n",
            " Accuracy = 82.9, Loss =  0.470480\n",
            "End of Epoch 367Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.421800 \n",
            "\n",
            "Epoch 368\n",
            "-------------------------------\n",
            "End of Epoch 368Train : \n",
            " Accuracy = 82.9, Loss =  0.474727\n",
            "End of Epoch 368Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.435165 \n",
            "\n",
            "Epoch 369\n",
            "-------------------------------\n",
            "End of Epoch 369Train : \n",
            " Accuracy = 82.6, Loss =  0.476955\n",
            "End of Epoch 369Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.469234 \n",
            "\n",
            "Epoch 370\n",
            "-------------------------------\n",
            "End of Epoch 370Train : \n",
            " Accuracy = 82.3, Loss =  0.487701\n",
            "End of Epoch 370Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.435223 \n",
            "\n",
            "Epoch 371\n",
            "-------------------------------\n",
            "End of Epoch 371Train : \n",
            " Accuracy = 83.7, Loss =  0.452222\n",
            "End of Epoch 371Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.418247 \n",
            "\n",
            "Epoch 372\n",
            "-------------------------------\n",
            "End of Epoch 372Train : \n",
            " Accuracy = 84.0, Loss =  0.442506\n",
            "End of Epoch 372Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.421359 \n",
            "\n",
            "Epoch 373\n",
            "-------------------------------\n",
            "End of Epoch 373Train : \n",
            " Accuracy = 84.3, Loss =  0.431535\n",
            "End of Epoch 373Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.376091 \n",
            "\n",
            "Epoch 374\n",
            "-------------------------------\n",
            "End of Epoch 374Train : \n",
            " Accuracy = 84.1, Loss =  0.439552\n",
            "End of Epoch 374Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.428274 \n",
            "\n",
            "Epoch 375\n",
            "-------------------------------\n",
            "End of Epoch 375Train : \n",
            " Accuracy = 83.5, Loss =  0.456084\n",
            "End of Epoch 375Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.491279 \n",
            "\n",
            "Epoch 376\n",
            "-------------------------------\n",
            "End of Epoch 376Train : \n",
            " Accuracy = 82.2, Loss =  0.489605\n",
            "End of Epoch 376Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.499006 \n",
            "\n",
            "Epoch 377\n",
            "-------------------------------\n",
            "End of Epoch 377Train : \n",
            " Accuracy = 83.1, Loss =  0.468296\n",
            "End of Epoch 377Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.422152 \n",
            "\n",
            "Epoch 378\n",
            "-------------------------------\n",
            "End of Epoch 378Train : \n",
            " Accuracy = 81.9, Loss =  0.497591\n",
            "End of Epoch 378Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.622399 \n",
            "\n",
            "Epoch 379\n",
            "-------------------------------\n",
            "End of Epoch 379Train : \n",
            " Accuracy = 80.7, Loss =  0.532856\n",
            "End of Epoch 379Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.487482 \n",
            "\n",
            "Epoch 380\n",
            "-------------------------------\n",
            "End of Epoch 380Train : \n",
            " Accuracy = 82.3, Loss =  0.488208\n",
            "End of Epoch 380Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.537051 \n",
            "\n",
            "Epoch 381\n",
            "-------------------------------\n",
            "End of Epoch 381Train : \n",
            " Accuracy = 61.0, Loss =  1.081721\n",
            "End of Epoch 381Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.964088 \n",
            "\n",
            "Epoch 382\n",
            "-------------------------------\n",
            "End of Epoch 382Train : \n",
            " Accuracy = 72.1, Loss =  0.797445\n",
            "End of Epoch 382Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.606074 \n",
            "\n",
            "Epoch 383\n",
            "-------------------------------\n",
            "End of Epoch 383Train : \n",
            " Accuracy = 78.7, Loss =  0.596691\n",
            "End of Epoch 383Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.524737 \n",
            "\n",
            "Epoch 384\n",
            "-------------------------------\n",
            "End of Epoch 384Train : \n",
            " Accuracy = 81.8, Loss =  0.507901\n",
            "End of Epoch 384Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.528932 \n",
            "\n",
            "Epoch 385\n",
            "-------------------------------\n",
            "End of Epoch 385Train : \n",
            " Accuracy = 80.9, Loss =  0.527769\n",
            "End of Epoch 385Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.503695 \n",
            "\n",
            "Epoch 386\n",
            "-------------------------------\n",
            "End of Epoch 386Train : \n",
            " Accuracy = 82.0, Loss =  0.497972\n",
            "End of Epoch 386Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.463182 \n",
            "\n",
            "Epoch 387\n",
            "-------------------------------\n",
            "End of Epoch 387Train : \n",
            " Accuracy = 82.6, Loss =  0.479547\n",
            "End of Epoch 387Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.468457 \n",
            "\n",
            "Epoch 388\n",
            "-------------------------------\n",
            "End of Epoch 388Train : \n",
            " Accuracy = 83.1, Loss =  0.464275\n",
            "End of Epoch 388Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.429268 \n",
            "\n",
            "Epoch 389\n",
            "-------------------------------\n",
            "End of Epoch 389Train : \n",
            " Accuracy = 83.1, Loss =  0.464078\n",
            "End of Epoch 389Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.431472 \n",
            "\n",
            "Epoch 390\n",
            "-------------------------------\n",
            "End of Epoch 390Train : \n",
            " Accuracy = 82.7, Loss =  0.475903\n",
            "End of Epoch 390Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.434717 \n",
            "\n",
            "Epoch 391\n",
            "-------------------------------\n",
            "End of Epoch 391Train : \n",
            " Accuracy = 82.1, Loss =  0.495777\n",
            "End of Epoch 391Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.436048 \n",
            "\n",
            "Epoch 392\n",
            "-------------------------------\n",
            "End of Epoch 392Train : \n",
            " Accuracy = 82.8, Loss =  0.475839\n",
            "End of Epoch 392Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.453032 \n",
            "\n",
            "Epoch 393\n",
            "-------------------------------\n",
            "End of Epoch 393Train : \n",
            " Accuracy = 81.9, Loss =  0.495875\n",
            "End of Epoch 393Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.447311 \n",
            "\n",
            "Epoch 394\n",
            "-------------------------------\n",
            "End of Epoch 394Train : \n",
            " Accuracy = 82.9, Loss =  0.478066\n",
            "End of Epoch 394Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.431732 \n",
            "\n",
            "Epoch 395\n",
            "-------------------------------\n",
            "End of Epoch 395Train : \n",
            " Accuracy = 82.2, Loss =  0.493689\n",
            "End of Epoch 395Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.528741 \n",
            "\n",
            "Epoch 396\n",
            "-------------------------------\n",
            "End of Epoch 396Train : \n",
            " Accuracy = 81.9, Loss =  0.499569\n",
            "End of Epoch 396Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.422194 \n",
            "\n",
            "Epoch 397\n",
            "-------------------------------\n",
            "End of Epoch 397Train : \n",
            " Accuracy = 84.1, Loss =  0.441865\n",
            "End of Epoch 397Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.394927 \n",
            "\n",
            "Epoch 398\n",
            "-------------------------------\n",
            "End of Epoch 398Train : \n",
            " Accuracy = 83.7, Loss =  0.447523\n",
            "End of Epoch 398Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.451514 \n",
            "\n",
            "Epoch 399\n",
            "-------------------------------\n",
            "End of Epoch 399Train : \n",
            " Accuracy = 82.8, Loss =  0.469525\n",
            "End of Epoch 399Test : \n",
            " Accuracy = 72.1, Loss =  0.900399\n",
            "Loss over all Training data : 0.449731 \n",
            "\n",
            "Epoch 400\n",
            "-------------------------------\n",
            "End of Epoch 400Train : \n",
            " Accuracy = 82.5, Loss =  0.477874\n",
            "Loss over all Test data : 0.911366 \n",
            "\n",
            "Loss over all Training data : 0.516535 \n",
            "\n",
            "End of Epoch 00400Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.516622 \n",
            "\n",
            "Epoch 401\n",
            "-------------------------------\n",
            "End of Epoch 401Train : \n",
            " Accuracy = 81.8, Loss =  0.501685\n",
            "End of Epoch 401Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.465841 \n",
            "\n",
            "Epoch 402\n",
            "-------------------------------\n",
            "End of Epoch 402Train : \n",
            " Accuracy = 80.2, Loss =  0.548630\n",
            "End of Epoch 402Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.455201 \n",
            "\n",
            "Epoch 403\n",
            "-------------------------------\n",
            "End of Epoch 403Train : \n",
            " Accuracy = 81.5, Loss =  0.512964\n",
            "End of Epoch 403Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.474319 \n",
            "\n",
            "Epoch 404\n",
            "-------------------------------\n",
            "End of Epoch 404Train : \n",
            " Accuracy = 82.8, Loss =  0.477867\n",
            "End of Epoch 404Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.459562 \n",
            "\n",
            "Epoch 405\n",
            "-------------------------------\n",
            "End of Epoch 405Train : \n",
            " Accuracy = 81.5, Loss =  0.512042\n",
            "End of Epoch 405Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.438535 \n",
            "\n",
            "Epoch 406\n",
            "-------------------------------\n",
            "End of Epoch 406Train : \n",
            " Accuracy = 82.9, Loss =  0.471049\n",
            "End of Epoch 406Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.460291 \n",
            "\n",
            "Epoch 407\n",
            "-------------------------------\n",
            "End of Epoch 407Train : \n",
            " Accuracy = 82.4, Loss =  0.486349\n",
            "End of Epoch 407Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.427394 \n",
            "\n",
            "Epoch 408\n",
            "-------------------------------\n",
            "End of Epoch 408Train : \n",
            " Accuracy = 84.1, Loss =  0.446135\n",
            "End of Epoch 408Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.409201 \n",
            "\n",
            "Epoch 409\n",
            "-------------------------------\n",
            "End of Epoch 409Train : \n",
            " Accuracy = 83.6, Loss =  0.451570\n",
            "End of Epoch 409Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.398448 \n",
            "\n",
            "Epoch 410\n",
            "-------------------------------\n",
            "End of Epoch 410Train : \n",
            " Accuracy = 83.9, Loss =  0.444506\n",
            "End of Epoch 410Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.428162 \n",
            "\n",
            "Epoch 411\n",
            "-------------------------------\n",
            "End of Epoch 411Train : \n",
            " Accuracy = 84.5, Loss =  0.432771\n",
            "End of Epoch 411Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.427948 \n",
            "\n",
            "Epoch 412\n",
            "-------------------------------\n",
            "End of Epoch 412Train : \n",
            " Accuracy = 84.2, Loss =  0.436253\n",
            "End of Epoch 412Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.437988 \n",
            "\n",
            "Epoch 413\n",
            "-------------------------------\n",
            "End of Epoch 413Train : \n",
            " Accuracy = 80.8, Loss =  0.533339\n",
            "End of Epoch 413Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.587464 \n",
            "\n",
            "Epoch 414\n",
            "-------------------------------\n",
            "End of Epoch 414Train : \n",
            " Accuracy = 79.1, Loss =  0.580637\n",
            "End of Epoch 414Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.514530 \n",
            "\n",
            "Epoch 415\n",
            "-------------------------------\n",
            "End of Epoch 415Train : \n",
            " Accuracy = 80.9, Loss =  0.524524\n",
            "End of Epoch 415Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.488759 \n",
            "\n",
            "Epoch 416\n",
            "-------------------------------\n",
            "End of Epoch 416Train : \n",
            " Accuracy = 79.7, Loss =  0.560288\n",
            "End of Epoch 416Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.467428 \n",
            "\n",
            "Epoch 417\n",
            "-------------------------------\n",
            "End of Epoch 417Train : \n",
            " Accuracy = 80.8, Loss =  0.532538\n",
            "End of Epoch 417Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.492591 \n",
            "\n",
            "Epoch 418\n",
            "-------------------------------\n",
            "End of Epoch 418Train : \n",
            " Accuracy = 80.8, Loss =  0.533847\n",
            "End of Epoch 418Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.438318 \n",
            "\n",
            "Epoch 419\n",
            "-------------------------------\n",
            "End of Epoch 419Train : \n",
            " Accuracy = 81.3, Loss =  0.517747\n",
            "End of Epoch 419Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.560785 \n",
            "\n",
            "Epoch 420\n",
            "-------------------------------\n",
            "End of Epoch 420Train : \n",
            " Accuracy = 80.2, Loss =  0.550411\n",
            "End of Epoch 420Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.471757 \n",
            "\n",
            "Epoch 421\n",
            "-------------------------------\n",
            "End of Epoch 421Train : \n",
            " Accuracy = 80.6, Loss =  0.540191\n",
            "End of Epoch 421Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.530336 \n",
            "\n",
            "Epoch 422\n",
            "-------------------------------\n",
            "End of Epoch 422Train : \n",
            " Accuracy = 81.3, Loss =  0.516497\n",
            "End of Epoch 422Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.453781 \n",
            "\n",
            "Epoch 423\n",
            "-------------------------------\n",
            "End of Epoch 423Train : \n",
            " Accuracy = 81.7, Loss =  0.513118\n",
            "End of Epoch 423Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.477964 \n",
            "\n",
            "Epoch 424\n",
            "-------------------------------\n",
            "End of Epoch 424Train : \n",
            " Accuracy = 78.8, Loss =  0.586980\n",
            "End of Epoch 424Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.903828 \n",
            "\n",
            "Epoch 425\n",
            "-------------------------------\n",
            "End of Epoch 425Train : \n",
            " Accuracy = 66.1, Loss =  0.951335\n",
            "End of Epoch 425Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.947584 \n",
            "\n",
            "Epoch 426\n",
            "-------------------------------\n",
            "End of Epoch 426Train : \n",
            " Accuracy = 69.2, Loss =  0.869605\n",
            "End of Epoch 426Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.750877 \n",
            "\n",
            "Epoch 427\n",
            "-------------------------------\n",
            "End of Epoch 427Train : \n",
            " Accuracy = 76.2, Loss =  0.677420\n",
            "End of Epoch 427Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.575073 \n",
            "\n",
            "Epoch 428\n",
            "-------------------------------\n",
            "End of Epoch 428Train : \n",
            " Accuracy = 80.0, Loss =  0.557634\n",
            "End of Epoch 428Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.479498 \n",
            "\n",
            "Epoch 429\n",
            "-------------------------------\n",
            "End of Epoch 429Train : \n",
            " Accuracy = 80.4, Loss =  0.548301\n",
            "End of Epoch 429Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.529583 \n",
            "\n",
            "Epoch 430\n",
            "-------------------------------\n",
            "End of Epoch 430Train : \n",
            " Accuracy = 78.6, Loss =  0.598502\n",
            "End of Epoch 430Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.535175 \n",
            "\n",
            "Epoch 431\n",
            "-------------------------------\n",
            "End of Epoch 431Train : \n",
            " Accuracy = 79.7, Loss =  0.573975\n",
            "End of Epoch 431Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.537115 \n",
            "\n",
            "Epoch 432\n",
            "-------------------------------\n",
            "End of Epoch 432Train : \n",
            " Accuracy = 79.6, Loss =  0.573432\n",
            "End of Epoch 432Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.555769 \n",
            "\n",
            "Epoch 433\n",
            "-------------------------------\n",
            "End of Epoch 433Train : \n",
            " Accuracy = 79.8, Loss =  0.568169\n",
            "End of Epoch 433Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.538998 \n",
            "\n",
            "Epoch 434\n",
            "-------------------------------\n",
            "End of Epoch 434Train : \n",
            " Accuracy = 81.0, Loss =  0.529798\n",
            "End of Epoch 434Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.519747 \n",
            "\n",
            "Epoch 435\n",
            "-------------------------------\n",
            "End of Epoch 435Train : \n",
            " Accuracy = 81.9, Loss =  0.500876\n",
            "End of Epoch 435Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.458303 \n",
            "\n",
            "Epoch 436\n",
            "-------------------------------\n",
            "End of Epoch 436Train : \n",
            " Accuracy = 82.6, Loss =  0.483605\n",
            "End of Epoch 436Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.475722 \n",
            "\n",
            "Epoch 437\n",
            "-------------------------------\n",
            "End of Epoch 437Train : \n",
            " Accuracy = 82.0, Loss =  0.501303\n",
            "End of Epoch 437Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.440968 \n",
            "\n",
            "Epoch 438\n",
            "-------------------------------\n",
            "End of Epoch 438Train : \n",
            " Accuracy = 82.1, Loss =  0.500718\n",
            "End of Epoch 438Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.522814 \n",
            "\n",
            "Epoch 439\n",
            "-------------------------------\n",
            "End of Epoch 439Train : \n",
            " Accuracy = 80.7, Loss =  0.537994\n",
            "End of Epoch 439Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.510568 \n",
            "\n",
            "Epoch 440\n",
            "-------------------------------\n",
            "End of Epoch 440Train : \n",
            " Accuracy = 82.7, Loss =  0.487860\n",
            "End of Epoch 440Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.459768 \n",
            "\n",
            "Epoch 441\n",
            "-------------------------------\n",
            "End of Epoch 441Train : \n",
            " Accuracy = 81.9, Loss =  0.504659\n",
            "End of Epoch 441Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.453662 \n",
            "\n",
            "Epoch 442\n",
            "-------------------------------\n",
            "End of Epoch 442Train : \n",
            " Accuracy = 81.6, Loss =  0.508866\n",
            "End of Epoch 442Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.455477 \n",
            "\n",
            "Epoch 443\n",
            "-------------------------------\n",
            "End of Epoch 443Train : \n",
            " Accuracy = 80.9, Loss =  0.530581\n",
            "End of Epoch 443Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.519160 \n",
            "\n",
            "Epoch 444\n",
            "-------------------------------\n",
            "End of Epoch 444Train : \n",
            " Accuracy = 79.9, Loss =  0.560307\n",
            "End of Epoch 444Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.496347 \n",
            "\n",
            "Epoch 445\n",
            "-------------------------------\n",
            "End of Epoch 445Train : \n",
            " Accuracy = 83.0, Loss =  0.475188\n",
            "End of Epoch 445Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.430029 \n",
            "\n",
            "Epoch 446\n",
            "-------------------------------\n",
            "End of Epoch 446Train : \n",
            " Accuracy = 83.3, Loss =  0.464646\n",
            "End of Epoch 446Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.446392 \n",
            "\n",
            "Epoch 447\n",
            "-------------------------------\n",
            "End of Epoch 447Train : \n",
            " Accuracy = 83.4, Loss =  0.463645\n",
            "End of Epoch 447Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.482685 \n",
            "\n",
            "Epoch 448\n",
            "-------------------------------\n",
            "End of Epoch 448Train : \n",
            " Accuracy = 83.4, Loss =  0.457420\n",
            "End of Epoch 448Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.440949 \n",
            "\n",
            "Epoch 449\n",
            "-------------------------------\n",
            "End of Epoch 449Train : \n",
            " Accuracy = 82.4, Loss =  0.490598\n",
            "End of Epoch 449Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.490961 \n",
            "\n",
            "Epoch 450\n",
            "-------------------------------\n",
            "End of Epoch 450Train : \n",
            " Accuracy = 82.5, Loss =  0.484766\n",
            "End of Epoch 450Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.484029 \n",
            "\n",
            "Epoch 451\n",
            "-------------------------------\n",
            "End of Epoch 451Train : \n",
            " Accuracy = 82.7, Loss =  0.476166\n",
            "End of Epoch 451Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.446365 \n",
            "\n",
            "Epoch 452\n",
            "-------------------------------\n",
            "End of Epoch 452Train : \n",
            " Accuracy = 82.3, Loss =  0.489356\n",
            "End of Epoch 452Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.457497 \n",
            "\n",
            "Epoch 453\n",
            "-------------------------------\n",
            "End of Epoch 453Train : \n",
            " Accuracy = 81.2, Loss =  0.523523\n",
            "End of Epoch 453Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.516953 \n",
            "\n",
            "Epoch 454\n",
            "-------------------------------\n",
            "End of Epoch 454Train : \n",
            " Accuracy = 82.3, Loss =  0.490065\n",
            "End of Epoch 454Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.454100 \n",
            "\n",
            "Epoch 455\n",
            "-------------------------------\n",
            "End of Epoch 455Train : \n",
            " Accuracy = 81.5, Loss =  0.514742\n",
            "End of Epoch 455Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.461002 \n",
            "\n",
            "Epoch 456\n",
            "-------------------------------\n",
            "End of Epoch 456Train : \n",
            " Accuracy = 83.3, Loss =  0.462561\n",
            "End of Epoch 456Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.415453 \n",
            "\n",
            "Epoch 457\n",
            "-------------------------------\n",
            "End of Epoch 457Train : \n",
            " Accuracy = 82.8, Loss =  0.474446\n",
            "End of Epoch 457Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.515553 \n",
            "\n",
            "Epoch 458\n",
            "-------------------------------\n",
            "End of Epoch 458Train : \n",
            " Accuracy = 83.5, Loss =  0.454486\n",
            "End of Epoch 458Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.460764 \n",
            "\n",
            "Epoch 459\n",
            "-------------------------------\n",
            "End of Epoch 459Train : \n",
            " Accuracy = 82.7, Loss =  0.477718\n",
            "End of Epoch 459Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.444142 \n",
            "\n",
            "Epoch 460\n",
            "-------------------------------\n",
            "End of Epoch 460Train : \n",
            " Accuracy = 82.4, Loss =  0.486050\n",
            "End of Epoch 460Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.458005 \n",
            "\n",
            "Epoch 461\n",
            "-------------------------------\n",
            "End of Epoch 461Train : \n",
            " Accuracy = 83.3, Loss =  0.460412\n",
            "End of Epoch 461Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.454893 \n",
            "\n",
            "Epoch 462\n",
            "-------------------------------\n",
            "End of Epoch 462Train : \n",
            " Accuracy = 82.9, Loss =  0.477220\n",
            "End of Epoch 462Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.659249 \n",
            "\n",
            "Epoch 463\n",
            "-------------------------------\n",
            "End of Epoch 463Train : \n",
            " Accuracy = 82.0, Loss =  0.503434\n",
            "End of Epoch 463Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.427370 \n",
            "\n",
            "Epoch 464\n",
            "-------------------------------\n",
            "End of Epoch 464Train : \n",
            " Accuracy = 83.3, Loss =  0.459407\n",
            "End of Epoch 464Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.461674 \n",
            "\n",
            "Epoch 465\n",
            "-------------------------------\n",
            "End of Epoch 465Train : \n",
            " Accuracy = 81.9, Loss =  0.505294\n",
            "End of Epoch 465Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.534769 \n",
            "\n",
            "Epoch 466\n",
            "-------------------------------\n",
            "End of Epoch 466Train : \n",
            " Accuracy = 78.6, Loss =  0.601087\n",
            "End of Epoch 466Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.553723 \n",
            "\n",
            "Epoch 467\n",
            "-------------------------------\n",
            "End of Epoch 467Train : \n",
            " Accuracy = 80.0, Loss =  0.553436\n",
            "End of Epoch 467Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.591442 \n",
            "\n",
            "Epoch 468\n",
            "-------------------------------\n",
            "End of Epoch 468Train : \n",
            " Accuracy = 79.7, Loss =  0.562296\n",
            "End of Epoch 468Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.459898 \n",
            "\n",
            "Epoch 469\n",
            "-------------------------------\n",
            "End of Epoch 469Train : \n",
            " Accuracy = 82.4, Loss =  0.490506\n",
            "End of Epoch 469Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.442369 \n",
            "\n",
            "Epoch 470\n",
            "-------------------------------\n",
            "End of Epoch 470Train : \n",
            " Accuracy = 82.6, Loss =  0.482781\n",
            "End of Epoch 470Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.518856 \n",
            "\n",
            "Epoch 471\n",
            "-------------------------------\n",
            "End of Epoch 471Train : \n",
            " Accuracy = 80.1, Loss =  0.554333\n",
            "End of Epoch 471Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.483099 \n",
            "\n",
            "Epoch 472\n",
            "-------------------------------\n",
            "End of Epoch 472Train : \n",
            " Accuracy = 81.0, Loss =  0.527542\n",
            "End of Epoch 472Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.603408 \n",
            "\n",
            "Epoch 473\n",
            "-------------------------------\n",
            "End of Epoch 473Train : \n",
            " Accuracy = 80.2, Loss =  0.554779\n",
            "End of Epoch 473Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.571161 \n",
            "\n",
            "Epoch 474\n",
            "-------------------------------\n",
            "End of Epoch 474Train : \n",
            " Accuracy = 82.5, Loss =  0.488030\n",
            "End of Epoch 474Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.412181 \n",
            "\n",
            "Epoch 475\n",
            "-------------------------------\n",
            "End of Epoch 475Train : \n",
            " Accuracy = 82.8, Loss =  0.479000\n",
            "End of Epoch 475Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.474274 \n",
            "\n",
            "Epoch 476\n",
            "-------------------------------\n",
            "End of Epoch 476Train : \n",
            " Accuracy = 81.0, Loss =  0.529477\n",
            "End of Epoch 476Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.513375 \n",
            "\n",
            "Epoch 477\n",
            "-------------------------------\n",
            "End of Epoch 477Train : \n",
            " Accuracy = 81.7, Loss =  0.512159\n",
            "End of Epoch 477Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.522411 \n",
            "\n",
            "Epoch 478\n",
            "-------------------------------\n",
            "End of Epoch 478Train : \n",
            " Accuracy = 81.4, Loss =  0.518242\n",
            "End of Epoch 478Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.534575 \n",
            "\n",
            "Epoch 479\n",
            "-------------------------------\n",
            "End of Epoch 479Train : \n",
            " Accuracy = 81.7, Loss =  0.506262\n",
            "End of Epoch 479Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.464538 \n",
            "\n",
            "Epoch 480\n",
            "-------------------------------\n",
            "End of Epoch 480Train : \n",
            " Accuracy = 81.5, Loss =  0.510618\n",
            "End of Epoch 480Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.495955 \n",
            "\n",
            "Epoch 481\n",
            "-------------------------------\n",
            "End of Epoch 481Train : \n",
            " Accuracy = 81.6, Loss =  0.508362\n",
            "End of Epoch 481Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.478204 \n",
            "\n",
            "Epoch 482\n",
            "-------------------------------\n",
            "End of Epoch 482Train : \n",
            " Accuracy = 81.3, Loss =  0.519808\n",
            "End of Epoch 482Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.497421 \n",
            "\n",
            "Epoch 483\n",
            "-------------------------------\n",
            "End of Epoch 483Train : \n",
            " Accuracy = 81.5, Loss =  0.511616\n",
            "End of Epoch 483Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.507046 \n",
            "\n",
            "Epoch 484\n",
            "-------------------------------\n",
            "End of Epoch 484Train : \n",
            " Accuracy = 83.7, Loss =  0.457893\n",
            "End of Epoch 484Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.456364 \n",
            "\n",
            "Epoch 485\n",
            "-------------------------------\n",
            "End of Epoch 485Train : \n",
            " Accuracy = 82.8, Loss =  0.476963\n",
            "End of Epoch 485Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.450101 \n",
            "\n",
            "Epoch 486\n",
            "-------------------------------\n",
            "End of Epoch 486Train : \n",
            " Accuracy = 83.7, Loss =  0.454660\n",
            "End of Epoch 486Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.436725 \n",
            "\n",
            "Epoch 487\n",
            "-------------------------------\n",
            "End of Epoch 487Train : \n",
            " Accuracy = 83.6, Loss =  0.456614\n",
            "End of Epoch 487Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.408739 \n",
            "\n",
            "Epoch 488\n",
            "-------------------------------\n",
            "End of Epoch 488Train : \n",
            " Accuracy = 82.4, Loss =  0.489361\n",
            "End of Epoch 488Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.470445 \n",
            "\n",
            "Epoch 489\n",
            "-------------------------------\n",
            "End of Epoch 489Train : \n",
            " Accuracy = 82.7, Loss =  0.479143\n",
            "End of Epoch 489Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.435115 \n",
            "\n",
            "Epoch 490\n",
            "-------------------------------\n",
            "End of Epoch 490Train : \n",
            " Accuracy = 83.1, Loss =  0.471723\n",
            "End of Epoch 490Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.434385 \n",
            "\n",
            "Epoch 491\n",
            "-------------------------------\n",
            "End of Epoch 491Train : \n",
            " Accuracy = 83.3, Loss =  0.458536\n",
            "End of Epoch 491Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.465867 \n",
            "\n",
            "Epoch 492\n",
            "-------------------------------\n",
            "End of Epoch 492Train : \n",
            " Accuracy = 82.6, Loss =  0.486679\n",
            "End of Epoch 492Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.472119 \n",
            "\n",
            "Epoch 493\n",
            "-------------------------------\n",
            "End of Epoch 493Train : \n",
            " Accuracy = 82.6, Loss =  0.482062\n",
            "End of Epoch 493Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.429373 \n",
            "\n",
            "Epoch 494\n",
            "-------------------------------\n",
            "End of Epoch 494Train : \n",
            " Accuracy = 84.0, Loss =  0.443344\n",
            "End of Epoch 494Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.408741 \n",
            "\n",
            "Epoch 495\n",
            "-------------------------------\n",
            "End of Epoch 495Train : \n",
            " Accuracy = 83.6, Loss =  0.451627\n",
            "End of Epoch 495Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.467443 \n",
            "\n",
            "Epoch 496\n",
            "-------------------------------\n",
            "End of Epoch 496Train : \n",
            " Accuracy = 82.2, Loss =  0.495684\n",
            "End of Epoch 496Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.467165 \n",
            "\n",
            "Epoch 497\n",
            "-------------------------------\n",
            "End of Epoch 497Train : \n",
            " Accuracy = 83.4, Loss =  0.463572\n",
            "End of Epoch 497Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.438479 \n",
            "\n",
            "Epoch 498\n",
            "-------------------------------\n",
            "End of Epoch 498Train : \n",
            " Accuracy = 83.8, Loss =  0.451014\n",
            "End of Epoch 498Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.418443 \n",
            "\n",
            "Epoch 499\n",
            "-------------------------------\n",
            "End of Epoch 499Train : \n",
            " Accuracy = 82.9, Loss =  0.474210\n",
            "End of Epoch 499Test : \n",
            " Accuracy = 70.8, Loss =  0.911366\n",
            "Loss over all Training data : 0.456630 \n",
            "\n",
            "Epoch 500\n",
            "-------------------------------\n",
            "End of Epoch 500Train : \n",
            " Accuracy = 81.4, Loss =  0.509826\n",
            "Loss over all Test data : 0.936461 \n",
            "\n",
            "Loss over all Training data : 0.526089 \n",
            "\n",
            "End of Epoch 00500Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.526036 \n",
            "\n",
            "Epoch 501\n",
            "-------------------------------\n",
            "End of Epoch 501Train : \n",
            " Accuracy = 81.8, Loss =  0.507098\n",
            "End of Epoch 501Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.490510 \n",
            "\n",
            "Epoch 502\n",
            "-------------------------------\n",
            "End of Epoch 502Train : \n",
            " Accuracy = 83.1, Loss =  0.471969\n",
            "End of Epoch 502Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.464892 \n",
            "\n",
            "Epoch 503\n",
            "-------------------------------\n",
            "End of Epoch 503Train : \n",
            " Accuracy = 82.7, Loss =  0.482647\n",
            "End of Epoch 503Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.523699 \n",
            "\n",
            "Epoch 504\n",
            "-------------------------------\n",
            "End of Epoch 504Train : \n",
            " Accuracy = 82.8, Loss =  0.477756\n",
            "End of Epoch 504Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.452382 \n",
            "\n",
            "Epoch 505\n",
            "-------------------------------\n",
            "End of Epoch 505Train : \n",
            " Accuracy = 82.8, Loss =  0.474403\n",
            "End of Epoch 505Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.457240 \n",
            "\n",
            "Epoch 506\n",
            "-------------------------------\n",
            "End of Epoch 506Train : \n",
            " Accuracy = 82.3, Loss =  0.491212\n",
            "End of Epoch 506Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.503134 \n",
            "\n",
            "Epoch 507\n",
            "-------------------------------\n",
            "End of Epoch 507Train : \n",
            " Accuracy = 82.0, Loss =  0.499228\n",
            "End of Epoch 507Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.460160 \n",
            "\n",
            "Epoch 508\n",
            "-------------------------------\n",
            "End of Epoch 508Train : \n",
            " Accuracy = 82.3, Loss =  0.491115\n",
            "End of Epoch 508Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.515220 \n",
            "\n",
            "Epoch 509\n",
            "-------------------------------\n",
            "End of Epoch 509Train : \n",
            " Accuracy = 80.8, Loss =  0.537227\n",
            "End of Epoch 509Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.494639 \n",
            "\n",
            "Epoch 510\n",
            "-------------------------------\n",
            "End of Epoch 510Train : \n",
            " Accuracy = 82.0, Loss =  0.498231\n",
            "End of Epoch 510Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.488053 \n",
            "\n",
            "Epoch 511\n",
            "-------------------------------\n",
            "End of Epoch 511Train : \n",
            " Accuracy = 81.3, Loss =  0.518248\n",
            "End of Epoch 511Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.479975 \n",
            "\n",
            "Epoch 512\n",
            "-------------------------------\n",
            "End of Epoch 512Train : \n",
            " Accuracy = 81.5, Loss =  0.512378\n",
            "End of Epoch 512Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.448578 \n",
            "\n",
            "Epoch 513\n",
            "-------------------------------\n",
            "End of Epoch 513Train : \n",
            " Accuracy = 82.3, Loss =  0.490350\n",
            "End of Epoch 513Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.532489 \n",
            "\n",
            "Epoch 514\n",
            "-------------------------------\n",
            "End of Epoch 514Train : \n",
            " Accuracy = 75.8, Loss =  0.680931\n",
            "End of Epoch 514Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 1.099569 \n",
            "\n",
            "Epoch 515\n",
            "-------------------------------\n",
            "End of Epoch 515Train : \n",
            " Accuracy = 69.5, Loss =  0.854883\n",
            "End of Epoch 515Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.687006 \n",
            "\n",
            "Epoch 516\n",
            "-------------------------------\n",
            "End of Epoch 516Train : \n",
            " Accuracy = 75.6, Loss =  0.682000\n",
            "End of Epoch 516Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.604343 \n",
            "\n",
            "Epoch 517\n",
            "-------------------------------\n",
            "End of Epoch 517Train : \n",
            " Accuracy = 79.1, Loss =  0.582434\n",
            "End of Epoch 517Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.618997 \n",
            "\n",
            "Epoch 518\n",
            "-------------------------------\n",
            "End of Epoch 518Train : \n",
            " Accuracy = 80.2, Loss =  0.555752\n",
            "End of Epoch 518Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.586079 \n",
            "\n",
            "Epoch 519\n",
            "-------------------------------\n",
            "End of Epoch 519Train : \n",
            " Accuracy = 79.7, Loss =  0.564774\n",
            "End of Epoch 519Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.514063 \n",
            "\n",
            "Epoch 520\n",
            "-------------------------------\n",
            "End of Epoch 520Train : \n",
            " Accuracy = 81.9, Loss =  0.505310\n",
            "End of Epoch 520Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.467217 \n",
            "\n",
            "Epoch 521\n",
            "-------------------------------\n",
            "End of Epoch 521Train : \n",
            " Accuracy = 81.7, Loss =  0.511994\n",
            "End of Epoch 521Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.486725 \n",
            "\n",
            "Epoch 522\n",
            "-------------------------------\n",
            "End of Epoch 522Train : \n",
            " Accuracy = 81.4, Loss =  0.515922\n",
            "End of Epoch 522Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.509694 \n",
            "\n",
            "Epoch 523\n",
            "-------------------------------\n",
            "End of Epoch 523Train : \n",
            " Accuracy = 81.7, Loss =  0.508846\n",
            "End of Epoch 523Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.480795 \n",
            "\n",
            "Epoch 524\n",
            "-------------------------------\n",
            "End of Epoch 524Train : \n",
            " Accuracy = 80.5, Loss =  0.543406\n",
            "End of Epoch 524Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.496463 \n",
            "\n",
            "Epoch 525\n",
            "-------------------------------\n",
            "End of Epoch 525Train : \n",
            " Accuracy = 80.3, Loss =  0.546905\n",
            "End of Epoch 525Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.613050 \n",
            "\n",
            "Epoch 526\n",
            "-------------------------------\n",
            "End of Epoch 526Train : \n",
            " Accuracy = 79.0, Loss =  0.592273\n",
            "End of Epoch 526Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.499921 \n",
            "\n",
            "Epoch 527\n",
            "-------------------------------\n",
            "End of Epoch 527Train : \n",
            " Accuracy = 81.6, Loss =  0.507635\n",
            "End of Epoch 527Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.474355 \n",
            "\n",
            "Epoch 528\n",
            "-------------------------------\n",
            "End of Epoch 528Train : \n",
            " Accuracy = 82.4, Loss =  0.490931\n",
            "End of Epoch 528Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.495128 \n",
            "\n",
            "Epoch 529\n",
            "-------------------------------\n",
            "End of Epoch 529Train : \n",
            " Accuracy = 77.4, Loss =  0.635656\n",
            "End of Epoch 529Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.571527 \n",
            "\n",
            "Epoch 530\n",
            "-------------------------------\n",
            "End of Epoch 530Train : \n",
            " Accuracy = 80.7, Loss =  0.539938\n",
            "End of Epoch 530Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.554288 \n",
            "\n",
            "Epoch 531\n",
            "-------------------------------\n",
            "End of Epoch 531Train : \n",
            " Accuracy = 80.4, Loss =  0.550320\n",
            "End of Epoch 531Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.474139 \n",
            "\n",
            "Epoch 532\n",
            "-------------------------------\n",
            "End of Epoch 532Train : \n",
            " Accuracy = 81.5, Loss =  0.515069\n",
            "End of Epoch 532Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.551601 \n",
            "\n",
            "Epoch 533\n",
            "-------------------------------\n",
            "End of Epoch 533Train : \n",
            " Accuracy = 78.6, Loss =  0.600207\n",
            "End of Epoch 533Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.500233 \n",
            "\n",
            "Epoch 534\n",
            "-------------------------------\n",
            "End of Epoch 534Train : \n",
            " Accuracy = 81.5, Loss =  0.514863\n",
            "End of Epoch 534Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.461618 \n",
            "\n",
            "Epoch 535\n",
            "-------------------------------\n",
            "End of Epoch 535Train : \n",
            " Accuracy = 81.7, Loss =  0.512317\n",
            "End of Epoch 535Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.457759 \n",
            "\n",
            "Epoch 536\n",
            "-------------------------------\n",
            "End of Epoch 536Train : \n",
            " Accuracy = 80.9, Loss =  0.529215\n",
            "End of Epoch 536Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.475742 \n",
            "\n",
            "Epoch 537\n",
            "-------------------------------\n",
            "End of Epoch 537Train : \n",
            " Accuracy = 79.9, Loss =  0.564153\n",
            "End of Epoch 537Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.590930 \n",
            "\n",
            "Epoch 538\n",
            "-------------------------------\n",
            "End of Epoch 538Train : \n",
            " Accuracy = 79.8, Loss =  0.569875\n",
            "End of Epoch 538Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.500047 \n",
            "\n",
            "Epoch 539\n",
            "-------------------------------\n",
            "End of Epoch 539Train : \n",
            " Accuracy = 78.6, Loss =  0.594041\n",
            "End of Epoch 539Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.601115 \n",
            "\n",
            "Epoch 540\n",
            "-------------------------------\n",
            "End of Epoch 540Train : \n",
            " Accuracy = 80.8, Loss =  0.534113\n",
            "End of Epoch 540Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.491231 \n",
            "\n",
            "Epoch 541\n",
            "-------------------------------\n",
            "End of Epoch 541Train : \n",
            " Accuracy = 81.7, Loss =  0.510822\n",
            "End of Epoch 541Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.460096 \n",
            "\n",
            "Epoch 542\n",
            "-------------------------------\n",
            "End of Epoch 542Train : \n",
            " Accuracy = 82.2, Loss =  0.495981\n",
            "End of Epoch 542Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.452242 \n",
            "\n",
            "Epoch 543\n",
            "-------------------------------\n",
            "End of Epoch 543Train : \n",
            " Accuracy = 82.7, Loss =  0.481870\n",
            "End of Epoch 543Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.459598 \n",
            "\n",
            "Epoch 544\n",
            "-------------------------------\n",
            "End of Epoch 544Train : \n",
            " Accuracy = 82.6, Loss =  0.484808\n",
            "End of Epoch 544Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.480185 \n",
            "\n",
            "Epoch 545\n",
            "-------------------------------\n",
            "End of Epoch 545Train : \n",
            " Accuracy = 83.3, Loss =  0.462123\n",
            "End of Epoch 545Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.469749 \n",
            "\n",
            "Epoch 546\n",
            "-------------------------------\n",
            "End of Epoch 546Train : \n",
            " Accuracy = 83.0, Loss =  0.470178\n",
            "End of Epoch 546Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.442438 \n",
            "\n",
            "Epoch 547\n",
            "-------------------------------\n",
            "End of Epoch 547Train : \n",
            " Accuracy = 81.3, Loss =  0.517415\n",
            "End of Epoch 547Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.476834 \n",
            "\n",
            "Epoch 548\n",
            "-------------------------------\n",
            "End of Epoch 548Train : \n",
            " Accuracy = 82.7, Loss =  0.482157\n",
            "End of Epoch 548Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.465016 \n",
            "\n",
            "Epoch 549\n",
            "-------------------------------\n",
            "End of Epoch 549Train : \n",
            " Accuracy = 82.9, Loss =  0.472875\n",
            "End of Epoch 549Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.441006 \n",
            "\n",
            "Epoch 550\n",
            "-------------------------------\n",
            "End of Epoch 550Train : \n",
            " Accuracy = 80.8, Loss =  0.534562\n",
            "End of Epoch 550Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.571835 \n",
            "\n",
            "Epoch 551\n",
            "-------------------------------\n",
            "End of Epoch 551Train : \n",
            " Accuracy = 80.3, Loss =  0.552283\n",
            "End of Epoch 551Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.472079 \n",
            "\n",
            "Epoch 552\n",
            "-------------------------------\n",
            "End of Epoch 552Train : \n",
            " Accuracy = 81.7, Loss =  0.505953\n",
            "End of Epoch 552Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.490171 \n",
            "\n",
            "Epoch 553\n",
            "-------------------------------\n",
            "End of Epoch 553Train : \n",
            " Accuracy = 80.5, Loss =  0.543022\n",
            "End of Epoch 553Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.537814 \n",
            "\n",
            "Epoch 554\n",
            "-------------------------------\n",
            "End of Epoch 554Train : \n",
            " Accuracy = 81.4, Loss =  0.522761\n",
            "End of Epoch 554Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.492102 \n",
            "\n",
            "Epoch 555\n",
            "-------------------------------\n",
            "End of Epoch 555Train : \n",
            " Accuracy = 80.7, Loss =  0.536632\n",
            "End of Epoch 555Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.470876 \n",
            "\n",
            "Epoch 556\n",
            "-------------------------------\n",
            "End of Epoch 556Train : \n",
            " Accuracy = 81.3, Loss =  0.526113\n",
            "End of Epoch 556Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.498890 \n",
            "\n",
            "Epoch 557\n",
            "-------------------------------\n",
            "End of Epoch 557Train : \n",
            " Accuracy = 81.5, Loss =  0.512162\n",
            "End of Epoch 557Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.462209 \n",
            "\n",
            "Epoch 558\n",
            "-------------------------------\n",
            "End of Epoch 558Train : \n",
            " Accuracy = 82.0, Loss =  0.506089\n",
            "End of Epoch 558Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.516964 \n",
            "\n",
            "Epoch 559\n",
            "-------------------------------\n",
            "End of Epoch 559Train : \n",
            " Accuracy = 80.4, Loss =  0.540266\n",
            "End of Epoch 559Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.532629 \n",
            "\n",
            "Epoch 560\n",
            "-------------------------------\n",
            "End of Epoch 560Train : \n",
            " Accuracy = 81.5, Loss =  0.518743\n",
            "End of Epoch 560Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.459722 \n",
            "\n",
            "Epoch 561\n",
            "-------------------------------\n",
            "End of Epoch 561Train : \n",
            " Accuracy = 82.3, Loss =  0.490605\n",
            "End of Epoch 561Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.514813 \n",
            "\n",
            "Epoch 562\n",
            "-------------------------------\n",
            "End of Epoch 562Train : \n",
            " Accuracy = 80.9, Loss =  0.532250\n",
            "End of Epoch 562Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.552238 \n",
            "\n",
            "Epoch 563\n",
            "-------------------------------\n",
            "End of Epoch 563Train : \n",
            " Accuracy = 82.1, Loss =  0.498598\n",
            "End of Epoch 563Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.484349 \n",
            "\n",
            "Epoch 564\n",
            "-------------------------------\n",
            "End of Epoch 564Train : \n",
            " Accuracy = 81.1, Loss =  0.527752\n",
            "End of Epoch 564Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.500061 \n",
            "\n",
            "Epoch 565\n",
            "-------------------------------\n",
            "End of Epoch 565Train : \n",
            " Accuracy = 81.9, Loss =  0.503857\n",
            "End of Epoch 565Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.504362 \n",
            "\n",
            "Epoch 566\n",
            "-------------------------------\n",
            "End of Epoch 566Train : \n",
            " Accuracy = 81.7, Loss =  0.511109\n",
            "End of Epoch 566Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.455304 \n",
            "\n",
            "Epoch 567\n",
            "-------------------------------\n",
            "End of Epoch 567Train : \n",
            " Accuracy = 82.7, Loss =  0.481317\n",
            "End of Epoch 567Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.510800 \n",
            "\n",
            "Epoch 568\n",
            "-------------------------------\n",
            "End of Epoch 568Train : \n",
            " Accuracy = 82.6, Loss =  0.488408\n",
            "End of Epoch 568Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.431642 \n",
            "\n",
            "Epoch 569\n",
            "-------------------------------\n",
            "End of Epoch 569Train : \n",
            " Accuracy = 82.9, Loss =  0.480636\n",
            "End of Epoch 569Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.417986 \n",
            "\n",
            "Epoch 570\n",
            "-------------------------------\n",
            "End of Epoch 570Train : \n",
            " Accuracy = 83.5, Loss =  0.456575\n",
            "End of Epoch 570Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.424633 \n",
            "\n",
            "Epoch 571\n",
            "-------------------------------\n",
            "End of Epoch 571Train : \n",
            " Accuracy = 80.7, Loss =  0.537242\n",
            "End of Epoch 571Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.458820 \n",
            "\n",
            "Epoch 572\n",
            "-------------------------------\n",
            "End of Epoch 572Train : \n",
            " Accuracy = 82.6, Loss =  0.488150\n",
            "End of Epoch 572Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.481098 \n",
            "\n",
            "Epoch 573\n",
            "-------------------------------\n",
            "End of Epoch 573Train : \n",
            " Accuracy = 82.5, Loss =  0.487060\n",
            "End of Epoch 573Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.457766 \n",
            "\n",
            "Epoch 574\n",
            "-------------------------------\n",
            "End of Epoch 574Train : \n",
            " Accuracy = 82.5, Loss =  0.488452\n",
            "End of Epoch 574Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.453275 \n",
            "\n",
            "Epoch 575\n",
            "-------------------------------\n",
            "End of Epoch 575Train : \n",
            " Accuracy = 83.2, Loss =  0.466704\n",
            "End of Epoch 575Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.444358 \n",
            "\n",
            "Epoch 576\n",
            "-------------------------------\n",
            "End of Epoch 576Train : \n",
            " Accuracy = 82.3, Loss =  0.486100\n",
            "End of Epoch 576Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.458988 \n",
            "\n",
            "Epoch 577\n",
            "-------------------------------\n",
            "End of Epoch 577Train : \n",
            " Accuracy = 82.0, Loss =  0.497747\n",
            "End of Epoch 577Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.457679 \n",
            "\n",
            "Epoch 578\n",
            "-------------------------------\n",
            "End of Epoch 578Train : \n",
            " Accuracy = 82.2, Loss =  0.495477\n",
            "End of Epoch 578Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.455177 \n",
            "\n",
            "Epoch 579\n",
            "-------------------------------\n",
            "End of Epoch 579Train : \n",
            " Accuracy = 75.4, Loss =  0.682040\n",
            "End of Epoch 579Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.620730 \n",
            "\n",
            "Epoch 580\n",
            "-------------------------------\n",
            "End of Epoch 580Train : \n",
            " Accuracy = 75.4, Loss =  0.684494\n",
            "End of Epoch 580Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.686269 \n",
            "\n",
            "Epoch 581\n",
            "-------------------------------\n",
            "End of Epoch 581Train : \n",
            " Accuracy = 76.2, Loss =  0.659643\n",
            "End of Epoch 581Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.581446 \n",
            "\n",
            "Epoch 582\n",
            "-------------------------------\n",
            "End of Epoch 582Train : \n",
            " Accuracy = 79.6, Loss =  0.566509\n",
            "End of Epoch 582Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.524815 \n",
            "\n",
            "Epoch 583\n",
            "-------------------------------\n",
            "End of Epoch 583Train : \n",
            " Accuracy = 80.4, Loss =  0.550526\n",
            "End of Epoch 583Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.519519 \n",
            "\n",
            "Epoch 584\n",
            "-------------------------------\n",
            "End of Epoch 584Train : \n",
            " Accuracy = 80.2, Loss =  0.549158\n",
            "End of Epoch 584Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.646149 \n",
            "\n",
            "Epoch 585\n",
            "-------------------------------\n",
            "End of Epoch 585Train : \n",
            " Accuracy = 77.4, Loss =  0.624142\n",
            "End of Epoch 585Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.557126 \n",
            "\n",
            "Epoch 586\n",
            "-------------------------------\n",
            "End of Epoch 586Train : \n",
            " Accuracy = 78.7, Loss =  0.591603\n",
            "End of Epoch 586Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.537482 \n",
            "\n",
            "Epoch 587\n",
            "-------------------------------\n",
            "End of Epoch 587Train : \n",
            " Accuracy = 81.4, Loss =  0.518542\n",
            "End of Epoch 587Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.497414 \n",
            "\n",
            "Epoch 588\n",
            "-------------------------------\n",
            "End of Epoch 588Train : \n",
            " Accuracy = 81.6, Loss =  0.508251\n",
            "End of Epoch 588Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.454556 \n",
            "\n",
            "Epoch 589\n",
            "-------------------------------\n",
            "End of Epoch 589Train : \n",
            " Accuracy = 83.0, Loss =  0.469832\n",
            "End of Epoch 589Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.456139 \n",
            "\n",
            "Epoch 590\n",
            "-------------------------------\n",
            "End of Epoch 590Train : \n",
            " Accuracy = 81.7, Loss =  0.508075\n",
            "End of Epoch 590Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.433908 \n",
            "\n",
            "Epoch 591\n",
            "-------------------------------\n",
            "End of Epoch 591Train : \n",
            " Accuracy = 81.7, Loss =  0.508552\n",
            "End of Epoch 591Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.486246 \n",
            "\n",
            "Epoch 592\n",
            "-------------------------------\n",
            "End of Epoch 592Train : \n",
            " Accuracy = 75.6, Loss =  0.677816\n",
            "End of Epoch 592Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.973803 \n",
            "\n",
            "Epoch 593\n",
            "-------------------------------\n",
            "End of Epoch 593Train : \n",
            " Accuracy = 77.1, Loss =  0.652078\n",
            "End of Epoch 593Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.530420 \n",
            "\n",
            "Epoch 594\n",
            "-------------------------------\n",
            "End of Epoch 594Train : \n",
            " Accuracy = 80.7, Loss =  0.544941\n",
            "End of Epoch 594Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.546537 \n",
            "\n",
            "Epoch 595\n",
            "-------------------------------\n",
            "End of Epoch 595Train : \n",
            " Accuracy = 78.6, Loss =  0.603783\n",
            "End of Epoch 595Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.889620 \n",
            "\n",
            "Epoch 596\n",
            "-------------------------------\n",
            "End of Epoch 596Train : \n",
            " Accuracy = 72.2, Loss =  0.785092\n",
            "End of Epoch 596Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.679090 \n",
            "\n",
            "Epoch 597\n",
            "-------------------------------\n",
            "End of Epoch 597Train : \n",
            " Accuracy = 78.7, Loss =  0.602675\n",
            "End of Epoch 597Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.534212 \n",
            "\n",
            "Epoch 598\n",
            "-------------------------------\n",
            "End of Epoch 598Train : \n",
            " Accuracy = 80.7, Loss =  0.539893\n",
            "End of Epoch 598Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.492619 \n",
            "\n",
            "Epoch 599\n",
            "-------------------------------\n",
            "End of Epoch 599Train : \n",
            " Accuracy = 81.6, Loss =  0.517183\n",
            "End of Epoch 599Test : \n",
            " Accuracy = 69.8, Loss =  0.936461\n",
            "Loss over all Training data : 0.505572 \n",
            "\n",
            "Epoch 600\n",
            "-------------------------------\n",
            "End of Epoch 600Train : \n",
            " Accuracy = 81.3, Loss =  0.524755\n",
            "Loss over all Test data : 0.924118 \n",
            "\n",
            "Loss over all Training data : 0.521070 \n",
            "\n",
            "End of Epoch 00600Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.520862 \n",
            "\n",
            "Epoch 601\n",
            "-------------------------------\n",
            "End of Epoch 601Train : \n",
            " Accuracy = 80.6, Loss =  0.542291\n",
            "End of Epoch 601Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.526450 \n",
            "\n",
            "Epoch 602\n",
            "-------------------------------\n",
            "End of Epoch 602Train : \n",
            " Accuracy = 81.2, Loss =  0.528572\n",
            "End of Epoch 602Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.499096 \n",
            "\n",
            "Epoch 603\n",
            "-------------------------------\n",
            "End of Epoch 603Train : \n",
            " Accuracy = 81.7, Loss =  0.507637\n",
            "End of Epoch 603Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.459294 \n",
            "\n",
            "Epoch 604\n",
            "-------------------------------\n",
            "End of Epoch 604Train : \n",
            " Accuracy = 82.2, Loss =  0.494214\n",
            "End of Epoch 604Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.472031 \n",
            "\n",
            "Epoch 605\n",
            "-------------------------------\n",
            "End of Epoch 605Train : \n",
            " Accuracy = 80.8, Loss =  0.534144\n",
            "End of Epoch 605Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.491119 \n",
            "\n",
            "Epoch 606\n",
            "-------------------------------\n",
            "End of Epoch 606Train : \n",
            " Accuracy = 81.6, Loss =  0.514564\n",
            "End of Epoch 606Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.524458 \n",
            "\n",
            "Epoch 607\n",
            "-------------------------------\n",
            "End of Epoch 607Train : \n",
            " Accuracy = 73.7, Loss =  0.745609\n",
            "End of Epoch 607Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.720659 \n",
            "\n",
            "Epoch 608\n",
            "-------------------------------\n",
            "End of Epoch 608Train : \n",
            " Accuracy = 73.8, Loss =  0.742048\n",
            "End of Epoch 608Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.654459 \n",
            "\n",
            "Epoch 609\n",
            "-------------------------------\n",
            "End of Epoch 609Train : \n",
            " Accuracy = 77.0, Loss =  0.646474\n",
            "End of Epoch 609Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.575372 \n",
            "\n",
            "Epoch 610\n",
            "-------------------------------\n",
            "End of Epoch 610Train : \n",
            " Accuracy = 80.1, Loss =  0.558543\n",
            "End of Epoch 610Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.505878 \n",
            "\n",
            "Epoch 611\n",
            "-------------------------------\n",
            "End of Epoch 611Train : \n",
            " Accuracy = 81.5, Loss =  0.516017\n",
            "End of Epoch 611Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.483904 \n",
            "\n",
            "Epoch 612\n",
            "-------------------------------\n",
            "End of Epoch 612Train : \n",
            " Accuracy = 82.5, Loss =  0.488964\n",
            "End of Epoch 612Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.456334 \n",
            "\n",
            "Epoch 613\n",
            "-------------------------------\n",
            "End of Epoch 613Train : \n",
            " Accuracy = 82.1, Loss =  0.494303\n",
            "End of Epoch 613Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.506398 \n",
            "\n",
            "Epoch 614\n",
            "-------------------------------\n",
            "End of Epoch 614Train : \n",
            " Accuracy = 79.7, Loss =  0.571981\n",
            "End of Epoch 614Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.526756 \n",
            "\n",
            "Epoch 615\n",
            "-------------------------------\n",
            "End of Epoch 615Train : \n",
            " Accuracy = 78.0, Loss =  0.622388\n",
            "End of Epoch 615Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.724670 \n",
            "\n",
            "Epoch 616\n",
            "-------------------------------\n",
            "End of Epoch 616Train : \n",
            " Accuracy = 77.5, Loss =  0.632516\n",
            "End of Epoch 616Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.599215 \n",
            "\n",
            "Epoch 617\n",
            "-------------------------------\n",
            "End of Epoch 617Train : \n",
            " Accuracy = 78.7, Loss =  0.594047\n",
            "End of Epoch 617Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.563020 \n",
            "\n",
            "Epoch 618\n",
            "-------------------------------\n",
            "End of Epoch 618Train : \n",
            " Accuracy = 78.0, Loss =  0.622838\n",
            "End of Epoch 618Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.720858 \n",
            "\n",
            "Epoch 619\n",
            "-------------------------------\n",
            "End of Epoch 619Train : \n",
            " Accuracy = 78.5, Loss =  0.600257\n",
            "End of Epoch 619Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.545202 \n",
            "\n",
            "Epoch 620\n",
            "-------------------------------\n",
            "End of Epoch 620Train : \n",
            " Accuracy = 76.6, Loss =  0.655080\n",
            "End of Epoch 620Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.616810 \n",
            "\n",
            "Epoch 621\n",
            "-------------------------------\n",
            "End of Epoch 621Train : \n",
            " Accuracy = 74.7, Loss =  0.710014\n",
            "End of Epoch 621Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.748997 \n",
            "\n",
            "Epoch 622\n",
            "-------------------------------\n",
            "End of Epoch 622Train : \n",
            " Accuracy = 75.3, Loss =  0.694743\n",
            "End of Epoch 622Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.614070 \n",
            "\n",
            "Epoch 623\n",
            "-------------------------------\n",
            "End of Epoch 623Train : \n",
            " Accuracy = 77.1, Loss =  0.641669\n",
            "End of Epoch 623Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.540545 \n",
            "\n",
            "Epoch 624\n",
            "-------------------------------\n",
            "End of Epoch 624Train : \n",
            " Accuracy = 76.5, Loss =  0.668373\n",
            "End of Epoch 624Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.617252 \n",
            "\n",
            "Epoch 625\n",
            "-------------------------------\n",
            "End of Epoch 625Train : \n",
            " Accuracy = 75.5, Loss =  0.689078\n",
            "End of Epoch 625Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.565896 \n",
            "\n",
            "Epoch 626\n",
            "-------------------------------\n",
            "End of Epoch 626Train : \n",
            " Accuracy = 79.3, Loss =  0.578115\n",
            "End of Epoch 626Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.525393 \n",
            "\n",
            "Epoch 627\n",
            "-------------------------------\n",
            "End of Epoch 627Train : \n",
            " Accuracy = 79.9, Loss =  0.562553\n",
            "End of Epoch 627Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.523292 \n",
            "\n",
            "Epoch 628\n",
            "-------------------------------\n",
            "End of Epoch 628Train : \n",
            " Accuracy = 80.4, Loss =  0.550015\n",
            "End of Epoch 628Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.520677 \n",
            "\n",
            "Epoch 629\n",
            "-------------------------------\n",
            "End of Epoch 629Train : \n",
            " Accuracy = 81.5, Loss =  0.517733\n",
            "End of Epoch 629Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.506407 \n",
            "\n",
            "Epoch 630\n",
            "-------------------------------\n",
            "End of Epoch 630Train : \n",
            " Accuracy = 81.8, Loss =  0.506645\n",
            "End of Epoch 630Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.468995 \n",
            "\n",
            "Epoch 631\n",
            "-------------------------------\n",
            "End of Epoch 631Train : \n",
            " Accuracy = 82.3, Loss =  0.493773\n",
            "End of Epoch 631Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.462058 \n",
            "\n",
            "Epoch 632\n",
            "-------------------------------\n",
            "End of Epoch 632Train : \n",
            " Accuracy = 81.6, Loss =  0.512148\n",
            "End of Epoch 632Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.521626 \n",
            "\n",
            "Epoch 633\n",
            "-------------------------------\n",
            "End of Epoch 633Train : \n",
            " Accuracy = 80.0, Loss =  0.558176\n",
            "End of Epoch 633Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.644784 \n",
            "\n",
            "Epoch 634\n",
            "-------------------------------\n",
            "End of Epoch 634Train : \n",
            " Accuracy = 79.5, Loss =  0.574059\n",
            "End of Epoch 634Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.485749 \n",
            "\n",
            "Epoch 635\n",
            "-------------------------------\n",
            "End of Epoch 635Train : \n",
            " Accuracy = 80.7, Loss =  0.540676\n",
            "End of Epoch 635Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.580680 \n",
            "\n",
            "Epoch 636\n",
            "-------------------------------\n",
            "End of Epoch 636Train : \n",
            " Accuracy = 80.7, Loss =  0.537116\n",
            "End of Epoch 636Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.478495 \n",
            "\n",
            "Epoch 637\n",
            "-------------------------------\n",
            "End of Epoch 637Train : \n",
            " Accuracy = 82.1, Loss =  0.501251\n",
            "End of Epoch 637Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.462644 \n",
            "\n",
            "Epoch 638\n",
            "-------------------------------\n",
            "End of Epoch 638Train : \n",
            " Accuracy = 81.9, Loss =  0.507049\n",
            "End of Epoch 638Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.483695 \n",
            "\n",
            "Epoch 639\n",
            "-------------------------------\n",
            "End of Epoch 639Train : \n",
            " Accuracy = 81.8, Loss =  0.505659\n",
            "End of Epoch 639Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.492718 \n",
            "\n",
            "Epoch 640\n",
            "-------------------------------\n",
            "End of Epoch 640Train : \n",
            " Accuracy = 81.4, Loss =  0.520066\n",
            "End of Epoch 640Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.469840 \n",
            "\n",
            "Epoch 641\n",
            "-------------------------------\n",
            "End of Epoch 641Train : \n",
            " Accuracy = 82.2, Loss =  0.494153\n",
            "End of Epoch 641Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.471409 \n",
            "\n",
            "Epoch 642\n",
            "-------------------------------\n",
            "End of Epoch 642Train : \n",
            " Accuracy = 79.5, Loss =  0.570284\n",
            "End of Epoch 642Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.685157 \n",
            "\n",
            "Epoch 643\n",
            "-------------------------------\n",
            "End of Epoch 643Train : \n",
            " Accuracy = 73.7, Loss =  0.741055\n",
            "End of Epoch 643Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.824629 \n",
            "\n",
            "Epoch 644\n",
            "-------------------------------\n",
            "End of Epoch 644Train : \n",
            " Accuracy = 77.0, Loss =  0.653972\n",
            "End of Epoch 644Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.516685 \n",
            "\n",
            "Epoch 645\n",
            "-------------------------------\n",
            "End of Epoch 645Train : \n",
            " Accuracy = 81.0, Loss =  0.526930\n",
            "End of Epoch 645Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.489089 \n",
            "\n",
            "Epoch 646\n",
            "-------------------------------\n",
            "End of Epoch 646Train : \n",
            " Accuracy = 81.6, Loss =  0.517896\n",
            "End of Epoch 646Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.491032 \n",
            "\n",
            "Epoch 647\n",
            "-------------------------------\n",
            "End of Epoch 647Train : \n",
            " Accuracy = 81.3, Loss =  0.525147\n",
            "End of Epoch 647Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.460215 \n",
            "\n",
            "Epoch 648\n",
            "-------------------------------\n",
            "End of Epoch 648Train : \n",
            " Accuracy = 82.6, Loss =  0.482858\n",
            "End of Epoch 648Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.464151 \n",
            "\n",
            "Epoch 649\n",
            "-------------------------------\n",
            "End of Epoch 649Train : \n",
            " Accuracy = 82.9, Loss =  0.479600\n",
            "End of Epoch 649Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.438299 \n",
            "\n",
            "Epoch 650\n",
            "-------------------------------\n",
            "End of Epoch 650Train : \n",
            " Accuracy = 82.9, Loss =  0.476462\n",
            "End of Epoch 650Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.460735 \n",
            "\n",
            "Epoch 651\n",
            "-------------------------------\n",
            "End of Epoch 651Train : \n",
            " Accuracy = 83.0, Loss =  0.474052\n",
            "End of Epoch 651Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.465533 \n",
            "\n",
            "Epoch 652\n",
            "-------------------------------\n",
            "End of Epoch 652Train : \n",
            " Accuracy = 83.1, Loss =  0.473067\n",
            "End of Epoch 652Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.425306 \n",
            "\n",
            "Epoch 653\n",
            "-------------------------------\n",
            "End of Epoch 653Train : \n",
            " Accuracy = 82.5, Loss =  0.487106\n",
            "End of Epoch 653Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.449522 \n",
            "\n",
            "Epoch 654\n",
            "-------------------------------\n",
            "End of Epoch 654Train : \n",
            " Accuracy = 83.3, Loss =  0.464153\n",
            "End of Epoch 654Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.426175 \n",
            "\n",
            "Epoch 655\n",
            "-------------------------------\n",
            "End of Epoch 655Train : \n",
            " Accuracy = 83.0, Loss =  0.468342\n",
            "End of Epoch 655Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.430313 \n",
            "\n",
            "Epoch 656\n",
            "-------------------------------\n",
            "End of Epoch 656Train : \n",
            " Accuracy = 83.1, Loss =  0.471354\n",
            "End of Epoch 656Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.467510 \n",
            "\n",
            "Epoch 657\n",
            "-------------------------------\n",
            "End of Epoch 657Train : \n",
            " Accuracy = 82.1, Loss =  0.492081\n",
            "End of Epoch 657Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.451743 \n",
            "\n",
            "Epoch 658\n",
            "-------------------------------\n",
            "End of Epoch 658Train : \n",
            " Accuracy = 82.4, Loss =  0.486678\n",
            "End of Epoch 658Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.461406 \n",
            "\n",
            "Epoch 659\n",
            "-------------------------------\n",
            "End of Epoch 659Train : \n",
            " Accuracy = 82.4, Loss =  0.488085\n",
            "End of Epoch 659Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.519936 \n",
            "\n",
            "Epoch 660\n",
            "-------------------------------\n",
            "End of Epoch 660Train : \n",
            " Accuracy = 80.4, Loss =  0.542921\n",
            "End of Epoch 660Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.583856 \n",
            "\n",
            "Epoch 661\n",
            "-------------------------------\n",
            "End of Epoch 661Train : \n",
            " Accuracy = 77.7, Loss =  0.624510\n",
            "End of Epoch 661Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.752668 \n",
            "\n",
            "Epoch 662\n",
            "-------------------------------\n",
            "End of Epoch 662Train : \n",
            " Accuracy = 79.6, Loss =  0.573647\n",
            "End of Epoch 662Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.520704 \n",
            "\n",
            "Epoch 663\n",
            "-------------------------------\n",
            "End of Epoch 663Train : \n",
            " Accuracy = 81.6, Loss =  0.513157\n",
            "End of Epoch 663Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.477871 \n",
            "\n",
            "Epoch 664\n",
            "-------------------------------\n",
            "End of Epoch 664Train : \n",
            " Accuracy = 81.1, Loss =  0.527873\n",
            "End of Epoch 664Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.505382 \n",
            "\n",
            "Epoch 665\n",
            "-------------------------------\n",
            "End of Epoch 665Train : \n",
            " Accuracy = 81.0, Loss =  0.527538\n",
            "End of Epoch 665Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.458211 \n",
            "\n",
            "Epoch 666\n",
            "-------------------------------\n",
            "End of Epoch 666Train : \n",
            " Accuracy = 82.6, Loss =  0.486031\n",
            "End of Epoch 666Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.451670 \n",
            "\n",
            "Epoch 667\n",
            "-------------------------------\n",
            "End of Epoch 667Train : \n",
            " Accuracy = 82.6, Loss =  0.481667\n",
            "End of Epoch 667Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.485007 \n",
            "\n",
            "Epoch 668\n",
            "-------------------------------\n",
            "End of Epoch 668Train : \n",
            " Accuracy = 82.0, Loss =  0.498244\n",
            "End of Epoch 668Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.509569 \n",
            "\n",
            "Epoch 669\n",
            "-------------------------------\n",
            "End of Epoch 669Train : \n",
            " Accuracy = 79.7, Loss =  0.569248\n",
            "End of Epoch 669Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.541877 \n",
            "\n",
            "Epoch 670\n",
            "-------------------------------\n",
            "End of Epoch 670Train : \n",
            " Accuracy = 81.6, Loss =  0.512388\n",
            "End of Epoch 670Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.475730 \n",
            "\n",
            "Epoch 671\n",
            "-------------------------------\n",
            "End of Epoch 671Train : \n",
            " Accuracy = 82.4, Loss =  0.485124\n",
            "End of Epoch 671Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.464704 \n",
            "\n",
            "Epoch 672\n",
            "-------------------------------\n",
            "End of Epoch 672Train : \n",
            " Accuracy = 78.6, Loss =  0.596194\n",
            "End of Epoch 672Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.701590 \n",
            "\n",
            "Epoch 673\n",
            "-------------------------------\n",
            "End of Epoch 673Train : \n",
            " Accuracy = 73.1, Loss =  0.757835\n",
            "End of Epoch 673Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.592650 \n",
            "\n",
            "Epoch 674\n",
            "-------------------------------\n",
            "End of Epoch 674Train : \n",
            " Accuracy = 80.8, Loss =  0.531067\n",
            "End of Epoch 674Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.478679 \n",
            "\n",
            "Epoch 675\n",
            "-------------------------------\n",
            "End of Epoch 675Train : \n",
            " Accuracy = 81.1, Loss =  0.521531\n",
            "End of Epoch 675Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.474608 \n",
            "\n",
            "Epoch 676\n",
            "-------------------------------\n",
            "End of Epoch 676Train : \n",
            " Accuracy = 81.9, Loss =  0.501770\n",
            "End of Epoch 676Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.500152 \n",
            "\n",
            "Epoch 677\n",
            "-------------------------------\n",
            "End of Epoch 677Train : \n",
            " Accuracy = 81.6, Loss =  0.507145\n",
            "End of Epoch 677Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.514401 \n",
            "\n",
            "Epoch 678\n",
            "-------------------------------\n",
            "End of Epoch 678Train : \n",
            " Accuracy = 81.6, Loss =  0.509064\n",
            "End of Epoch 678Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.443566 \n",
            "\n",
            "Epoch 679\n",
            "-------------------------------\n",
            "End of Epoch 679Train : \n",
            " Accuracy = 82.3, Loss =  0.486653\n",
            "End of Epoch 679Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.429568 \n",
            "\n",
            "Epoch 680\n",
            "-------------------------------\n",
            "End of Epoch 680Train : \n",
            " Accuracy = 82.8, Loss =  0.478662\n",
            "End of Epoch 680Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.466924 \n",
            "\n",
            "Epoch 681\n",
            "-------------------------------\n",
            "End of Epoch 681Train : \n",
            " Accuracy = 82.3, Loss =  0.492199\n",
            "End of Epoch 681Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.453465 \n",
            "\n",
            "Epoch 682\n",
            "-------------------------------\n",
            "End of Epoch 682Train : \n",
            " Accuracy = 82.0, Loss =  0.491861\n",
            "End of Epoch 682Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.490241 \n",
            "\n",
            "Epoch 683\n",
            "-------------------------------\n",
            "End of Epoch 683Train : \n",
            " Accuracy = 81.9, Loss =  0.502550\n",
            "End of Epoch 683Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.460423 \n",
            "\n",
            "Epoch 684\n",
            "-------------------------------\n",
            "End of Epoch 684Train : \n",
            " Accuracy = 82.2, Loss =  0.495070\n",
            "End of Epoch 684Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.495978 \n",
            "\n",
            "Epoch 685\n",
            "-------------------------------\n",
            "End of Epoch 685Train : \n",
            " Accuracy = 81.5, Loss =  0.514270\n",
            "End of Epoch 685Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.470453 \n",
            "\n",
            "Epoch 686\n",
            "-------------------------------\n",
            "End of Epoch 686Train : \n",
            " Accuracy = 81.7, Loss =  0.505412\n",
            "End of Epoch 686Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.476630 \n",
            "\n",
            "Epoch 687\n",
            "-------------------------------\n",
            "End of Epoch 687Train : \n",
            " Accuracy = 81.5, Loss =  0.513500\n",
            "End of Epoch 687Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.485434 \n",
            "\n",
            "Epoch 688\n",
            "-------------------------------\n",
            "End of Epoch 688Train : \n",
            " Accuracy = 80.9, Loss =  0.526283\n",
            "End of Epoch 688Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.486301 \n",
            "\n",
            "Epoch 689\n",
            "-------------------------------\n",
            "End of Epoch 689Train : \n",
            " Accuracy = 82.0, Loss =  0.496314\n",
            "End of Epoch 689Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.469933 \n",
            "\n",
            "Epoch 690\n",
            "-------------------------------\n",
            "End of Epoch 690Train : \n",
            " Accuracy = 80.7, Loss =  0.535024\n",
            "End of Epoch 690Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.567101 \n",
            "\n",
            "Epoch 691\n",
            "-------------------------------\n",
            "End of Epoch 691Train : \n",
            " Accuracy = 80.1, Loss =  0.550365\n",
            "End of Epoch 691Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.484435 \n",
            "\n",
            "Epoch 692\n",
            "-------------------------------\n",
            "End of Epoch 692Train : \n",
            " Accuracy = 79.9, Loss =  0.556430\n",
            "End of Epoch 692Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.539034 \n",
            "\n",
            "Epoch 693\n",
            "-------------------------------\n",
            "End of Epoch 693Train : \n",
            " Accuracy = 75.7, Loss =  0.686023\n",
            "End of Epoch 693Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.643518 \n",
            "\n",
            "Epoch 694\n",
            "-------------------------------\n",
            "End of Epoch 694Train : \n",
            " Accuracy = 79.4, Loss =  0.571549\n",
            "End of Epoch 694Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.522288 \n",
            "\n",
            "Epoch 695\n",
            "-------------------------------\n",
            "End of Epoch 695Train : \n",
            " Accuracy = 80.5, Loss =  0.540284\n",
            "End of Epoch 695Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.511346 \n",
            "\n",
            "Epoch 696\n",
            "-------------------------------\n",
            "End of Epoch 696Train : \n",
            " Accuracy = 79.9, Loss =  0.559103\n",
            "End of Epoch 696Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.541136 \n",
            "\n",
            "Epoch 697\n",
            "-------------------------------\n",
            "End of Epoch 697Train : \n",
            " Accuracy = 80.4, Loss =  0.541930\n",
            "End of Epoch 697Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.514108 \n",
            "\n",
            "Epoch 698\n",
            "-------------------------------\n",
            "End of Epoch 698Train : \n",
            " Accuracy = 81.1, Loss =  0.533805\n",
            "End of Epoch 698Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.495930 \n",
            "\n",
            "Epoch 699\n",
            "-------------------------------\n",
            "End of Epoch 699Train : \n",
            " Accuracy = 80.7, Loss =  0.548361\n",
            "End of Epoch 699Test : \n",
            " Accuracy = 69.8, Loss =  0.924118\n",
            "Loss over all Training data : 0.573548 \n",
            "\n",
            "Epoch 700\n",
            "-------------------------------\n",
            "End of Epoch 700Train : \n",
            " Accuracy = 75.5, Loss =  0.687888\n",
            "Loss over all Test data : 0.922598 \n",
            "\n",
            "Loss over all Training data : 0.608113 \n",
            "\n",
            "End of Epoch 00700Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.608128 \n",
            "\n",
            "Epoch 701\n",
            "-------------------------------\n",
            "End of Epoch 701Train : \n",
            " Accuracy = 78.6, Loss =  0.600292\n",
            "End of Epoch 701Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.553682 \n",
            "\n",
            "Epoch 702\n",
            "-------------------------------\n",
            "End of Epoch 702Train : \n",
            " Accuracy = 77.5, Loss =  0.631600\n",
            "End of Epoch 702Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.582694 \n",
            "\n",
            "Epoch 703\n",
            "-------------------------------\n",
            "End of Epoch 703Train : \n",
            " Accuracy = 77.6, Loss =  0.630619\n",
            "End of Epoch 703Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.620863 \n",
            "\n",
            "Epoch 704\n",
            "-------------------------------\n",
            "End of Epoch 704Train : \n",
            " Accuracy = 76.9, Loss =  0.646708\n",
            "End of Epoch 704Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.606016 \n",
            "\n",
            "Epoch 705\n",
            "-------------------------------\n",
            "End of Epoch 705Train : \n",
            " Accuracy = 72.0, Loss =  0.789681\n",
            "End of Epoch 705Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 1.025287 \n",
            "\n",
            "Epoch 706\n",
            "-------------------------------\n",
            "End of Epoch 706Train : \n",
            " Accuracy = 68.8, Loss =  0.878476\n",
            "End of Epoch 706Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.782702 \n",
            "\n",
            "Epoch 707\n",
            "-------------------------------\n",
            "End of Epoch 707Train : \n",
            " Accuracy = 71.2, Loss =  0.809456\n",
            "End of Epoch 707Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.756626 \n",
            "\n",
            "Epoch 708\n",
            "-------------------------------\n",
            "End of Epoch 708Train : \n",
            " Accuracy = 75.0, Loss =  0.703144\n",
            "End of Epoch 708Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.707433 \n",
            "\n",
            "Epoch 709\n",
            "-------------------------------\n",
            "End of Epoch 709Train : \n",
            " Accuracy = 75.1, Loss =  0.698301\n",
            "End of Epoch 709Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.782791 \n",
            "\n",
            "Epoch 710\n",
            "-------------------------------\n",
            "End of Epoch 710Train : \n",
            " Accuracy = 73.0, Loss =  0.759512\n",
            "End of Epoch 710Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.654553 \n",
            "\n",
            "Epoch 711\n",
            "-------------------------------\n",
            "End of Epoch 711Train : \n",
            " Accuracy = 77.1, Loss =  0.643052\n",
            "End of Epoch 711Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.611136 \n",
            "\n",
            "Epoch 712\n",
            "-------------------------------\n",
            "End of Epoch 712Train : \n",
            " Accuracy = 75.0, Loss =  0.698047\n",
            "End of Epoch 712Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.699626 \n",
            "\n",
            "Epoch 713\n",
            "-------------------------------\n",
            "End of Epoch 713Train : \n",
            " Accuracy = 77.0, Loss =  0.646625\n",
            "End of Epoch 713Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.615437 \n",
            "\n",
            "Epoch 714\n",
            "-------------------------------\n",
            "End of Epoch 714Train : \n",
            " Accuracy = 77.1, Loss =  0.644616\n",
            "End of Epoch 714Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.572397 \n",
            "\n",
            "Epoch 715\n",
            "-------------------------------\n",
            "End of Epoch 715Train : \n",
            " Accuracy = 79.3, Loss =  0.578019\n",
            "End of Epoch 715Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.551700 \n",
            "\n",
            "Epoch 716\n",
            "-------------------------------\n",
            "End of Epoch 716Train : \n",
            " Accuracy = 79.4, Loss =  0.580084\n",
            "End of Epoch 716Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.681173 \n",
            "\n",
            "Epoch 717\n",
            "-------------------------------\n",
            "End of Epoch 717Train : \n",
            " Accuracy = 75.1, Loss =  0.707784\n",
            "End of Epoch 717Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.650736 \n",
            "\n",
            "Epoch 718\n",
            "-------------------------------\n",
            "End of Epoch 718Train : \n",
            " Accuracy = 77.0, Loss =  0.649271\n",
            "End of Epoch 718Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.635726 \n",
            "\n",
            "Epoch 719\n",
            "-------------------------------\n",
            "End of Epoch 719Train : \n",
            " Accuracy = 75.8, Loss =  0.681871\n",
            "End of Epoch 719Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.656443 \n",
            "\n",
            "Epoch 720\n",
            "-------------------------------\n",
            "End of Epoch 720Train : \n",
            " Accuracy = 75.1, Loss =  0.696795\n",
            "End of Epoch 720Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.635340 \n",
            "\n",
            "Epoch 721\n",
            "-------------------------------\n",
            "End of Epoch 721Train : \n",
            " Accuracy = 77.0, Loss =  0.644993\n",
            "End of Epoch 721Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.591417 \n",
            "\n",
            "Epoch 722\n",
            "-------------------------------\n",
            "End of Epoch 722Train : \n",
            " Accuracy = 78.3, Loss =  0.611417\n",
            "End of Epoch 722Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.598651 \n",
            "\n",
            "Epoch 723\n",
            "-------------------------------\n",
            "End of Epoch 723Train : \n",
            " Accuracy = 77.6, Loss =  0.629784\n",
            "End of Epoch 723Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.614662 \n",
            "\n",
            "Epoch 724\n",
            "-------------------------------\n",
            "End of Epoch 724Train : \n",
            " Accuracy = 76.5, Loss =  0.665967\n",
            "End of Epoch 724Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.728077 \n",
            "\n",
            "Epoch 725\n",
            "-------------------------------\n",
            "End of Epoch 725Train : \n",
            " Accuracy = 75.2, Loss =  0.702193\n",
            "End of Epoch 725Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.610777 \n",
            "\n",
            "Epoch 726\n",
            "-------------------------------\n",
            "End of Epoch 726Train : \n",
            " Accuracy = 78.7, Loss =  0.601964\n",
            "End of Epoch 726Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.573370 \n",
            "\n",
            "Epoch 727\n",
            "-------------------------------\n",
            "End of Epoch 727Train : \n",
            " Accuracy = 76.9, Loss =  0.652972\n",
            "End of Epoch 727Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.588339 \n",
            "\n",
            "Epoch 728\n",
            "-------------------------------\n",
            "End of Epoch 728Train : \n",
            " Accuracy = 78.5, Loss =  0.602248\n",
            "End of Epoch 728Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.547804 \n",
            "\n",
            "Epoch 729\n",
            "-------------------------------\n",
            "End of Epoch 729Train : \n",
            " Accuracy = 77.4, Loss =  0.634753\n",
            "End of Epoch 729Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.625350 \n",
            "\n",
            "Epoch 730\n",
            "-------------------------------\n",
            "End of Epoch 730Train : \n",
            " Accuracy = 77.4, Loss =  0.636656\n",
            "End of Epoch 730Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.615217 \n",
            "\n",
            "Epoch 731\n",
            "-------------------------------\n",
            "End of Epoch 731Train : \n",
            " Accuracy = 77.6, Loss =  0.628457\n",
            "End of Epoch 731Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.644858 \n",
            "\n",
            "Epoch 732\n",
            "-------------------------------\n",
            "End of Epoch 732Train : \n",
            " Accuracy = 76.6, Loss =  0.655573\n",
            "End of Epoch 732Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.691095 \n",
            "\n",
            "Epoch 733\n",
            "-------------------------------\n",
            "End of Epoch 733Train : \n",
            " Accuracy = 74.5, Loss =  0.712342\n",
            "End of Epoch 733Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.610545 \n",
            "\n",
            "Epoch 734\n",
            "-------------------------------\n",
            "End of Epoch 734Train : \n",
            " Accuracy = 75.0, Loss =  0.704716\n",
            "End of Epoch 734Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.840296 \n",
            "\n",
            "Epoch 735\n",
            "-------------------------------\n",
            "End of Epoch 735Train : \n",
            " Accuracy = 74.5, Loss =  0.723412\n",
            "End of Epoch 735Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.592405 \n",
            "\n",
            "Epoch 736\n",
            "-------------------------------\n",
            "End of Epoch 736Train : \n",
            " Accuracy = 77.6, Loss =  0.629394\n",
            "End of Epoch 736Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.662119 \n",
            "\n",
            "Epoch 737\n",
            "-------------------------------\n",
            "End of Epoch 737Train : \n",
            " Accuracy = 76.5, Loss =  0.657886\n",
            "End of Epoch 737Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.571308 \n",
            "\n",
            "Epoch 738\n",
            "-------------------------------\n",
            "End of Epoch 738Train : \n",
            " Accuracy = 78.7, Loss =  0.603966\n",
            "End of Epoch 738Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.570129 \n",
            "\n",
            "Epoch 739\n",
            "-------------------------------\n",
            "End of Epoch 739Train : \n",
            " Accuracy = 79.7, Loss =  0.572010\n",
            "End of Epoch 739Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.579858 \n",
            "\n",
            "Epoch 740\n",
            "-------------------------------\n",
            "End of Epoch 740Train : \n",
            " Accuracy = 80.5, Loss =  0.553902\n",
            "End of Epoch 740Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.525436 \n",
            "\n",
            "Epoch 741\n",
            "-------------------------------\n",
            "End of Epoch 741Train : \n",
            " Accuracy = 80.9, Loss =  0.539113\n",
            "End of Epoch 741Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.499744 \n",
            "\n",
            "Epoch 742\n",
            "-------------------------------\n",
            "End of Epoch 742Train : \n",
            " Accuracy = 81.1, Loss =  0.528349\n",
            "End of Epoch 742Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.495141 \n",
            "\n",
            "Epoch 743\n",
            "-------------------------------\n",
            "End of Epoch 743Train : \n",
            " Accuracy = 81.5, Loss =  0.520191\n",
            "End of Epoch 743Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.505657 \n",
            "\n",
            "Epoch 744\n",
            "-------------------------------\n",
            "End of Epoch 744Train : \n",
            " Accuracy = 81.1, Loss =  0.530468\n",
            "End of Epoch 744Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.500661 \n",
            "\n",
            "Epoch 745\n",
            "-------------------------------\n",
            "End of Epoch 745Train : \n",
            " Accuracy = 78.6, Loss =  0.605372\n",
            "End of Epoch 745Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.539751 \n",
            "\n",
            "Epoch 746\n",
            "-------------------------------\n",
            "End of Epoch 746Train : \n",
            " Accuracy = 79.0, Loss =  0.588313\n",
            "End of Epoch 746Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.514582 \n",
            "\n",
            "Epoch 747\n",
            "-------------------------------\n",
            "End of Epoch 747Train : \n",
            " Accuracy = 79.8, Loss =  0.559987\n",
            "End of Epoch 747Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.500327 \n",
            "\n",
            "Epoch 748\n",
            "-------------------------------\n",
            "End of Epoch 748Train : \n",
            " Accuracy = 81.8, Loss =  0.510780\n",
            "End of Epoch 748Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.497220 \n",
            "\n",
            "Epoch 749\n",
            "-------------------------------\n",
            "End of Epoch 749Train : \n",
            " Accuracy = 82.2, Loss =  0.504946\n",
            "End of Epoch 749Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.470023 \n",
            "\n",
            "Epoch 750\n",
            "-------------------------------\n",
            "End of Epoch 750Train : \n",
            " Accuracy = 82.0, Loss =  0.504694\n",
            "End of Epoch 750Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.474547 \n",
            "\n",
            "Epoch 751\n",
            "-------------------------------\n",
            "End of Epoch 751Train : \n",
            " Accuracy = 82.3, Loss =  0.493567\n",
            "End of Epoch 751Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.481859 \n",
            "\n",
            "Epoch 752\n",
            "-------------------------------\n",
            "End of Epoch 752Train : \n",
            " Accuracy = 80.6, Loss =  0.544137\n",
            "End of Epoch 752Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.517225 \n",
            "\n",
            "Epoch 753\n",
            "-------------------------------\n",
            "End of Epoch 753Train : \n",
            " Accuracy = 79.3, Loss =  0.583389\n",
            "End of Epoch 753Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.599559 \n",
            "\n",
            "Epoch 754\n",
            "-------------------------------\n",
            "End of Epoch 754Train : \n",
            " Accuracy = 78.5, Loss =  0.610028\n",
            "End of Epoch 754Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.532693 \n",
            "\n",
            "Epoch 755\n",
            "-------------------------------\n",
            "End of Epoch 755Train : \n",
            " Accuracy = 79.4, Loss =  0.582731\n",
            "End of Epoch 755Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.547264 \n",
            "\n",
            "Epoch 756\n",
            "-------------------------------\n",
            "End of Epoch 756Train : \n",
            " Accuracy = 79.2, Loss =  0.583450\n",
            "End of Epoch 756Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.570735 \n",
            "\n",
            "Epoch 757\n",
            "-------------------------------\n",
            "End of Epoch 757Train : \n",
            " Accuracy = 79.4, Loss =  0.571401\n",
            "End of Epoch 757Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.665405 \n",
            "\n",
            "Epoch 758\n",
            "-------------------------------\n",
            "End of Epoch 758Train : \n",
            " Accuracy = 77.1, Loss =  0.639067\n",
            "End of Epoch 758Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.574472 \n",
            "\n",
            "Epoch 759\n",
            "-------------------------------\n",
            "End of Epoch 759Train : \n",
            " Accuracy = 79.5, Loss =  0.576469\n",
            "End of Epoch 759Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.640040 \n",
            "\n",
            "Epoch 760\n",
            "-------------------------------\n",
            "End of Epoch 760Train : \n",
            " Accuracy = 77.9, Loss =  0.615354\n",
            "End of Epoch 760Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.567026 \n",
            "\n",
            "Epoch 761\n",
            "-------------------------------\n",
            "End of Epoch 761Train : \n",
            " Accuracy = 76.8, Loss =  0.645765\n",
            "End of Epoch 761Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.664355 \n",
            "\n",
            "Epoch 762\n",
            "-------------------------------\n",
            "End of Epoch 762Train : \n",
            " Accuracy = 77.4, Loss =  0.637122\n",
            "End of Epoch 762Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.596983 \n",
            "\n",
            "Epoch 763\n",
            "-------------------------------\n",
            "End of Epoch 763Train : \n",
            " Accuracy = 77.4, Loss =  0.637486\n",
            "End of Epoch 763Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.581814 \n",
            "\n",
            "Epoch 764\n",
            "-------------------------------\n",
            "End of Epoch 764Train : \n",
            " Accuracy = 78.5, Loss =  0.602488\n",
            "End of Epoch 764Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.551138 \n",
            "\n",
            "Epoch 765\n",
            "-------------------------------\n",
            "End of Epoch 765Train : \n",
            " Accuracy = 79.8, Loss =  0.564443\n",
            "End of Epoch 765Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.596132 \n",
            "\n",
            "Epoch 766\n",
            "-------------------------------\n",
            "End of Epoch 766Train : \n",
            " Accuracy = 77.7, Loss =  0.621971\n",
            "End of Epoch 766Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.521799 \n",
            "\n",
            "Epoch 767\n",
            "-------------------------------\n",
            "End of Epoch 767Train : \n",
            " Accuracy = 80.4, Loss =  0.548525\n",
            "End of Epoch 767Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.512974 \n",
            "\n",
            "Epoch 768\n",
            "-------------------------------\n",
            "End of Epoch 768Train : \n",
            " Accuracy = 80.3, Loss =  0.545668\n",
            "End of Epoch 768Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.520871 \n",
            "\n",
            "Epoch 769\n",
            "-------------------------------\n",
            "End of Epoch 769Train : \n",
            " Accuracy = 79.7, Loss =  0.569280\n",
            "End of Epoch 769Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.562063 \n",
            "\n",
            "Epoch 770\n",
            "-------------------------------\n",
            "End of Epoch 770Train : \n",
            " Accuracy = 78.8, Loss =  0.596943\n",
            "End of Epoch 770Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.551013 \n",
            "\n",
            "Epoch 771\n",
            "-------------------------------\n",
            "End of Epoch 771Train : \n",
            " Accuracy = 81.1, Loss =  0.528283\n",
            "End of Epoch 771Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.500368 \n",
            "\n",
            "Epoch 772\n",
            "-------------------------------\n",
            "End of Epoch 772Train : \n",
            " Accuracy = 81.4, Loss =  0.522938\n",
            "End of Epoch 772Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.493563 \n",
            "\n",
            "Epoch 773\n",
            "-------------------------------\n",
            "End of Epoch 773Train : \n",
            " Accuracy = 80.8, Loss =  0.535606\n",
            "End of Epoch 773Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.525250 \n",
            "\n",
            "Epoch 774\n",
            "-------------------------------\n",
            "End of Epoch 774Train : \n",
            " Accuracy = 80.9, Loss =  0.539027\n",
            "End of Epoch 774Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.538401 \n",
            "\n",
            "Epoch 775\n",
            "-------------------------------\n",
            "End of Epoch 775Train : \n",
            " Accuracy = 76.2, Loss =  0.669324\n",
            "End of Epoch 775Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.717115 \n",
            "\n",
            "Epoch 776\n",
            "-------------------------------\n",
            "End of Epoch 776Train : \n",
            " Accuracy = 75.9, Loss =  0.675157\n",
            "End of Epoch 776Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.604449 \n",
            "\n",
            "Epoch 777\n",
            "-------------------------------\n",
            "End of Epoch 777Train : \n",
            " Accuracy = 79.7, Loss =  0.574497\n",
            "End of Epoch 777Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.531569 \n",
            "\n",
            "Epoch 778\n",
            "-------------------------------\n",
            "End of Epoch 778Train : \n",
            " Accuracy = 79.6, Loss =  0.573963\n",
            "End of Epoch 778Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.556946 \n",
            "\n",
            "Epoch 779\n",
            "-------------------------------\n",
            "End of Epoch 779Train : \n",
            " Accuracy = 78.9, Loss =  0.593900\n",
            "End of Epoch 779Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.549561 \n",
            "\n",
            "Epoch 780\n",
            "-------------------------------\n",
            "End of Epoch 780Train : \n",
            " Accuracy = 79.1, Loss =  0.586202\n",
            "End of Epoch 780Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.560866 \n",
            "\n",
            "Epoch 781\n",
            "-------------------------------\n",
            "End of Epoch 781Train : \n",
            " Accuracy = 79.6, Loss =  0.571837\n",
            "End of Epoch 781Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.531237 \n",
            "\n",
            "Epoch 782\n",
            "-------------------------------\n",
            "End of Epoch 782Train : \n",
            " Accuracy = 80.2, Loss =  0.552239\n",
            "End of Epoch 782Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.537838 \n",
            "\n",
            "Epoch 783\n",
            "-------------------------------\n",
            "End of Epoch 783Train : \n",
            " Accuracy = 78.9, Loss =  0.583544\n",
            "End of Epoch 783Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.565603 \n",
            "\n",
            "Epoch 784\n",
            "-------------------------------\n",
            "End of Epoch 784Train : \n",
            " Accuracy = 79.9, Loss =  0.564456\n",
            "End of Epoch 784Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.553784 \n",
            "\n",
            "Epoch 785\n",
            "-------------------------------\n",
            "End of Epoch 785Train : \n",
            " Accuracy = 79.1, Loss =  0.587054\n",
            "End of Epoch 785Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.543262 \n",
            "\n",
            "Epoch 786\n",
            "-------------------------------\n",
            "End of Epoch 786Train : \n",
            " Accuracy = 80.2, Loss =  0.556243\n",
            "End of Epoch 786Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.571886 \n",
            "\n",
            "Epoch 787\n",
            "-------------------------------\n",
            "End of Epoch 787Train : \n",
            " Accuracy = 80.0, Loss =  0.557458\n",
            "End of Epoch 787Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.525043 \n",
            "\n",
            "Epoch 788\n",
            "-------------------------------\n",
            "End of Epoch 788Train : \n",
            " Accuracy = 78.3, Loss =  0.606902\n",
            "End of Epoch 788Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.644135 \n",
            "\n",
            "Epoch 789\n",
            "-------------------------------\n",
            "End of Epoch 789Train : \n",
            " Accuracy = 77.5, Loss =  0.634898\n",
            "End of Epoch 789Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.595462 \n",
            "\n",
            "Epoch 790\n",
            "-------------------------------\n",
            "End of Epoch 790Train : \n",
            " Accuracy = 77.5, Loss =  0.634507\n",
            "End of Epoch 790Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.523301 \n",
            "\n",
            "Epoch 791\n",
            "-------------------------------\n",
            "End of Epoch 791Train : \n",
            " Accuracy = 80.9, Loss =  0.540040\n",
            "End of Epoch 791Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.503546 \n",
            "\n",
            "Epoch 792\n",
            "-------------------------------\n",
            "End of Epoch 792Train : \n",
            " Accuracy = 81.2, Loss =  0.523904\n",
            "End of Epoch 792Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.507125 \n",
            "\n",
            "Epoch 793\n",
            "-------------------------------\n",
            "End of Epoch 793Train : \n",
            " Accuracy = 79.5, Loss =  0.575950\n",
            "End of Epoch 793Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.635982 \n",
            "\n",
            "Epoch 794\n",
            "-------------------------------\n",
            "End of Epoch 794Train : \n",
            " Accuracy = 79.5, Loss =  0.568379\n",
            "End of Epoch 794Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.519173 \n",
            "\n",
            "Epoch 795\n",
            "-------------------------------\n",
            "End of Epoch 795Train : \n",
            " Accuracy = 81.6, Loss =  0.517414\n",
            "End of Epoch 795Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.493217 \n",
            "\n",
            "Epoch 796\n",
            "-------------------------------\n",
            "End of Epoch 796Train : \n",
            " Accuracy = 81.4, Loss =  0.518550\n",
            "End of Epoch 796Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.486441 \n",
            "\n",
            "Epoch 797\n",
            "-------------------------------\n",
            "End of Epoch 797Train : \n",
            " Accuracy = 82.0, Loss =  0.496966\n",
            "End of Epoch 797Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.464016 \n",
            "\n",
            "Epoch 798\n",
            "-------------------------------\n",
            "End of Epoch 798Train : \n",
            " Accuracy = 81.8, Loss =  0.505665\n",
            "End of Epoch 798Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.520153 \n",
            "\n",
            "Epoch 799\n",
            "-------------------------------\n",
            "End of Epoch 799Train : \n",
            " Accuracy = 81.3, Loss =  0.519942\n",
            "End of Epoch 799Test : \n",
            " Accuracy = 70.0, Loss =  0.922598\n",
            "Loss over all Training data : 0.475950 \n",
            "\n",
            "Epoch 800\n",
            "-------------------------------\n",
            "End of Epoch 800Train : \n",
            " Accuracy = 82.1, Loss =  0.498707\n",
            "Loss over all Test data : 0.902493 \n",
            "\n",
            "Loss over all Training data : 0.456844 \n",
            "\n",
            "End of Epoch 00800Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.456913 \n",
            "\n",
            "Epoch 801\n",
            "-------------------------------\n",
            "End of Epoch 801Train : \n",
            " Accuracy = 81.8, Loss =  0.505839\n",
            "End of Epoch 801Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.521433 \n",
            "\n",
            "Epoch 802\n",
            "-------------------------------\n",
            "End of Epoch 802Train : \n",
            " Accuracy = 81.3, Loss =  0.519648\n",
            "End of Epoch 802Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.491524 \n",
            "\n",
            "Epoch 803\n",
            "-------------------------------\n",
            "End of Epoch 803Train : \n",
            " Accuracy = 81.0, Loss =  0.530077\n",
            "End of Epoch 803Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.548508 \n",
            "\n",
            "Epoch 804\n",
            "-------------------------------\n",
            "End of Epoch 804Train : \n",
            " Accuracy = 82.1, Loss =  0.501383\n",
            "End of Epoch 804Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.465834 \n",
            "\n",
            "Epoch 805\n",
            "-------------------------------\n",
            "End of Epoch 805Train : \n",
            " Accuracy = 82.7, Loss =  0.483325\n",
            "End of Epoch 805Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.462143 \n",
            "\n",
            "Epoch 806\n",
            "-------------------------------\n",
            "End of Epoch 806Train : \n",
            " Accuracy = 81.9, Loss =  0.503303\n",
            "End of Epoch 806Test : \n",
            " Accuracy = 71.3, Loss =  0.902493\n",
            "Loss over all Training data : 0.525686 \n",
            "\n",
            "Epoch 807\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-ab5870037ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                                                                               \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                                                                               \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                                                                                               epsilon = None)\n\u001b[0m\u001b[1;32m    105\u001b[0m           \u001b[0;31m# state_collections.append((state_info_5_runs,predictions_5_runs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m           \u001b[0;31m# state_collections[0] = (state_info_5_runs,predictions_5_runs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-5603bc5e9e39>\u001b[0m in \u001b[0;36mrun_npf_npv_model\u001b[0;34m(train_dl_npf, train_dl_npv, test_dl_npf, test_dl_npv, mini_dl, protocol, learning_type, act_type, inp_dim, out_dim, n_h_l, n_n, n_runs, n_epochs, lr, bias, epsilon)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                                                                              \u001b[0mtest_dl_npv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dl_npv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                                                                              \u001b[0mmini_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                                                                                              state_info = state_info)\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstep_stop_point\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-c3d2b2968cce>\u001b[0m in \u001b[0;36mtrain_decoupled\u001b[0;34m(X1_dataloader, X2_dataloader, npf_model, npv_model, loss_fn, optimizer, act_type, test_dl_npf, test_dl_npv, mini_dl, state_info)\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "x_axis_global = []\n",
        "\n",
        "train_loss_stopping_criteria = .00000001\n",
        "\n",
        "# n_neurons_list = [8*32*32, 8*32*32, 8*32*32, 64]\n",
        "n_neurons_list = [64, 64]\n",
        "# n_neurons_list = [256]\n",
        "# n_epochs = 100\n",
        "n_cnn_layers = 5\n",
        "n_filters = 26\n",
        "n_hidden_layers,  n_neurons, n_runs =  6, 64, 1 #n_hidden_layers = n_cnn_layers + n_dense_layers; n_neurons is for dense layer\n",
        "inp_dim, out_dim = inp_dim, n_classes if is_classification else 1\n",
        "lr = 2e-4\n",
        "save_result, download_result = True, False\n",
        "'''\n",
        "Available Protocls : MLP, DGN, DLGN, DLGN-SF, DGN-DLGN-SF\n",
        "Available learning types : \"BOTH\",  'ONPV', 'ONPF'\n",
        "Available input types : 'PWL', 'PWC'\n",
        "'''\n",
        "# protocols = ['MLP', \"DGN\", 'DLGN', 'DLGN-SF','DGN-DLGN-SF']\n",
        "\n",
        "# protocols = ['DGN', 'DLGN', 'DLGN-SF']\n",
        "# learning_types = ['ONPV', 'ONPF']\n",
        "# input_types = ['PWL', 'PWC']\n",
        "\n",
        "protocols = [ 'DLGN' ]\n",
        "learning_types = ['BOTH'] \n",
        "input_types = ['PWC']\n",
        "# protocols = [ 'DLGN']\n",
        "# learning_types = [ 'BOTH', 'ONPV', 'BOTH_ONPV'] \n",
        "# input_types = ['PWC']\n",
        "# epsilon = .075\n",
        "n_points_vicinity = 2000\n",
        "pos_neg_ratio_criteria = 4\n",
        "\n",
        "bias = True\n",
        "plot_hyperplanes_figures = False\n",
        "set_seed(seed = seed)\n",
        "\n",
        "for protocol in protocols:\n",
        "  for learning_type in learning_types:\n",
        "      if learning_type == 'ONPF' or learning_type == 'BOTH':\n",
        "        act_types = ['soft']\n",
        "      elif learning_type == 'BOTH_ONPV':\n",
        "        act_types = ['soft_soft']\n",
        "      elif learning_type == 'ONPV':\n",
        "        act_types = [ 'soft']\n",
        "      if learning_type == 'BOTH':\n",
        "        n_epochs = 2000\n",
        "        step_stop_point = 2\n",
        "        step_stepsize = 200\n",
        "        epoch_stop_point = 10\n",
        "        epoch_stepsize = 10\n",
        "        start_point = get_start_point(epoch_stop_point, epoch_stepsize)\n",
        "      else:\n",
        "        n_epochs = 2000\n",
        "        step_stop_point = 2\n",
        "        step_stepsize = 200\n",
        "        epoch_stop_point = 10\n",
        "        epoch_stepsize = 10\n",
        "        start_point = get_start_point(epoch_stop_point, epoch_stepsize)\n",
        "\n",
        "      for input_type in input_types:\n",
        "        for act_type in act_types:\n",
        "          set_seed(seed = seed)\n",
        "          \n",
        "          if protocol == 'MLP':\n",
        "            act_type = 'hard'\n",
        "            learning_type = 'BOTH'\n",
        "            hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs = run_mlp_model(train_dataloader= train_dataloader,\n",
        "                                                                                            test_dataloader = test_dataloader,\n",
        "                                                                                            mini_dl = mini_dl,\n",
        "                                                                                            inp_dim = inp_dim,\n",
        "                                                                                            out_dim = out_dim,\n",
        "                                                                                            n_h_l = n_hidden_layers,\n",
        "                                                                                            n_n = n_neurons,\n",
        "                                                                                            n_runs = n_runs, n_epochs = n_epochs, bias = bias)\n",
        "          else:\n",
        "            if input_type == 'PWL':\n",
        "              train_dl_npv = train_dataloader\n",
        "              test_dl_npv = test_dataloader\n",
        "            elif input_type == 'PWC':\n",
        "              train_dl_npv = train_dl_PWC\n",
        "              test_dl_npv = test_dl_PWC\n",
        "          \n",
        "          \n",
        "\n",
        "            hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs, weights_biases_all_runs = run_npf_npv_model(train_dl_npf = train_dataloader,\n",
        "                                                                                              train_dl_npv = train_dl_npv,\n",
        "                                                                                              test_dl_npf = test_dataloader,\n",
        "                                                                                              test_dl_npv = test_dl_npv,\n",
        "                                                                                              mini_dl = mini_dl,\n",
        "                                                                                              protocol = protocol,\n",
        "                                                                                              learning_type = learning_type,\n",
        "                                                                                              act_type = act_type,\n",
        "                                                                                              inp_dim = inp_dim,\n",
        "                                                                                              out_dim = out_dim,\n",
        "                                                                                              n_h_l = n_hidden_layers,\n",
        "                                                                                              n_n = n_neurons,\n",
        "                                                                                              n_runs = n_runs,\n",
        "                                                                                              n_epochs = n_epochs,\n",
        "                                                                                              lr = lr,\n",
        "                                                                                              bias = bias,\n",
        "                                                                                              epsilon = None)\n",
        "          # state_collections.append((state_info_5_runs,predictions_5_runs))\n",
        "          # state_collections[0] = (state_info_5_runs,predictions_5_runs)\n",
        "          # kernel_5_runs = build_kernels(protocol, hidden_layer_outs_5_runs)\n",
        "          # for kernel_name in ['K2', 'K3','K']:\n",
        "          #   plot_kernel_values_stepwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "          #   plot_kernel_values_epochwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "\n",
        "          # plot_stepwise_activations(hidden_layer_no =2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "          # plot_epochwise_activations(hidden_layer_no = 2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "          # x_axis_global = get_x_axis_global(state_info_5_runs, x_axis_global)\n",
        "          # plot_stepwise_loss(state_info_5_runs)\n",
        "          plot_epochwise_loss(state_info_5_runs)\n",
        "          # plot_stepwise_acc(state_info_5_runs)\n",
        "          plot_epochwise_acc(state_info_5_runs)\n",
        "         \n",
        "          # plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)\n",
        "          # plot_hyperplanes(weights_biases_all_runs, state_info_5_runs, n_rows = 8 , n_cols = 2)\n",
        "          # plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs, n_rows = 8, n_cols = 2)\n",
        "          # for epsilon in [.6]:\n",
        "            \n",
        "          #   posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs, epsilon, train_data_curr, train_labels_curr)\n",
        "          # #   plot_hyp_posneg(posneg_pairs_all_runs,n_neurons_list, epsilon)\n",
        "          # #   plot_hyp_posneg_ratio(posneg_pairs_all_runs,n_neurons_list, epsilon)\n",
        "          #   plot_hyp_local_usefulness(local_usefulness_all_runs,n_neurons_list, epsilon)\n",
        "\n",
        "            # plot_hyperplanes_temp(weights_biases_all_runs, state_info_5_runs)\n",
        "            # plot_hyperplanes_all_temp(weights_biases_all_runs, state_info_5_runs)\n",
        "#-----------\n",
        "            #   plot_hyp_posneg(hyp_5_runs)\n",
        "            # plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)\n",
        "            # plot_predictions(21, predictions_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)\n",
        "            # #Saving results of each protocol\n",
        "#------------\n",
        "          if save_result:\n",
        "            # file_name = f'{protocol}_{learning_type}_{input_type}_{act_type}_{n_hidden_layers}_{n_neurons}_n={n_data}_modes={num_modes}_d={d}_dsphere'\n",
        "            file_name = f'{protocol}_{learning_type}_{input_type}_{n_hidden_layers}_{n_neurons}_{dataset_name}'\n",
        "\n",
        "            save_results(file_name)\n",
        "\n",
        "  # #Download all protocols result\n",
        "if download_result:\n",
        "  download_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)"
      ],
      "metadata": {
        "id": "4qNNTQI7-5VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "etTjvOENWuTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IG0cTRzVWtuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqahzG5Varyh"
      },
      "outputs": [],
      "source": [
        "# plot_predictions(None, state_collections[0][1], state_collections[0][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[1][1], state_collections[1][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[2][1], state_collections[2][0],for_all_inputs = True, save_fig = False, show_fig = True)\n",
        "# plot_predictions(None, state_collections[3][1], state_collections[3][0],for_all_inputs = True, save_fig = False, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTX16i5onSao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ufiHOUCCkOR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EQQZ38ynSX5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TqiKaZjnSUz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUBQoBSSnw5K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vyrGKmfnw2T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLwDGDOlY1aq"
      },
      "outputs": [],
      "source": [
        "# !rm -r *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg-hIvqHOdEo"
      },
      "outputs": [],
      "source": [
        "# plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEpAJdblLniI"
      },
      "outputs": [],
      "source": [
        "# state_info_5_runs[0][5][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A0Wn8wBrLwY"
      },
      "outputs": [],
      "source": [
        "# plot_kernels(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w1qEhYoexl1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mAs03p_gm_x"
      },
      "outputs": [],
      "source": [
        "# plot_hyperplanes_all(weights_biases_all_runs, state_info_5_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zncSiZNFK12l"
      },
      "outputs": [],
      "source": [
        "# file_name = f'{protocol}_{learning_type}_{input_type}_{n_hidden_layers}_{n_neurons}_{dataset_name}'\n",
        "# save_results(file_name)\n",
        "# download_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NYTooMfn1fG"
      },
      "source": [
        "#Testing arena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKPTwirGuwFJ"
      },
      "outputs": [],
      "source": [
        "for r in range(len(hidden_layer_outs_5_runs)):\n",
        "  for e in range(len(hidden_layer_outs_5_runs[r])):\n",
        "    for s in range(len(hidden_layer_outs_5_runs[r][e])):\n",
        "      print(hidden_layer_outs_5_runs[r][e][s][1].shape, state_info_5_runs[r][e][s][\"epoch\"], state_info_5_runs[r][e][s][\"step\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQR7dYUfuwCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3cYcHIMnxMF"
      },
      "source": [
        "#Commented codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5PoOBQMbvIi"
      },
      "outputs": [],
      "source": [
        "# plot_hyp_posneg(posneg_pairs_all_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doEdNkMXL5cA"
      },
      "outputs": [],
      "source": [
        "# posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs, epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEd9d9MCBJvY"
      },
      "outputs": [],
      "source": [
        "# plot_hyp_posneg(posneg_pairs_all_runs)\n",
        "# plot_hyp_posneg_ratio(posneg_pairs_all_runs)\n",
        "# plot_hyp_local_usefulness(local_usefulness_all_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBpUB9COMTn8"
      },
      "outputs": [],
      "source": [
        "# posneg_pairs_all_runs, local_usefulness_all_runs = unknown(weights_biases_all_runs)\n",
        "# plot_hyp_posneg(posneg_pairs_all_runs)\n",
        "# plot_hyp_posneg_ratio(posneg_pairs_all_runs)\n",
        "# plot_hyp_local_usefulness(local_usefulness_all_runs)\n",
        "\n",
        "# plot_hyperplanes_temp(weights_biases_all_runs, state_info_5_runs)\n",
        "# plot_hyperplanes_all_temp(weights_biases_all_runs, state_info_5_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDylYxC7L0jj"
      },
      "outputs": [],
      "source": [
        "# !rm -r *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPPvW2UeaSQm"
      },
      "outputs": [],
      "source": [
        "# class NPFNeuralNetwork(nn.Module):\n",
        "#     def __init__(self, n_hidden_layers, n_neurons, protocol_type, bias):\n",
        "#         super(NPFNeuralNetwork, self).__init__()\n",
        "#         self.protocol_type = protocol_type\n",
        "#         self.n_hidden_layers = n_hidden_layers\n",
        "#         self.n_neurons = n_neurons\n",
        "#         self.bias = bias\n",
        "        \n",
        "#         self.layers = nn.ModuleList([])\n",
        "   \n",
        "#         #For DLGN-SF, DGN-DLGN-SF\n",
        "#         if self.protocol_type == \"DLGN-SF\":\n",
        "#           self.layers.append(nn.Linear(2,  self.n_neurons, bias = self.bias).to(device))\n",
        "#           for i in range(self.n_hidden_layers-1):\n",
        "#             self.layers.append(nn.Linear( 2,  self.n_neurons, bias = self.bias).to(device))\n",
        "            \n",
        "#         elif self.protocol_type == \"DGN-DLGN-SF\":\n",
        "#           self.layers.append(nn.Sequential(nn.Linear(2,  self.n_neurons, bias = self.bias).to(device),\n",
        "#                                             nn.ReLU(),\n",
        "#                                             nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "#                                             )\n",
        "#                             )\n",
        "          \n",
        "#           for i in range(self.n_hidden_layers-1):\n",
        "#             self.layers.append(nn.Sequential(\n",
        "#                                   nn.Linear( 2,  self.n_neurons, bias = self.bias).to(device),\n",
        "#                                   nn.ReLU(),\n",
        "#                                   nn.Linear(self.n_neurons,  self.n_neurons, bias = self.bias).to(device)\n",
        "#                               )\n",
        "#                         )\n",
        "#         #For DGN, DLGN\n",
        "#         else: \n",
        "#           self.layers.append(nn.Linear(2,  self.n_neurons, bias = self.bias).to(device))\n",
        "#           for i in range(self.n_hidden_layers-1):\n",
        "#             self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "        \n",
        "        \n",
        "        \n",
        "#         # self.layers.append(nn.Linear( self.n_neurons, 1, bias = False).to(device))\n",
        "  \n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         hidden_layer_outputs = []\n",
        "      \n",
        "#         if self.protocol_type == \"DGN\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             x = self.layers[i](x)\n",
        "#             out = torch.relu(x)\n",
        "#             hidden_layer_outputs.append(out)\n",
        "#             x = out\n",
        "#           #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "#         elif  self.protocol_type == \"DLGN\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             x = self.layers[i](x)\n",
        "#             out = x\n",
        "#             hidden_layer_outputs.append(out)\n",
        "#             x = out\n",
        "#           #x = self.layers[len(layers)-1](x)\n",
        "\n",
        "#         elif  self.protocol_type == \"DLGN-SF\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             out = self.layers[i](x)\n",
        "#             hidden_layer_outputs.append(out)\n",
        "#         elif  self.protocol_type == \"DGN-DLGN-SF\":\n",
        "#           for i in range(len(self.layers)):\n",
        "#             out = self.layers[i](x) \n",
        "#             hidden_layer_outputs.append(out)\n",
        "        \n",
        "\n",
        "#         return hidden_layer_outputs\n",
        "\n",
        "\n",
        "\n",
        "# class NPVNeuralNetwork(nn.Module):\n",
        "#     def __init__(self, n_hidden_layers, n_neurons, bias):\n",
        "#         super(NPVNeuralNetwork, self).__init__()\n",
        "  \n",
        "#         self.n_hidden_layers = n_hidden_layers\n",
        "#         self.n_neurons = n_neurons\n",
        "#         self.bias = bias\n",
        "#         self.layers = nn.ModuleList([nn.Linear(2,  self.n_neurons, bias = self.bias).to(device)])\n",
        "      \n",
        "#         for i in range(self.n_hidden_layers-1):\n",
        "#           self.layers.append(nn.Linear( self.n_neurons,  self.n_neurons, bias = self.bias).to(device))\n",
        "          \n",
        "#         self.layers.append(nn.Linear( self.n_neurons, 1, bias = self.bias).to(device))\n",
        "\n",
        "#         self.gate = Gate()\n",
        "      \n",
        "#     def forward(self, x,act_type, gating_mask):\n",
        "#         hidden_layer_outputs = []\n",
        "#         for i in range(len(self.layers)-1):\n",
        "#           x = self.layers[i](x)\n",
        "          \n",
        "#           out = self.gate(act_type, x, i, gating_mask)\n",
        "          \n",
        "#           hidden_layer_outputs.append(out)\n",
        "#           x = out\n",
        "#         x = self.layers[len(self.layers)-1](x)\n",
        "#         #print(\"debug 2\", x.shape, x)\n",
        "#         return x,  hidden_layer_outputs\n",
        "\n",
        "\n",
        "# def apply_gate(beta, idx, gating_mask):\n",
        "#   out = beta*(gating_mask[idx])\n",
        "  \n",
        "#   return out\n",
        "  \n",
        "# class Gate(nn.Module):\n",
        "#     def __init__(self, beta = 4):\n",
        "#         super(Gate,self).__init__()\n",
        "#         self.beta = beta\n",
        "\n",
        "#     def forward(self,act_type, x, idx, gating_mask):\n",
        "#       #Soft Relu\n",
        "#       if act_type == 'soft':\n",
        "#         return torch.mul(x,torch.sigmoid(apply_gate(self.beta, idx,gating_mask)))\n",
        "#       elif act_type == 'hard':\n",
        "#       #Hard Relu\n",
        "#         temp = torch.sign(gating_mask[idx])\n",
        "#         temp[temp <= 0] = 0\n",
        "#         return torch.mul(x,temp)\n",
        "\n",
        "\n",
        "\n",
        "# def train_decoupled(X1_dataloader,X2_dataloader, npf_model, npv_model, loss_fn, optimizer, dataloader_all = None, state_info = None):\n",
        "#     size = len(X1_dataloader.dataset)\n",
        "#     correct = 0\n",
        "#     state_info_over_batches = []\n",
        "#     hidden_layer_outs_over_batches = []\n",
        "#     predictions_over_batches = []\n",
        "\n",
        "#     if int(state_info['epoch']) <= 2:\n",
        "#       act_type = 'soft'\n",
        "#     elif int(state_info['epoch']) > 2:\n",
        "#       act_type = 'soft'\n",
        "#     else:\n",
        "#       print(\"Some error\")\n",
        "\n",
        "#     for batch, ((X1, y1), (X2,y2)) in enumerate(zip(X1_dataloader, X2_dataloader)):\n",
        "#         X1, y1 = X1.to(device), y1.to(device)\n",
        "#         X2, y2 = X2.to(device), y2.to(device)\n",
        "        \n",
        "#         npf_model_hidden_layer_outs = npf_model(X1)\n",
        "#         pred, npv_model_hidden_layer_outs = npv_model(X2,act_type, npf_model_hidden_layer_outs)\n",
        "#         #print(\"Debug1 \", pred.shape)\n",
        "#         loss = loss_fn(pred, y1)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # if batch%10 == 0:\n",
        "#         #     #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#         #     loss, current = loss.item(), batch * len(X1)\n",
        "#         #     print(f\"loss: {loss:>7f} Batch:{batch} [{current:>5d}/{size:>5d}]\")\n",
        "#         if int(state_info['epoch']) <= 2:\n",
        "#           predictions, hidden_layer_outputs, loss = evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#           state_info.update({'step' : batch+1})\n",
        "#           state_info.update({'loss' : loss})\n",
        "#           state_info = format_state_info(state_info)\n",
        "#           routine(hidden_layer_outs_container = hidden_layer_outs_over_batches, state_info_container = state_info_over_batches, predictions_container = predictions_over_batches,\n",
        "#             hidden_layer_outputs = hidden_layer_outputs, state_info = state_info, predictions = predictions\n",
        "#             )\n",
        "        \n",
        "#     return predictions_over_batches, hidden_layer_outs_over_batches, state_info_over_batches\n",
        "\n",
        "# def evaluate_decoupled(dataloader, npf_model, npv_model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     npf_model.eval()\n",
        "#     npv_model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch, (X, y) in enumerate(dataloader):\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             npf_model_hidden_layer_outs = npf_model(X)\n",
        "#             pred, npv_model_hidden_layer_outs = npv_model(X,'soft', npf_model_hidden_layer_outs)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     npf_model_hidden_layer_outs = [x.clone().detach().to('cpu').numpy() for x in npf_model_hidden_layer_outs]\n",
        "#     print(f\"Loss over all data : {test_loss:>5f} \\n\")\n",
        "#     return pred, npf_model_hidden_layer_outs, test_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def run_npf_npv_model(train_dataloader, dataloader_all, protocol = 'DGN', n_h_l = 5,n_n = 32, n_runs = 5, n_epochs = 250, bias = True):\n",
        "\n",
        "#     model_protocol_type = protocol\n",
        "#     n_hidden_layers = n_h_l\n",
        "#     n_neurons = n_n\n",
        "\n",
        "#     state_info = {\"model_protocol_type\" : model_protocol_type,\"n_hidden_layers\":n_hidden_layers, \"n_neurons\" : n_neurons}\n",
        "#     hidden_layer_outs_5_runs = []\n",
        "#     state_info_5_runs = []\n",
        "#     predictions_5_runs = []\n",
        "    \n",
        "#     for run in range(n_runs):\n",
        "      \n",
        "#       #Model Init\n",
        "#       npf_model = NPFNeuralNetwork(n_hidden_layers, n_neurons, model_protocol_type, bias).to(device)\n",
        "#       npv_model = NPVNeuralNetwork(n_hidden_layers, n_neurons, bias).to(device)\n",
        "#       torch.save(npv_model.state_dict(), \"npv_init_weights\")\n",
        "\n",
        "#       loss_fn = nn.MSELoss()\n",
        "#       optimizer = torch.optim.Adam([\n",
        "#                       {'params': npf_model.parameters()},\n",
        "#                       {'params': npv_model.parameters()}],\n",
        "#                       lr = 3e-3) \n",
        "      \n",
        "#       #Make the non linear layer of NPF model untrainable\n",
        "#       #NOTE: Unchecked for different hidden layers(Currently working for n_h_l = 5)\n",
        "#       if model_protocol_type == 'DGN-DLGN-SF':\n",
        "#         for idx, param in enumerate(npf_model.parameters()):\n",
        "#           if idx % 4 == 0 or idx-1 % 4 == 0:\n",
        "#             param.requires_grad = False\n",
        "      \n",
        "#       #Local Variable Init\n",
        "#       hidden_layer_outs_run = []\n",
        "#       state_info_run = []\n",
        "#       predictions_run = []\n",
        "#       epochs = n_epochs\n",
        "\n",
        "#       #Evaluate before training starts\n",
        "#       predictions, hidden_layer_outputs, loss =  evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#       state_info.update({'run' : run+1, 'epoch' : 0,'step' : 0,'loss' : loss,'learning_status' : 'UnLearned'})\n",
        "#       state_info = format_state_info(state_info)\n",
        "#       routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#               hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions]\n",
        "#               )\n",
        "\n",
        "#       for epoch in range(epochs):\n",
        "#         print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "#         state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : None,'loss' : loss,'learning_status' : 'Learned'})\n",
        "#         predictions_batches, hidden_layer_outs_batches, state_info_batches = train_decoupled(train_dataloader,train_dataloader,npf_model, npv_model, loss_fn, optimizer,\n",
        "#                                                                   dataloader_all = dataloader_all, state_info = state_info)\n",
        "        \n",
        "\n",
        "#         if int(state_info['epoch']) <= 2: \n",
        "#             routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#                     hidden_layer_outputs = hidden_layer_outs_batches, state_info = state_info_batches, predictions = predictions_batches\n",
        "#                     )\n",
        "        \n",
        "#         # else: #int(state_info['epoch']) > 2:\n",
        "#         elif int(state_info['epoch'])  <= 20:\n",
        "#             predictions, hidden_layer_outputs, loss =  evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#             state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : 16,'loss' : loss,'learning_status' : 'Learned'})\n",
        "#             state_info = format_state_info(state_info)\n",
        "#             routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#                     hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions]\n",
        "#                     )\n",
        "#         elif int(state_info['epoch'])  % 10 == 0:\n",
        "#             predictions, hidden_layer_outputs, loss =  evaluate_decoupled(dataloader_all,npf_model, npv_model, loss_fn)\n",
        "#             state_info.update({'run' : run+1, 'epoch' : epoch+1,'step' : 16,'loss' : loss,'learning_status' : 'Learned'})\n",
        "#             state_info = format_state_info(state_info)\n",
        "#             routine(hidden_layer_outs_container = hidden_layer_outs_run, state_info_container = state_info_run, predictions_container = predictions_run,\n",
        "#                     hidden_layer_outputs = [hidden_layer_outputs], state_info = [state_info], predictions = [predictions]\n",
        "#                     )\n",
        "#         if int(state_info['epoch']) == 2:\n",
        "#               for idx, param in enumerate(npf_model.parameters()):\n",
        "#                   param.requires_grad = False\n",
        "#               npv_model.load_state_dict(torch.load(\"npv_init_weights\"))\n",
        "#       state_info_5_runs.append(state_info_run)\n",
        "#       hidden_layer_outs_5_runs.append(hidden_layer_outs_run)\n",
        "#       predictions_5_runs.append(predictions_run)\n",
        "\n",
        "#     return hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3WoEdcgX7B7"
      },
      "outputs": [],
      "source": [
        "# seed = 10\n",
        "# set_seed(seed = seed)\n",
        "\n",
        "# train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr = get_multi_modes_data()\n",
        "# n_data = len(train_data_curr)\n",
        "# mini = get_mini_multi_modes_data(test_data_curr,num_modes = 21, n_examples_per_mode = 10)#get_mini_multi_modes_data(instance = 10)\n",
        "\n",
        "# train_dataloader, test_dataloader, train_dl_PWC, test_dl_PWC = get_dataloaders(train_data_curr, test_data_curr, train_labels_curr, test_labels_curr, batch_size = 64)\n",
        "# mini_dl = get_mini_multi_mode_dataloaders(mini, batch_size = len(mini))\n",
        "# x_idxs_pair = get_input_idxs(len(mini))\n",
        "# n_batches = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkPXWK6orsSa"
      },
      "outputs": [],
      "source": [
        "# def  get_hyperplanes_params_temp(model,protocol_type,  for_layers, for_mode):\n",
        "#   layer_weights = []\n",
        "#   layer_slopes = []\n",
        "#   layer_intercepts = []\n",
        "#   layer_biases = []\n",
        "\n",
        "#   # if for_mode == 1:\n",
        "#   #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,0:2]\n",
        "#   # elif for_mode == 21:\n",
        "#   #   layer_weight = model.layers[0].weight.detach().to(\"cpu\")[:,-2:]\n",
        "\n",
        "\n",
        "#   layer_weight = model.layers[0].weight.detach().to(\"cpu\")\n",
        "\n",
        "#   layer_bias = model.layers[0].bias.detach().to(\"cpu\")\n",
        "\n",
        "#   layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "#   layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "#   layer_slopes.append(layer_slope)\n",
        "#   layer_intercepts.append(layer_intercept)\n",
        "\n",
        "#   layer_weights.append(layer_weight)\n",
        "#   layer_biases.append(layer_bias)\n",
        "#   for i in range(1, for_layers):\n",
        "#     curr_layer_weight =  model.layers[i].weight.detach().to(\"cpu\")\n",
        "#     curr_layer_bias = model.layers[i].bias.detach().to(\"cpu\")\n",
        "#     if protocol_type == 'DLGN':\n",
        "#       layer_weight = torch.matmul(input= curr_layer_weight, other =  layer_weight)\n",
        "#       layer_bias = torch.matmul(input= curr_layer_weight, other =  layer_bias) + curr_layer_bias\n",
        "#     elif protocol_type == 'DLGN-SF':\n",
        "#       layer_weight = curr_layer_weight\n",
        "#       layer_bias = curr_layer_bias\n",
        "\n",
        "#     layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "#     layer_intercept = -(layer_bias / layer_weight[:,1])\n",
        "    \n",
        "#     layer_slopes.append(layer_slope)\n",
        "#     layer_intercepts.append(layer_intercept)\n",
        "\n",
        "#     layer_weights.append(layer_weight)\n",
        "#     layer_biases.append(layer_bias)\n",
        "#   return layer_weights, layer_biases, layer_slopes, layer_intercepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMRUt_d1QxQG"
      },
      "outputs": [],
      "source": [
        "#1010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2qL0ZIfebRT"
      },
      "outputs": [],
      "source": [
        "# seed = 1010 #10 for NN\n",
        "# set_seed(seed = seed)\n",
        "\n",
        "# train_data_curr, train_labels_curr, vali_data_curr, vali_labels_curr, test_data_curr, test_labels_curr, mini_test_data, mini_test_labels = get_cube_data()\n",
        "# n_data = len(train_data_curr)\n",
        "# # mini = get_mini_multi_modes_data(test_data_curr,num_modes = 6, n_examples_per_mode = 10)\n",
        "\n",
        "# train_dataloader, test_dataloader, train_dl_PWC, test_dl_PWC = get_dataloaders(train_data_curr, test_data_curr, train_labels_curr, test_labels_curr, batch_size = 64)\n",
        "# mini_dl = get_mini_multi_mode_dataloaders(mini_test_data, batch_size = len(mini_test_data))\n",
        "# x_idxs_pair = get_input_idxs(len(mini_test_data))\n",
        "# n_batches = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT-BugDaYWK4"
      },
      "outputs": [],
      "source": [
        "# def h(a, b, sigma):\n",
        "#   return np.amin([a/sigma, b/sigma, 1])\n",
        "# def Variable_K(X1, X2, sigma):\n",
        "#   print(X1.shape, X2.shape)\n",
        "#   K = np.zeros((X1.shape[0], X2.shape[0]))\n",
        "#   dist = get_dist(train_data_curr)\n",
        "#   # sigma = .1\n",
        "#   for i, x1 in enumerate(X1):\n",
        "#     for j, x2 in enumerate(X2):\n",
        "#       # K[i][j] = np.exp(- np.sum( np.power((x1 - x2),2) ) / float( 2*(sigma**2) ) )\n",
        "#       K[i][j] = np.exp(-np.square(np.linalg.norm(x1 - x2))/(2*sigma*sigma*h(dist[i], dist[j], sigma)))\n",
        "#   return K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN0tOHDz4e_D"
      },
      "outputs": [],
      "source": [
        "# def svm_run():\n",
        "\n",
        "#     gammas=[.001]\n",
        "#     C_vals = [ 5]\n",
        "# #     gammas=[10]\n",
        "# #     C_vals = [1.]\n",
        "#     if np.mean(train_labels_curr)==1 or np.mean(train_labels_curr)==0:\n",
        "#            return np.mean(test_labels_curr!=train_labels_curr[0]) , 0. , 0.01, 1.\n",
        "\n",
        "\n",
        "#     min_valierror=np.inf\n",
        "#     for g in gammas:\n",
        "#         for C in C_vals:\n",
        "#           for sigma in [.1]:\n",
        "#             classification_alg = SVC(kernel= \"precomputed\", gamma=g, C=C)\n",
        "#             classifier = classification_alg.fit(Variable_K(train_data_curr, train_data_curr, sigma), train_labels_curr)\n",
        "#             predictions = classifier.predict(Variable_K(train_data_curr, train_data_curr, sigma))\n",
        "#             train_err = np.sum(predictions!=train_labels_curr)\n",
        "#             # train_err = 0\n",
        "#             predictions = classifier.predict(Variable_K(vali_data_curr, train_data_curr, sigma))\n",
        "#             vali_err=np.sum(predictions!=vali_labels_curr)\n",
        "            \n",
        "#             predictions = classifier.predict(Variable_K(test_data_curr, train_data_curr, sigma))\n",
        "#             test_err=np.sum(predictions!=test_labels_curr)\n",
        "#             # test_err = 0\n",
        "#             if vali_err<min_valierror:\n",
        "#                 min_valierror = vali_err\n",
        "#                 min_testerror = test_err\n",
        "#                 best_C = C\n",
        "#                 best_gamma = g\n",
        "#                 best_train_err = train_err\n",
        "#                 best_pred = predictions\n",
        "#             print(vali_err*100/len(vali_labels_curr), g, C)\n",
        "#     print(min_testerror*100/len(test_labels_curr),min_valierror*100/len(vali_labels_curr), best_train_err, best_gamma, best_C, best_pred)\n",
        "#     return min_testerror*100/len(test_labels_curr),min_valierror*100/len(vali_labels_curr), best_train_err, best_gamma, best_C, best_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9TKGoSLPtN8"
      },
      "outputs": [],
      "source": [
        "# K = np.zeros((n_data, n_data))\n",
        "# dist = get_dist(train_data_curr)\n",
        "# sigma = 1\n",
        "# for i in range(n_data):\n",
        "#   for j in range(n_data):\n",
        "#     K[i][j] = np.exp(-np.square(np.linalg.norm(train_data_curr[i] - train_data_curr[j]))/(2*sigma*sigma*h(dist[i], dist[j], sigma)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw_Emr9AYpC9"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import classification_report\n",
        "# tuned_parameters = [\n",
        "#     {\"kernel\": [\"rbf\"], \"gamma\": [0.01,0.03,0.1,0.3,1,3,10,30,100,1000,10000], \"C\": [0.1, 1.,10.,100.,1000, 15000, 20000, 25000]},\n",
        "    \n",
        "# ]\n",
        "\n",
        "# scores = [\"accuracy\"]\n",
        "\n",
        "# for score in scores:\n",
        "#     print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "#     print()\n",
        "\n",
        "#     clf = GridSearchCV(SVC(), tuned_parameters, scoring='accuracy')\n",
        "#     clf.fit(train_data_curr, train_labels_curr)\n",
        "\n",
        "#     print(\"Best parameters set found on development set:\")\n",
        "#     print()\n",
        "#     print(clf.best_params_)\n",
        "#     print()\n",
        "#     print(\"Grid scores on development set:\")\n",
        "#     print()\n",
        "#     means = clf.cv_results_[\"mean_test_score\"]\n",
        "#     stds = clf.cv_results_[\"std_test_score\"]\n",
        "#     for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
        "#         print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "#     print()\n",
        "\n",
        "#     print(\"Detailed classification report:\")\n",
        "#     print()\n",
        "#     print(\"The model is trained on the full development set.\")\n",
        "#     print(\"The scores are computed on the full evaluation set.\")\n",
        "#     print()\n",
        "#     y_true, y_pred = vali_labels_curr, clf.predict(vali_data_curr)\n",
        "#     print(classification_report(y_true, y_pred))\n",
        "#     print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwBXMKanhEnw"
      },
      "outputs": [],
      "source": [
        "# plt.scatter(test_data_curr[1000:2000,-2],test_data_curr[1000:2000,-1], c = test_labels_curr[1000:2000] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kof2Nt9-4T5k"
      },
      "outputs": [],
      "source": [
        "# n_data = 10000\n",
        "# a, b, d = 10, 100, 4\n",
        "# both_mode, mode1, mode2, mini = get_dsphere_dls(n_data = n_data, a = a, b = b, d = d, batch_size = None, is_perturbed = True)\n",
        "# mode = 'both'\n",
        "# if mode == 'both':\n",
        "#   X_train, X_test, y_train, y_test = both_mode[0], both_mode[1], both_mode[2], both_mode[3]\n",
        "# elif mode == 'm1':\n",
        "#   X_train, X_test, y_train, y_test = mode1[0], mode1[1], mode1[2], mode1[3]\n",
        "# elif mode == 'm2':\n",
        "#   X_train, X_test, y_train, y_test = mode2[0], mode2[1], mode2[2], mode2[3]\n",
        "\n",
        "# train_dataloader, test_dataloader = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# mini_dl, Y_sorted = get_mini_dls(mini[0], mini[1], batch_size = 100)\n",
        "# Y_sorted = np.array(Y_sorted).reshape((len(Y_sorted), 1))\n",
        "# x_idxs_pair = get_input_idxs(Y_sorted)\n",
        "# n_batches = len(train_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjV696thp-kv"
      },
      "outputs": [],
      "source": [
        "# seed = 10\n",
        "# set_seed(seed = seed)\n",
        "# a, b = 2, 20\n",
        "# both_mode, mode1, mode2, mini = get_mnist_modified_data(a, b)\n",
        "# mode = 'both'\n",
        "# if mode == 'both':\n",
        "#   X_train, X_test, y_train, y_test = both_mode[0], both_mode[1], both_mode[2], both_mode[3]\n",
        "# elif mode == 'm1':\n",
        "#   X_train, X_test, y_train, y_test = mode1[0], mode1[1], mode1[2], mode1[3]\n",
        "# elif mode == 'm2':\n",
        "#   X_train, X_test, y_train, y_test = mode2[0], mode2[1], mode2[2], mode2[3]\n",
        "\n",
        "# train_dataloader, test_dataloader = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# # X_train, X_test, y_train, y_test = mode1[0], mode1[1], mode1[2], mode1[3]\n",
        "# # train_dataloader_m1, test_dataloader_m1 = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# # X_train, X_test, y_train, y_test = mode2[0], mode2[1], mode2[2], mode2[3]\n",
        "# # train_dataloader_m2, test_dataloader_m2 = get_dataloaders(X_train, X_test, y_train, y_test, batch_size = 64)\n",
        "\n",
        "# mini_dl, Y_sorted = get_mini_dls(mini[0], mini[1], batch_size = 100)\n",
        "# Y_sorted = np.array(Y_sorted).reshape((len(Y_sorted), 1))\n",
        "# x_idxs_pair = get_input_idxs(Y_sorted)\n",
        "# n_batches = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ86z3ItJLwm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8EaJhAqKBPQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKlemLTz2ppN"
      },
      "outputs": [],
      "source": [
        "# for run in range(hyp_5_runs):\n",
        "#   for epoch in range(hyp_5_runs[run]):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nRCk8kKB9RH"
      },
      "outputs": [],
      "source": [
        "# n_parts = 4\n",
        "# n_neurons = 32\n",
        "# n_layers = 3\n",
        "# fig = plt.figure(figsize=(7*n_parts, n_layers*10))\n",
        "# outer = gridspec.GridSpec(3,1, wspace=0.2, hspace=0.2)\n",
        "\n",
        "# for layer_no in range(3):\n",
        "#     inner = gridspec.GridSpecFromSubplotSpec(2,4,\n",
        "#                     subplot_spec=outer[layer_no], wspace=0.1, hspace=0.2)\n",
        "\n",
        "#     # for j in range(8):\n",
        "#     #     ax = plt.Subplot(fig, inner[j])\n",
        "#         # t = ax.text(0.5,0.5, 'outer=%d, inner=%d' % (i, j))\n",
        "#     for i in range(n_parts):\n",
        "#       ax = plt.Subplot(fig, inner[i])\n",
        "#       ax.plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),0])\n",
        "#       ax.set_title(f\"# of positive pts,{int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "#       fig.add_subplot(ax)\n",
        "      \n",
        "#     for i in range(n_parts):\n",
        "#       ax = plt.Subplot(fig, inner[i+4])\n",
        "#       ax.plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),1])\n",
        "#       ax.set_title(f\"# of negative pts, {int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "#       fig.add_subplot(ax)\n",
        "\n",
        "# fig.show(N)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # for layer_no in range(1,4):  \n",
        " \n",
        "# #   n_parts = 4\n",
        "# #   n_neurons = 32\n",
        "# #   f, axes = plt.subplots(2, n_parts,figsize = (n_parts*10,10))\n",
        "\n",
        "# #   for i in range(n_parts):\n",
        "# #     axes[0][i].plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),0])\n",
        "# #     axes[0][i].set_title(f\"# of positive pts,{int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "# #   for i in range(n_parts):\n",
        "# #     axes[1][i].plot(np.array(hyp_5_runs)[0][:,layer_no-1,int(i*n_neurons/n_parts):int((i+1)*n_neurons/n_parts),1])\n",
        "# #     axes[1][i].set_title(f\"# of negative pts, {int(i*n_neurons/n_parts)} to {int((i+1)*n_neurons/n_parts)}\")\n",
        "# #   f.suptitle(f'Layer No = {layer_no}')\n",
        "# # plt.savefig('afdfdsfds')\n",
        "# # plt.plot(np.array(hyp_5_runs)[0][:,layer_no-1,:8,0], label = ['pos'])\n",
        "# # plt.figure()\n",
        "# # plt.plot(np.array(hyp_5_runs)[0][:,layer_no-1,8:16,0], label = ['pos'])\n",
        "\n",
        "# # plt.plot(np.array(hyp_5_runs)[0][:,layer_no-1,n_no-1,1], label = ['neg'])\n",
        "\n",
        "# # plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTf9aRebQ1Kw"
      },
      "outputs": [],
      "source": [
        "# B = pairwise_distances(mini_test_data,mini_test_data) \n",
        "# IP = np.dot(mini_test_data, mini_test_data.T)\n",
        "# A = overlap[0]*IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7Ps2AkNPcxE"
      },
      "outputs": [],
      "source": [
        "# effective_kernel_width = np.zeros(2400)\n",
        "# closeness_laplacian = np.zeros(2400)\n",
        "# for i in range(2400):\n",
        "#   d = np.inf\n",
        "#   for alpha in np.arange(.2, 1,.01):\n",
        "#     v1 = A[i,:]/A[i,i]\n",
        "#     v2 = np.exp(-B[i,:]/alpha)\n",
        "#     # plt.figure()\n",
        "#     # plt.plot(v1)\n",
        "#     # plt.plot(v2)\n",
        "#     if np.linalg.norm(v1-v2) < d:\n",
        "#       d = np.linalg.norm(v1-v2)\n",
        "#       effective_kernel_width[i] = alpha\n",
        "#       closeness_laplacian[i] = d/np.linalg.norm(v1)\n",
        "\n",
        "# plt.plot(effective_kernel_width)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG9UkwI-TRrV"
      },
      "outputs": [],
      "source": [
        "# plt.plot(closeness_laplacian)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyCxYYRwS_7m"
      },
      "outputs": [],
      "source": [
        "# plt.plot(effective_kernel_width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cnE_xxZJXs1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Epoch 10\n",
        "# -------------------------------\n",
        "# Train Loss: 0.253225 Batch:0 [    0/  640]\n",
        "# Train : \n",
        "#  Accuracy = 78.8, Loss =  0.346732\n",
        "# Test : \n",
        "#  Accuracy: 72.0%, Loss: 0.409975 \n",
        "\n",
        "# Epoch 11\n",
        "# -------------------------------\n",
        "# Train Loss: 0.371883 Batch:0 [    0/  640]\n",
        "# Train : \n",
        "#  Accuracy = 77.7, Loss =  0.345637\n",
        "# Epoch 12\n",
        "# -------------------------------\n",
        "# Train Loss: 0.362101 Batch:0 [    0/  640]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLhcSY7iKKrj"
      },
      "outputs": [],
      "source": [
        "# !zip -r resultall.zip /content/\n",
        "# files.download('resultall.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdri1UctAyjJ"
      },
      "outputs": [],
      "source": [
        "# def plot_kernels_all_modes(kernel_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False): \n",
        "#   instance = 100\n",
        "#   s_i = state_info_5_runs\n",
        "#   for run in range(1):\n",
        "#     for epoch in range(len(kernel_5_runs[run])):\n",
        "#       for step in range(len(kernel_5_runs[run][epoch])):\n",
        "#         for mode in range(num_modes):\n",
        "          \n",
        "#           curr_mode_kernels = kernel_5_runs[run][epoch][step][mode*instance : (mode+1)*instance][mode*instance : (mode+1)*instance]\n",
        "#           YKYs = get_YKYs(curr_mode_kernels)\n",
        "#           plot_heatmap(curr_mode_kernels,YKYs, s_i[run][epoch][step], all_input = for_all_inputs, save_fig = save_fig, show_fig = show_fig, mode = mode+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfQ3pG_xt5yy"
      },
      "outputs": [],
      "source": [
        "# acc_runs_test = []\n",
        "# acc_runs_train = []\n",
        "\n",
        "# for run in range(len(state_info_5_runs)):\n",
        "#   acc = []\n",
        "#   acc_tr = []\n",
        "#   for epoch in range(len(state_info_5_runs[run])):\n",
        "#     acc.append(state_info_5_runs[run][epoch][-1]['test_acc'])\n",
        "#     acc_tr.append(state_info_5_runs[run][epoch][-1]['train_acc'])\n",
        "#   acc_runs_test.append(max(acc))\n",
        "#   acc_runs_train.append(max(acc_tr))\n",
        "\n",
        "\n",
        "# acc_runs_train, acc_runs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuTeKnq_xWub"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mRC3dVwuXzi"
      },
      "outputs": [],
      "source": [
        "# max(acc), np.argmax(acc), len(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpoCD2aq23PO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# '''\n",
        "#  a := Number of half cycles in the top half of the circle\n",
        "#  b := number of half cycles in the bottom half of the circle\n",
        "#  For a, b = 1, 9:\n",
        "#   MLP takes 60 epochs\n",
        "#   DGN takes 130 epochs\n",
        "#   DLGN takes 80 epochs\n",
        "#   DLGN takes 70 epochs\n",
        "#   DLGN takes 70 epochs\n",
        "# '''\n",
        "# n_epochs = 200\n",
        "\n",
        "\n",
        "# step_stop_point = 2 \n",
        "# step_stepsize = 10\n",
        "# epoch_stop_point = 50\n",
        "# epoch_stepsize = 10\n",
        "# start_point = get_start_point(epoch_stop_point, epoch_stepsize)\n",
        "# # n_data_points = 500\n",
        "# # a, b = 1,9\n",
        "# # point1_degrees, point2_degrees, diff_in_degrees = 40, 300, 10\n",
        "# seed = 0\n",
        "# set_seed(seed = seed)\n",
        "\n",
        "# # train_dataloader, dataloader_all, Y_sorted = get_mnist_dls(batch_size = 64, is_perturbed = True)\n",
        "# train_dataloader, dataloader_all, Y_sorted = get_dsphere_dls(batch_size = 64, is_perturbed = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# n_hidden_layers,  n_neurons, n_runs =  5, 64, 5\n",
        "# save_result = True\n",
        "\n",
        "# #Available Protocols : MLP, DGN, DLGN, DLGN-SF, DGN-DLGN-SF\n",
        "# # protocols = ['MLP', \"DGN\", 'DLGN', 'DLGN-SF','DGN-DLGN-SF']\n",
        "# # protocols = ['DLGN', 'DLGN-SF','DGN-DLGN-SF']\n",
        "# protocols = ['MLP', 'DGN']\n",
        "\n",
        "# bias = True\n",
        "# for protocol in protocols:\n",
        "#   set_seed(seed = seed)\n",
        "#   if protocol == 'MLP':\n",
        "#     hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs = run_mlp_model(train_dataloader=train_dataloader,\n",
        "#                                                                                     dataloader_all = dataloader_all,\n",
        "#                                                                                     n_h_l = n_hidden_layers,\n",
        "#                                                                                     n_n = n_neurons,\n",
        "#                                                                                     n_runs = n_runs, n_epochs = n_epochs, bias = bias, is_mnist_data = True)\n",
        "#   else:\n",
        "#     hidden_layer_outs_5_runs, state_info_5_runs, predictions_5_runs = run_npf_npv_model(train_dataloader=train_dataloader,\n",
        "#                                                                                         dataloader_all = dataloader_all,\n",
        "#                                                                                         protocol = protocol,\n",
        "#                                                                                         n_h_l = n_hidden_layers,\n",
        "#                                                                                         n_n = n_neurons,\n",
        "#                                                                                         n_runs = n_runs,\n",
        "#                                                                                         n_epochs = n_epochs, bias = bias)\n",
        "\n",
        "# kernel_5_runs = build_kernels(hidden_layer_outs_5_runs)\n",
        "# for kernel_name in ['K2', 'K3','K']:\n",
        "#   plot_kernel_values_stepwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "#   plot_kernel_values_epochwise(kernel_name,kernel_5_runs, state_info_5_runs, x_idxs_pair )\n",
        "\n",
        "# # plot_stepwise_activations(hidden_layer_no =2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "# # plot_epochwise_activations(hidden_layer_no = 2, hidden_layer_outs_5_runs = hidden_layer_outs_5_runs)\n",
        "# plot_stepwise_loss(state_info_5_runs)\n",
        "# plot_epochwise_loss(state_info_5_runs)\n",
        "# plot_kernels(hidden_layer_outs_5_runs, state_info_5_runs,for_all_inputs = True, save_fig = True, show_fig = False)\n",
        "\n",
        "# #Saving results of each protocol\n",
        "# if save_result:\n",
        "#   file_name = f'{protocol}_5_64_dsphere_all_epochs_figs'\n",
        "#   save_results(file_name)\n",
        "\n",
        "# #Download all protocols result\n",
        "# if save_result:\n",
        "#   download_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlvwtDngPoEM"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# pred = torch.argmax(nn.Softmax(dim = 1)(predictions_5_runs[0][-1][-1]), dim = 1).detach().to('cpu')\n",
        "# def get_predictions(predictions_5_runs):\n",
        "#   predictions = []\n",
        "#   for run in range(1):\n",
        "#     for epoch in range(len(predictions_5_runs[run])):\n",
        "#       for step in range(len(predictions_5_runs[run][epoch])):\n",
        "#         pred = torch.argmax(nn.Softmax(dim = 1)(predictions_5_runs[run][epoch][step]), dim = 1).detach().to('cpu')\n",
        "#         print(pred)\n",
        "#         predictions.append(pred)\n",
        "#   return predictions\n",
        "# preds = get_predictions(predictions_5_runs)\n",
        "# ConfusionMatrixDisplay.from_predictions(Y_sorted, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDuROlYUvNII"
      },
      "outputs": [],
      "source": [
        "#Slider for visulalizing Y_true v/s Y_predicted\n",
        "# preds = [v for x in predictions_5_runs[0] for v in x]\n",
        "# plot_y_prediction(Y_sorted, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuyw3HYsWA4C"
      },
      "outputs": [],
      "source": [
        "# #Store in pickle file\n",
        "# temp_dict = {'hidden_layer_outs':hidden_layer_outs_5_runs, 'state_info':state_info_5_runs}\n",
        "# info = state_info_5_runs[0][0][0]\n",
        "# fname = f'{info['model_protocol_type']}_{info['n_hidden_layers']}_{info['n_neurons']}_pkl.pkl'\n",
        "# with open(fname, \"wb\") as fp:   \n",
        "#   pickle.dump(temp_dict, fp)\n",
        "\n",
        "\n",
        "# os.system(f\"zip -R {fname}.zip '*.pkl'\")\n",
        "# # !zip -r /content/DGN_DLGN_SF_5_32_all_epochs_pkl.zip /content/DGN_DLGN_SF_5_32_all_epochs_pkl\n",
        "# files.download(f\"{fname}.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZISI7e5Q2MP"
      },
      "outputs": [],
      "source": [
        "# # !unzip /content/MLP_5_32_all_epochs_pkl.zip \n",
        "# with open(\"/content/content/MLP_5_32_all_epochs_pkl\", \"rb\") as fp:  \n",
        "#   b = pickle.load(fp)\n",
        "# kernel_5_runs = b['kernels']\n",
        "# state_info_5_runs = b['state_info']\n",
        "# intermediate_outs_5_runs = b['hidden_layer_outs']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQBqtlhzJ_o"
      },
      "source": [
        "----------------------------------------------------------- Commented Old Codes -----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FjPk7HKBtH4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "[ 4629  1771 50630 50628 43602 51228  5368 30384  4962 15825 30044 48554\n",
        " 32609 16326 45905 23372  2539  1386 39029 40320  1000 32574 48780 38177\n",
        " 39155 15288 36412   283 10254 54404  7409  2084   119 55785 18278 10323\n",
        " 57827 37220 24069 51629 33302 25325 43727 38055   489 17871 31476 40241\n",
        " 33640 28904]\n",
        "[46518 11637 53341 18546 51882 52868  5477  2907 21554 39444 45197 19742\n",
        " 23554  6356 50838 56291 33577 46936 53531 11901 33440 17662 14293 23885\n",
        " 59530 30117 36687 27967  3532  8873 57192 37155  4772 44705 26429 41524\n",
        " 25940 24288 39622 13725 49035 37635 56491 45182 53001 34033 55338  3120\n",
        " 46254 11793]\n",
        "[20028 57933  8960 13922 41489 37734 53237   917 16309 10988 55589 37084\n",
        "  8993 23171 19028 28832 39228 13774 47839 30975 30346 26794 39467 53837\n",
        " 48624 43153 34155  8598  8460 57966 55528 48946 55381  1715  2418  3965\n",
        " 43851  5624 52456 45505  3989 14063 21219 26164 50478 55001 16142 32485\n",
        " 25700 29911]\n",
        "[ 6118   495 51837 42120 49529 22234 56297 46525 25377 24356 37729 21325\n",
        " 15872  6539  2286 58878 24042 38894 18588 31249 20356 25291 18380  4001\n",
        " 46971 41629 44385 37913 51260 51776  6719   629  7625  4864 18044 58412\n",
        "  9730 29023 38086 34427 28174 52939  3670 29225 59652 30417 16074 30489\n",
        " 56721 15226]\n",
        "[11008 58650 10560 28253 52522 19033 19032 57443 55552 43481 55004  4781\n",
        " 39239 35624 13775 21590  3350  4768 39894  3903  7863 52988 23538 23103\n",
        " 55024 54819 31458 16802 33165 29941 16338 44523 35904 44213 18846 57537\n",
        " 11471 34389 40519  7327  3990 58225 29814 46052 55654 42346 35926 34465\n",
        " 35141 47680]\n",
        "[49048 19839 30390 36480 18416 24630 41533 42889 32501 18234 59822  6997\n",
        " 52364 57345 49820 52540 51254 59216 22101 47854 19706 47940  5456 13863\n",
        " 32570 42447 12484  7232  5292 41521   376 25295 34841   836 44819 55455\n",
        " 47320 26290 46774 25508    11 41693  4004 10904 54400 25407 29029 24887\n",
        " 57837 23183]\n",
        "[17626 46003 35099 47006 30950 19293 11132 47415 28883 26347 11871 55456\n",
        " 31606 48823 23421 49043 40491  7008 28934 19118 21025 16746 15051  9929\n",
        "  9120  6789 16957 33751 58724  7504 34698 36637 33692 48380  2320 59591\n",
        " 42027 51481 47864 43225 53081 16736 37966 34621 44742 37724 21003 19451\n",
        " 59866 43141]\n",
        "[52879 17703 24705  6221 59124  9992 54426 28723 54494  4217 34981 20370\n",
        " 28117  3899 34472 38243 30952 55431 54207 45533  7961 13255 14455 43277\n",
        " 27292 23621 10427 14076   968 11235  8828  6956 58977 42057 17443 58524\n",
        " 16771 42016 45530 36691 35539 35321 37971 49553 18498 15098 50742 21437\n",
        " 26560  9768]\n",
        "[35428 43372 22849 27508  7026 33018 59542 56613 16937 50413  6006 37030\n",
        " 53420   386 32573 20690 51412 49963 10738 58849 34820  8732 55761 59097\n",
        "  9008 38267 32166 51513 19557 58643 21694 25796 50346 20725  5394 57244\n",
        " 46793  6129 56284 44944 47030 16158 30067 12455 24110 54846 40453 58679\n",
        " 55929 32175]\n",
        "[46655 30085 11900 33199 22782 43596 51654 37115 50029  8899  2997 18731\n",
        "  3904 50558 28168 15222 49734 37400 45944 58796 33891 10799 11366  3598\n",
        " 11499 11388 21724 48580 52184 43888 55849 23732 42773 26877 16006 39932\n",
        " 55454  3316 37306 35741 53280  7053 34616  9394 28750 33701  9969 45787\n",
        " 21592 41953]\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dFQZtmAZs3"
      },
      "outputs": [],
      "source": [
        "# #activation for epoch wise for MLP\n",
        "# def plot_epochwise_activations():\n",
        "#   if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "#     temp = []\n",
        "#     y_label = 'ReLU(x)' if state_info_5_runs[0][0][0]['model_protocol_type'] == 'MLP' else 'ReLU(x)*G(x)'\n",
        "#     for i in range(5):\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
        "#           121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
        "#           134, 135, 136, 137, 138, 139, 140, 141, 142]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "    \n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()\n",
        "#   #activation for epoch wise for DGN\n",
        "#   else:\n",
        "#     temp = []\n",
        "#     x = np.concatenate((np.arange(31), np.arange(31,250,5)))\n",
        "#     for i in range(5):\n",
        "#     #   if i ==0:\n",
        "#     #     temp = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     #   else:\n",
        "#     #     temp += np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     # temp /= 5\n",
        "#       # intermediate_outs = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "#       y_label = 'ReLU(x)*G(x)'\n",
        "\n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(x,plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(x, plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(x, plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(x, plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPiXewrmpPjU"
      },
      "outputs": [],
      "source": [
        "# #activation for epoch wise for MLP\n",
        "# def plot_epochwise_activations():\n",
        "#   if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "#     temp = []\n",
        "#     y_label = 'ReLU(x)' if state_info_5_runs[0][0][0]['model_protocol_type'] == 'MLP' else 'ReLU(x)*G(x)'\n",
        "#     for i in range(5):\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
        "#           121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
        "#           134, 135, 136, 137, 138, 139, 140, 141, 142]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "    \n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()\n",
        "#   #activation for epoch wise for DGN\n",
        "#   else:\n",
        "#     temp = []\n",
        "#     x = np.concatenate((np.arange(31), np.arange(31,250,5)))\n",
        "#     for i in range(5):\n",
        "#     #   if i ==0:\n",
        "#     #     temp = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     #   else:\n",
        "#     #     temp += np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#     # temp /= 5\n",
        "#       # intermediate_outs = np.concatenate(all_intermediate_outs[i], axis = 0)\n",
        "#       intermediate_outs = np.array(intermediate_outs_5_runs[i])\n",
        "#       n_steps = [0,15, 30, 45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
        "#             56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
        "#             69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "#             82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
        "#             95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
        "#           108, 109, 110, 111, 112, 113, 114, 115, 116]\n",
        "#       plotx1 = intermediate_outs[n_steps,0]\n",
        "#       plotx2 = intermediate_outs[n_steps,1]\n",
        "#       plotx3 = intermediate_outs[n_steps,2]\n",
        "#       plotx4 = intermediate_outs[n_steps,3]\n",
        "#       fig = plt.figure(figsize=(30,10))\n",
        "\n",
        "#       y_label = 'ReLU(x)*G(x)'\n",
        "\n",
        "#       plt.subplot(2, 2, 1)\n",
        "#       plt.suptitle('Hidden Layer 2, Run = '+ str(i+1))\n",
        "#       plt.plot(x,plotx1)\n",
        "#       plt.title('x1')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "    \n",
        "\n",
        "#       plt.subplot(2, 2, 2)\n",
        "#       plt.plot(x, plotx2)\n",
        "#       plt.title('x2')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 3)\n",
        "#       plt.plot(x, plotx3)\n",
        "#       plt.title('x3')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "#       plt.subplot(2, 2, 4)\n",
        "#       plt.plot(x, plotx4)\n",
        "#       plt.title('x4')\n",
        "#       plt.xlabel('epoch')\n",
        "#       plt.ylabel(y_label)\n",
        "\n",
        "#       plt.savefig(f'Run_{i+1}_epoch.png')\n",
        "#       # plt.plot(plotx1, ax = axes[0])\n",
        "#       # plt.title('x1')\n",
        "#       # plt.plot(plotx2, ax = axes[1])\n",
        "#       # plt.title('x2')\n",
        "#       # plt.plot(plotx3, ax = axes[2])\n",
        "#       # plt.title('x3')\n",
        "#       # plt.plot(plotx4, ax = axes[3])\n",
        "#       # plt.title('x4')\n",
        "#     # intermediate_outs = temp\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9NK-t0fizc_"
      },
      "outputs": [],
      "source": [
        "# # For epoch wise loss analysis\n",
        "# def plot_epochwise_loss():\n",
        "#   values = []\n",
        "#   for run in range(len(kernel_5_runs)):\n",
        "#     for epoch in range(len(kernel_5_runs[run])):\n",
        "#         loss = state_info_5_runs[run][epoch][-1]['loss']\n",
        "#         values.append(loss)\n",
        "        \n",
        "#         # print(run, epoch)\n",
        "#         # plot_heatmap(kernel_5_runs[run][epoch][step], state_info_5_runs[run][epoch][step])\n",
        "      \n",
        "#   values = np.array(values)\n",
        "\n",
        "#   #For loss v/s epoch for MLP\n",
        "#   if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "#     f, axes = plt.subplots(1, 5,figsize = (5*10,10))\n",
        "#     for run in range(5):\n",
        "#       plt.suptitle('Loss v/s epoch',ha = 'center', fontsize = 30)\n",
        "#       for i in range(len(values)):\n",
        "#         one_run_len = int(values.shape[0]/5)\n",
        "#         start = run*one_run_len\n",
        "#         axes[run].plot(values[start:start+one_run_len])\n",
        "#         axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "#         axes[run].set_xticks(np.arange(0,one_run_len, 5.0))\n",
        "\n",
        "#     plt.savefig('loss_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#     plt.show()\n",
        "\n",
        "#   # For loss v/s epoch for DGN \n",
        "#   else:\n",
        "#     f, axes = plt.subplots(1, 5,figsize = (5*10,10))\n",
        "#     x = np.concatenate((np.arange(31), np.arange(31,250,5)))  \n",
        "#     for run in range(5):\n",
        "#       plt.suptitle('Loss v/s epoch',ha = 'center', fontsize = 30)\n",
        "#       for i in range(len(values)):\n",
        "#         one_run_len = int(values.shape[0]/5)\n",
        "#         start = run*one_run_len\n",
        "#         axes[run].plot(x, values[start:start+one_run_len])\n",
        "#         axes[run].set_xticks(np.arange(min(x), max(x)+1, 10.0))\n",
        "#         axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "#     plt.savefig('loss_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wffd27fzQbnu"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "  # Kernel value graphs epoch wise for MLP\n",
        "  if state_info_5_runs[0][0][0]['model_protocol_type'] != 'DGN':\n",
        "    labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "    f, axes = plt.subplots(5, 1,figsize = (20,7*5))\n",
        "    for run in range(5):\n",
        "      # plt.figure(figsize = (10,10))\n",
        "      plt.suptitle(f'{kernel_name}_epoch',ha = 'center', fontsize = 30)\n",
        "      for i in range(len(values)):\n",
        "        one_run_len = int(values.shape[1]/5)\n",
        "        start = run*one_run_len\n",
        "        axes[run].plot(values[i][start:start+one_run_len], label=labels[i])\n",
        "        axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "        axes[run].legend(loc='best')\n",
        "\n",
        "    plt.savefig(f'{kernel_name}_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "\n",
        "  #Kernel value graphs epoch wise for DGN since it is plotted in interval of 5 after 30 epochs\n",
        "  else:\n",
        "    labels = ['x1x1', 'x2x2', 'x3x3', 'x4x4', 'x1x2', 'x3x4']\n",
        "    f, axes = plt.subplots(5, 1,figsize = (20,7*5))\n",
        "    x = np.concatenate((np.arange(31), np.arange(31,250,5)))  \n",
        "    for run in range(5):\n",
        "      plt.suptitle(f'{kernel_name}_epoch',ha = 'center', fontsize = 30)\n",
        "      for i in range(len(values)):\n",
        "        one_run_len = int(values.shape[1]/5)\n",
        "        start = run*one_run_len\n",
        "        axes[run].plot(x, values[i][start:start+one_run_len], label=labels[i])\n",
        "        axes[run].set_xticks(np.arange(min(x), max(x)+1, 10.0))\n",
        "        axes[run].set_xlabel(f'Run {run+1}', fontsize = 20)\n",
        "        axes[run].legend(loc='best')\n",
        "\n",
        "    plt.savefig(f'{kernel_name}_epoch' + \".png\" ,format = \"png\",bbox_inches='tight', dpi = 100)\n",
        "    plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oFhwInbQy01"
      },
      "outputs": [],
      "source": [
        "# import numpy as np \n",
        "# import plotly.graph_objs as go\n",
        "# from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
        "\n",
        "# #from  https://stackoverflow.com/questions/47230817/plotly-notebook-mode-with-google-colaboratory\n",
        "# def configure_plotly_browser_state():\n",
        "#   import IPython\n",
        "#   display(IPython.core.display.HTML('''\n",
        "#         <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "#         <script>\n",
        "#           requirejs.config({\n",
        "#             paths: {\n",
        "#               base: '/static/base',\n",
        "#               plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext', \n",
        "#             },\n",
        "#           });\n",
        "#         </script>\n",
        "#         '''))\n",
        "\n",
        "\n",
        "# init_notebook_mode(connected=True)\n",
        "\n",
        "# #from plotly sliders\n",
        "# data = [dict(\n",
        "#         visible = False,\n",
        "#         line=dict(color='#00CED1', width=2),\n",
        "#         name = ' = '+str(step),\n",
        "#         x = np.arange(0,500,1),\n",
        "#         y = d.to('cpu').flatten(),\n",
        "#         #  y =Y_sorted.flatten()\n",
        "#        ) for d in predictions_run]\n",
        "# data[10]['visible'] = True\n",
        "\n",
        "# data2 = [dict(\n",
        "#         visible = False,\n",
        "#         line=dict(color='#00CED1', width=2),\n",
        "#         name = ' = '+str(step),\n",
        "#         x = np.arange(0,500,1),\n",
        "#         y = Y_sorted.flatten(),\n",
        "#        ) for d in predictions_run]\n",
        "# data[10]['visible'] = True\n",
        "\n",
        "# #configure added to visualize in colab\n",
        "# configure_plotly_browser_state()\n",
        "\n",
        "# steps = []\n",
        "# for i in range(len(data)):\n",
        "#     step = dict(\n",
        "#         method = 'restyle',  \n",
        "#         args = ['visible', [False] * len(data)],\n",
        "#     )\n",
        "#     step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
        "#     steps.append(step)\n",
        "\n",
        "# sliders = [dict(\n",
        "#     active = 10,\n",
        "#     currentvalue = {\"prefix\": \"Frequency: \"},\n",
        "#     pad = {\"t\": 50},\n",
        "#     steps = steps\n",
        "# )]\n",
        "\n",
        "# layout = dict(sliders=sliders)\n",
        "\n",
        "# fig = dict(data=data, layout=layout)\n",
        "\n",
        "# fig2 = dict(data=data2, layout=layout)\n",
        "# configure_plotly_browser_state()\n",
        "\n",
        "\n",
        "# #removed py. from original plotly code \n",
        "# iplot(fig, filename='Sine Wave Slider')\n",
        "# iplot(fig2, filename='Sine Wave Slider')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxeoMCVWUe0Y"
      },
      "outputs": [],
      "source": [
        "# output = pd.DataFrame(state_info)\n",
        "# sub_out = pd.DataFrame(list(output[4]))\n",
        "\n",
        "# sub_out = sub_out.fillna(value='Null')\n",
        "# cols = sub_out.ne('Null').cumsum(axis=1).idxmax(axis=1)\n",
        "# K_temp = sub_out.lookup(sub_out.index, cols)\n",
        "\n",
        "# cols = list(cols)\n",
        "\n",
        "# for i, row in sub_out.iterrows():\n",
        "#   sub_out.at[i,cols[i]] = 'Null'\n",
        "# sub_out.insert(0, 'K', K_temp)\n",
        "\n",
        "# final_sub_out = sub_out\n",
        "# final_output = pd.concat([output, final_sub_out], axis = 1, ignore_index=True)\n",
        "\n",
        "\n",
        "# final_output = final_output.drop([4, final_output.shape[1]-1], 1)\n",
        "# final_output\n",
        "\n",
        "#final_output = final_output.drop([4, -1], 1)\n",
        "# final_output = final_output.set_axis(['protocol', 'n_h_l', 'n_n','Run', 'Status', 'K', 'K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8'], axis=1, inplace=False)\n",
        "# final_output = final_output.set_axis(['protocol', 'n_h_l', 'n_n','Run', 'Status', 'K', 'K1', 'K2', 'K3', 'K4', 'K5'], axis=1, inplace=False)\n",
        "\n",
        "# final_output.to_csv('final_output.csv')\n",
        "# files.download(\"final_output.csv\")\n",
        "# dF1 = pd.read_csv(\"final_output1.csv\")\n",
        "# dF2 = pd.read_csv(\"final_output2.csv\")\n",
        "# dF3 = pd.read_csv(\"final_output3.csv\")\n",
        "# dF4 = pd.read_csv(\"final_output4.csv\")\n",
        "# dF5 = pd.read_csv(\"final_output5.csv\")\n",
        "# dF6 = pd.read_csv(\"final_output6.csv\")\n",
        "# sin10_all_out = pd.concat([dF6, dF1,dF3, dF4,dF5, dF2], axis=0).drop(['Unnamed: 0'], axis = 1)\n",
        "# dF7 = pd.read_csv(\"final_output7.csv\")\n",
        "# dF8 = pd.read_csv(\"final_output8.csv\")\n",
        "# sin4_all_out = pd.concat([dF7, dF8], axis=0).drop(['Unnamed: 0'], axis = 1)\n",
        "# sin10_all_out.to_csv('sin10_all_output.csv')\n",
        "# sin4_all_out.to_csv('sin4_all_output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMnnenOPBely"
      },
      "outputs": [],
      "source": [
        "# def  get_hyperplanes_params(model, for_layers):\n",
        "#   layer_weights = []\n",
        "#   layer_slopes = []\n",
        "#   layer_intercepts = []\n",
        "#   for i in range(for_layers):\n",
        "#     layer_weight = model.layers[i].weight.detach().to(\"cpu\")\n",
        "#     layer_slope = -(layer_weight[:,0]/layer_weight[:,1])\n",
        "#     layer_intercept = [0]*32#model.layers[i].bias.detach().to(\"cpu\")\n",
        "\n",
        "#     layer_weights.append(layer_weight)\n",
        "#     layer_slopes.append(layer_slope)\n",
        "#     layer_intercepts.append(layer_intercept)\n",
        "#   return layer_slopes, layer_intercepts\n",
        "\n",
        "\n",
        "# def plot_hyperplanes(model, train_status):\n",
        "  \n",
        "#   for_layers = len(model.layers)-1\n",
        "#   layer_slopes, layer_intercepts = get_hyperplanes_params(model, for_layers)\n",
        "  \n",
        "  \n",
        "#   x_axis = np.linspace(start = -2, stop = 2)\n",
        "#   for i in range(for_layers):\n",
        "#     plt.figure(figsize = (10,10))\n",
        "#     #fig, ax = plt.subplots(figsize = (10,10))\n",
        "#     ax = sns.scatterplot(X_sorted[:,0], X_sorted[:,1], c = prediction.to(\"cpu\"))\n",
        "#     ax.set_xlim((-2,2))\n",
        "#     ax.set_ylim((-2,2))\n",
        "#     for j in range(32):\n",
        "#       y_axis = layer_slopes[i][j]*x_axis + layer_intercepts[i][j]\n",
        "#       #plt.plot(x_axis, y_axis)\n",
        "#       sns.lineplot(x = x_axis, y = y_axis)\n",
        "#     plt.title(train_status+\" Layer_\"+str(i+1))\n",
        "#     plt.savefig(train_status+\" Layer_\"+str(i+1))\n",
        "#     plt.show()\n",
        "\n",
        "#     plot_hyperplanes(npf_model, model_protocol_type+\" \"+model_learning_status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcSE8YdjH1yo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_Tcijr0Lq35P",
        "pHzf9mTzrLYm",
        "4M72vt8AmiUl",
        "XbqVgGWLnG1X",
        "3NYTooMfn1fG",
        "L3cYcHIMnxMF"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMjWb3O9z3PFdyNSTVUtLoS",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}